* [2017-04-03 lun.]
** Notes on setting up for the MoNoGe project              :eclipse:emfviews:
Eclipse 3.8 does not start with Java 9 from Oracle.

: sudo update-alternatives java

-> java8-openjdk was installed on the machine

Installed Eclipse 4.6 anyway.

Additional dependencies for the project:

- ATL
- Xtext
- Epsilon (http://download.eclipse.org/epsilon/updates/)

First two can be installed from Eclipse (Help -> Install modeling components).
Epsilon has an update site.

For running the ECNA2014 demo, I will probably need:

- BPMN 2
- RMF (ReqIf)

Short-term tasks:

- Re-run the examples from the video with the source code
- Learn EMF
- Draw a diagram of how EMFViews works, structurally (what plugin does what, how
  they fit together, etc.)

** Importing EMFViews into Eclipse                         :eclipse:emfviews:
For some reason, when importing the plugin projects, Eclipse thinks the packages
in the src/ folder begin with src.  Save for the very first one.

I just need to open the project properties, go to build path, and click apply.
The src folder is already the build path, but the setting is not applied
correctly.

Then, I also need to add the "plugin dependencies" library for each project.

Doing that for all plugins...

There are still errors from missing dependencies.  Why is there no "one-button"
install?

Finally, some discrepancies in version numbers for our own plugins, that could
be tracked to the way different versions of the plugins have been merged into
the repository.

Only 7 errors left out of >1000 thousands when I started.  The remaining errors
I have to look into the APIs of the libraries to see if some things have changed
since 2013.

* [2017-04-04 mar.]
** Inria provided infrastructure                                   :atlanmod:
R told me about continuous integration server provided by Inria.  They run on
jenkins, and we can have a decent amount of VMs apparently.

They also host a GitLab service, so we could host repositories there as well.

** Understanding the code                                          :emfviews:
There's a feature.xml that seem to describe the EMFViews feature, with juicy
info like dependency with version numbers:

: <import plugin="org.eclipse.uml2" version="4.0.0" match="greaterOrEqual"/>
: <import plugin="org.eclipse.emf.ecore.xmi" version="2.7.0" match="greaterOrEqual"/>
: <import plugin="org.eclipse.uml2.uml" version="4.0.0" match="greaterOrEqual"/>

Not all dependencies have explicit version numbers, but that's a start.

There's also a discrepancy in version numbers: in the feature.xml file, all the
plugins have the version "0.2.0.qualifier", but in the MANIFEST.MF for the
EMFViews plugin, we ask for 1.0.0 versions:

: Require-Bundle: org.eclipse.emf.ecore,
:  org.eclipse.emf.ecore.xmi;bundle-version="2.7.0",
:  fr.inria.atlanmod.emfviews.vlink-mm;bundle-version="1.0.0",
:  org.eclipse.core.resources;bundle-version="3.8.1",
:  fr.inria.atlanmod.emfviews.virtuallinksdelegator;bundle-version="1.0.0",

I understand we also have code generated from ECore models (vlink-mm) which is
checked in the repo.  Since the code is generated from the model, it might make
more sense to not check it in, and regenerate after a ~build~ step.

Other discrepancy: copyright attribution on the feature.xml file is for Inria
Rennes Bretagne Atlantique, while the rest of the plugin, you can see individual
contributor names.  Which is it?

* [2017-04-05 mer.]
** Exploring Eclipse                                     :eclipse:
Specifically, I'm interested in understanding how and where Eclipse saves
preferences for a project: dependencies, how to build, etc.

That's important when putting these things into a repository.

For instance: when I "Run as.. Eclipse Application" an Eclipse plugin like the
one from this [[http://eclipsesource.com/blogs/tutorials/emf-tutorial/][EMF tutorial]], it runs the new Eclipse under another workspace.
The path of this new workspace is specified by the "Eclipse Application" run
configuration.

By default, it is:

: ${workspace_loc}/../runtime-EclipseApplication

So it creates a folder above the current workspace.  Since my workspace is in
$HOME, it creates $HOME/runtime-EclipseApplication.  But, for the purpose of the
tutorial, the files I create in this new workspace should reside inside the
repository.  So, I rather want:

: ${project_loc}/../runtime-EclipseApplication

so it creates the folder at the same level of the project folder I am running.

Now, where is this preference saved?

: $ rg --hidden "project_loc" ./eclipse/ eclipse/ workspace/ proj/monoge/emftuto/
: workspace/.metadata/.plugins/org.eclipse.debug.core/.launches/Eclipse Application.launch

In my workspace.  I guess it makes sense since "Run as..." is an Eclipse thing,
so it should be an Eclipse pref.

But, if you checkout the repository, you would have to manually replicate the
"Run as..." setting in order to get my examples working.

One solution is to export the workspace preferences and put it in the
repository.  So at least, if you use Eclipse, you can import these settings as a
one-click solution.

** Speeding up Eclipse                                              :eclipse:
Even on a powerful and recent machine, Eclipse is quite slow and feels
unresponsive.  Things to set in eclipse.ini to ([[https://www.eclipsecon.org/europe2015/sites/default/files/slides/Boosting%2520the%2520Performance%2520of%2520your%2520Eclipse%2520IDE_0.pdf][allegedly]]) speed it up:

#+BEGIN_EXAMPLE
-server
-Xms512m
-Xmx2g
-Xmn512m
-Xverify:none
-XX:+AggressiveOpts
-XX:+UseParallelGC
#+END_EXAMPLE

Removing unnecessary stuff also helps: Git control, startup plugins,
auto-updates, etc.

Curiously, in the Modeling Tools distribution, there is another EGit plugin that
you also have to remove.

** Trying to replicate the ECNA2014 demo                           :emfviews:
I've got the emfviews plugin building without errors in Eclipse.  Run
as... application (I guess?)

Now to create an EAdata project, add the travelAgencyEA.xmi.

Uhoh, error:

: http://www.obeonetwork.org/dsl/togaf/contentfwk/9.0.0' not found

Hmm hmm.  Maybe I need [[https://github.com/ObeoNetwork/TOGAF-Designer][this plugin]]?

Getting the source, adding the plugin (subfolders) to the EMFViews workspace,
and adding them as a dependency for the EMFViews plugin, running "Run as.."
again.  I can check in Help -> Installation details of the recursive Eclipse
that EMFViews and TOGAF plugins are both present.

After that, opening the travelAgencyEA.xmi again yields:

: org.eclipse.emf.ecore.xmi.FeatureNotFoundException: Feature 'belongsTo' not found. (platform:/resource/EAdata/models/1_travelAgencyEA.xmi, 26, 270)

A bunch of them.  'isOwnedByUnit', 'communicatedWithFunctions',
'providesEntities', 'containers', 'labels'.

And some IllegalValueExceptions:

: Value 'BusinessService[TRANSIENT]' is not legal. (platform:/resource/EAdata/models/1_travelAgencyEA.xmi, -1, -1)

Could it be a mismatch between the TOGAF version I'm using and the one used in
the demo?

Grepping around the TOGAF repo, I can see hits for these strings, especially in

: plugins/org.obeonetwork.dsl.togaf.contentfwk/model/contentfwk.aird

#+BEGIN_EXAMPLE
1277:      <ownedDiagramElements xmi:type="diagram:DEdge" xmi:id="_eyUzMP63Ed-AK7xgn-H1PA" name="[0..1] isOwnedByUnit - [0..*] ownsFunctions" sourceNode="_ugYTwJ-9Ed-hg-_nMagkzg" targetNode="_zjeaMJ-9Ed-hg-_nMagkzg">
1280:        <semanticElements xmi:type="ecore:EReference" href="contentfwk.ecore#//Function/isOwnedByUnit"/>
2562:        <lines xmi:type="table:DLine" xmi:id="_BsDvz6AREd-mRqry0T_xvQ" label="EReference : isOwnedByUnit">
2563:          <target xmi:type="ecore:EReference" href="contentfwk.ecore#//Function/isOwnedByUnit"/>
2564:          <semanticElements xmi:type="ecore:EReference" href="contentfwk.ecore#//Function/isOwnedByUnit"/>
2567:            <target xmi:type="ecore:EReference" href="contentfwk.ecore#//Function/isOwnedByUnit"/>
#+END_EXAMPLE

I can also see these strings in the EA.ecore metamodel, and these looks like
definitions.

Maybe the example was self-sufficient after all.  But how to link the XMI file
to an ECore metamodel?

Further investigation.  If I load up the TOGAF contentfwk model (one I found to
load it is to create a dummy ECore file, and Right click -> Load resource, and
look for the obeo URL near the bottom).

I can see the mismatch.  The TOGAF contentfwk has, in the Function class:

- communicatesWithFunctions
- isOwnedByOrganizationUnit

whereas the EA.ecore has:

- communicatedWithFunctions
- isOwnedByUnit

So the errors make sense.  And, it would seem that I need to point the XMI files
to use my EA.ecore metamodel.

Looking for help.. G helpfully answered my questions:

You can't link an XMI to an Ecore as-is.  Eclipse has a global registry of
metamodels, and there is no way to Right-click on an Ecore file, and add it to
the registry.

(Unless, possibly maybe, through some ATL plugin, but I don't have it installed
right now)

The alternative is to generate plugin code from the Ecore metamodel, and "Run
as.." again.

So:

In the base Eclipse workspace (where the EMFViews plugin resides), create an
empty EMF project add the EA.ecore file to the model folder.  Create a genmodel
file, and generate the model code.

Then, launch the recursive Eclipse.  All open projects will be loaded as
plugins, so no need to add this project (or TOGAF) as dependency to EMFViews.

This time, I can load the XMI without troubles.

The two other XMI files require BPMN2 and ReqIf10.  At this point, I'm
remembering that the emfviews repo had a "dropins" folder containing JAR files
for BPMN, ReqIf and.. TOGAF contentfwk.  And eclipse has a dropins folder as
well.

I'm thinking that the dropins are supposed to be added to Eclipse in order to
run the examples.

And indeed, it works.
For BPMN2 and ReqIf.  For TOGAF, I get a NullPointerException when trying to
open the Ecore file.  So maybe sticking to what I have generated is better.

Next step in the demo is to create an EMF Viewtype through an Eclipse wizard.  I
don't have that wizard in my recursive Eclipse.  Probably because the "editor"
and "ui" plugin projects are closed in my base Eclipse.

Let's open them.

Uhoh, compile error.

: Viewtype.getHiddenElements()

is undefined.  There is a ~getHiddenAttributes()~ method though.  Let's try
that.

Haha!  Now I have the wizard in my recursive Eclipse.  Clicking next
enthusiastically and:

: java.lang.ArrayIndexOutOfBoundsException: 0
: 	at fr.inria.atlanmod.emfviews.ui.wizard.view.CreateViewtypeScreen.createControl(CreateViewtypeScreen.java:132)

:(

Time to quit for the day.

* [2017-04-07 ven.]
** IndexOfOfBounds exception                                       :emfviews:
Okay so, the incriminating line:

: comboLinksDsl.setText(availableLinksDsls[0]);

We made no provision to check that this array (of strings) had any element, and
we access the first one.

Let's add a check.  I guess I'll be greeted by an empty window, since it means
we haven't found any LinkDSL, but at least I won't crash.

(Though, like JavaScript, Eclipse doesn't crash: it just throws an Exception and
keeps going).

Changing the array value at runtime in the debugger to {"foo"} let me proceed.
I can add the metamodels.  But I can't select any linking DSL (since there is
none, and "foo" isn't a valid one I'd wager).

(To change an array of String in the debugger, right-click -> change value, and:

: return new String[]{"foo"}

primitive values are easier to change, usually just click.)

Still, it let me proceed to the next screen.  Of course, "Finish" triggers a
NullPointerException.

In the demo video, he has "ecl" as DSL language.

Looking at the code that populates availableLinksDsls, it iterates into:

#+BEGIN_SRC java
extensions = Platform
				.getExtensionRegistry()
				.getExtensionPoint(
						"fr.inria.atlanmod.emfviews.virtuallinksdelegator.type")
				.getExtensions();
#+END_SRC

But this is also empty.  I guess because I haven't opened
virtuallinksepsilondelegate.  Let's do that.

I need ECL to compile it.  It comes from org.eclipse.epsilon.  Let's not install
that into Eclipse, but put the JAR in dropins instead.  Wait no, I don't want it
to run as plugin in my Eclipse.  I just need to add it as a dependency to the
project.

Import errors disappear.  Other errors appear.  Deprecated methods
and... methods that are not here anymore.  Presumably because I got the latest
ECL version, and the project used another one.  Is there any trace of the
versions we used previously?

In emfviews/feature.xml:

: <import plugin="org.eclipse.epsilon.ecl.engine"/>
: <import plugin="org.eclipse.epsilon.eol.engine" version="1.0.0" match="greaterOrEqual"/>

Let's try the 1.0.0 version then.  It's from 2012.  I guess the project was
working in 2016, so let's try the 1.3 instead.

1.3 makes one error disappear, still 2 left.  1.2 has only deprecation warnings.

Still errors in the MANIFEST file for unmet dependencies.  But it's for stuff we
don't need, otherwise we wouldn't compile, right?  Let's ditch them.

Ah!  I've got "ecl" in the dropdown menu now.  But clicking "Finish" triggers a
NullPointerException in CreateViewtypeWizard.  It's because we want to open the
newly created .eviewtype using our editor for that file type, and we fail in
Viewtype.loadFilterMetamodel:

#+BEGIN_SRC java
private void loadFilterMetamodel(String filtersMetamodel) {
  ResourceSet filtersResourceSet = new ResourceSetImpl();
  attributesToHideMM = filtersResourceSet.getResource(URI.createPlatformResourceURI(filtersMetamodel, true), true);
}
#+END_SRC

because filtersMetamodel is null at this point.

Culprit: Viewtype.doLoad which pass

: loadFilterMetamodel(properties.getProperty("filtersMetamodel"));

but there is no "filtersMetamodel" property there.  "properties" is created from
parsing an inputStream which seem to correspond to the contents of the eviewtype
file.  And the eviewtype file has no filtersMetamodel value.

When is this written?

Line 545, in Viewtype.serialize.  It puts the value of ~filtersMM~, which is a
String, and populated in the constructor of Viewtype.  Hmm, so that's actually
just the serialization of a Viewtype, but since the constructor already calls
loadFilterMetamodel, I guess this is the wrong place.

In CreateViewtypeWizard.performFinish, we are writing to the eviewtype file.
Then it calls

: serializeViewtype(viewTypeFile, fileContent);

Stepping through CrewteViewtypeWizard.performFinish, there is no code adding the
"filtersMetamodel" line.  And I see no trace of code that /would/ add these
lines to the file.

Also, in the video, there are four files created by the wizard: an ECL, an
Ecore, an EViewtype and an XMI.

I've only got two: EViewtype and XMI.

Strange.

** Reading about Eclipse as a platform                              :eclipse:
http://www.aosabook.org/en/eclipse.html

All classes in a plugin are not considered part of the plugin API.  You need to
define extension points for that.  Visibility of class/method/attributes
is presumably restricted to your plugin.

At Eclipse startup, all plugins manifests are scanned to know the extension
points in advance, but the plugins themselves are not loaded.  It's very much
like Emacs autoloads: they give an example of a plugin adding a menu item, and
only when the user clicks on the menu item will the corresponding plugin be
actually loaded.

Instead of Swing or AWT, Eclipse uses SWT as widget toolkit.  JFace comes on
top, and provides frameworks for preferences and wizards.

Hmm the bit about plugin class visibility is somewhat in contradiction in \sect6.2:

#+BEGIN_QUOTE
If plugin A requires plugin B, plugin A can see all the Java classes and
resources from B, respecting Java class visibility conventions
#+END_QUOTE

Ah I get it know: the above describe the situation before the move to OSGi, and
the paragraph at the start describes the situation after the move.

#+BEGIN_QUOTE
With the switch to OSGi, Eclipse plugins became known as bundles. A plugin and a
bundle are the same thing [...]  Previously, dependencies, exported packages and
the extensions and extension points were described in plugin.xml. With the move
to OSGi bundles, the extensions and extension points continued to be described
in plugin.xml since they are Eclipse concepts. The remaining information was
described in the META-INF/MANIFEST.MF, OSGi's version of the bundle manifest.
#+END_QUOTE

Good news: OSGi supports semantic versioning, very much like SemVer:

major.minor.service.qualifier

Increment major when breaking API
Increment minor when adding API
Increment service for bug fixing
Qualifier is used to indicate a build tag

It's OSGi that takes care of resolving dependencies for a package.

Ah: apart from the extension registry in Eclipse, there is also a service
registry provided by OSGi.  Unlike extensions, services can be discovered
dynamically, after startup.

#+BEGIN_QUOTE
A feature is a PDE (Plugin Development Environment) artifact that defines a set
of bundles that are packaged together in a format that can be built or
installed. Features can also include other features.
#+END_QUOTE

p2 has replaced Update Manager for provisioning Eclipse.  Might be useful for
continuous integration.

** What if I provide filtersMetamodel myself?                      :emfviews:
Since it seems this line is not going to write itself in the eviewtype file,
might as well put it, just to see if the rest of the demo can work.

Ah yes, of course, it points to an ECore file that was also not generated.
Let's bring that in.

Hmm, this time I have a NPE in ViewtypeEditor.createViewtypeTreeEditorPage,
at this line:

: treeViewer.setInput(((Viewtype) viewtypeResource).getResourceSet().getPackageRegistry().values());

because the ~getResourceSet()~ returns null.  The viewtypeResource is populated
from the file at the beginning of the try block:

: viewtypeResource.load(uri.toURL().openStream(), new HashMap<Object, Object>());

and after that, the resourceSet attribute is null.

Since viewtypeResource is an EMF Resource object, maybe a change in the EMF API?

I am tempted to try archeology and rebuild an environment circa 2014.  Here I've
got EMF Ecore 2.12.0 and the feature... has no version requirement.

But, there are version requirements for:

: <import plugin="org.eclipse.emf.ecore.editor" version="2.8.0" match="greaterOrEqual"/>
: <import plugin="org.eclipse.emf.ecore.xmi" version="2.8.1" match="greaterOrEqual"/>

and I've got 2.12 loaded, again.  So since these are part of EMF, a safe guess
would be to find EMF Ecore 2.8.

2.8 is from 2012.

Now, according to the semantic versioning, that shouldn't change anything,
right?

Hmm, trying to put the EMF 2.8 JARs into a copy of my Eclipse Neon 3 resulted in
lots of deep stack straces and an error at launch.  p2 couldn't resolve the
frankenEclipse I created I guess.

Let's get a MDE Eclipse circa 2012 then.  Luna is the first version supporting
Java 8.  That's 2014; maybe it will still make a difference.

Ah of course, I have to set up a new workspace.

Hmm, just importing the projects, and everything builds without errors.
Adding dropins, TOGAF project...

And same exact error!  ViewtypeEditor.createViewtypeTreeEditorPage.

So I guess I would like to know what a ResourceSet is, and what value it should
take at this point.  Maybe brush up my EMF knowledge.

* [2017-04-10 lun.]
** Trying to replicate friday's situation: new error       :eclipse:emfviews:
Can't even get to the EViewtype creation wizard as Eclipse crashes on load with:

: org.eclipse.core.runtime.CoreException: Plug-in "fr.inria.atlanmod.emfviews.virtuallinksepsilondelegate" was unable to instantiate class "fr.inria.atlanmod.emfviews.virtuallinksepsilondelegate.EclDelegate".

How in hell did it work Friday?

Looking at changes I did on the project in Git... wow, there are .class files
checked in.

There are also ~._trace~ files, which I understand are generated by Xtex.  Since
I'm not dealing with Xtext there, and these are generated files, I'd rather get
rid of them.

Some .classpath are checked in, some are not.

Let's just remove the useless files and ignore them to get a usable git diff.

Now, I did modify the MANIFEST which included ECL.  Maybe this wasn't a good
idea?

If I restore these lines, Eclipse complains that it cannot resolve bundles
pertaining to ECL in the MANIFEST of virtuallinksepsilondelegate.

The JARs are in the build path, but maybe they need to be loaded into Eclipse
instead.  Let's remove them from the build path.  Cannot build now because
imports are not resolved.  Let's add them as drop-ins.

Hmm, they don't seem to be recognized when added to the dropins folder.

Opening Window -> Show view -> Error log displays the errors when loading
Eclipse.  To start, I can see that it's trying to load the Git plugin for each
project, even though I've removed it.

Removing all org.eclipse.team plugins fails to start Eclipse.  It's not as
modular when many pieces depend on each other!

Restoring team.core and team.ui did the trick.  At least I got rid of CVS.
I might investigate a minimal Eclipse setup another time.  And one that can be
auto-provisioned trough a config file for reproductibility.

Still have GitProvider errors.  Why is it trying to load the plugin?  Since I
see to Git-related feature under eclipse/features, I'm guessing Git is tightly
integrated into another feature that is still being loaded.  Ugh.

In any case, I can see errors related to my dropins JAR:

#+BEGIN_EXAMPLE
!ENTRY org.eclipse.equinox.p2.publisher.eclipse 4 0 2017-04-10 15:04:47.288
!MESSAGE Unable to acquire PluginConverter service during generation for: /home/fmdkdd/eclipse/dropins/epsilon-1.2-emf-src.jar.

!ENTRY org.eclipse.equinox.p2.core 4 0 2017-04-10 15:04:47.411
!MESSAGE Provisioning exception
!STACK 1
org.eclipse.equinox.p2.core.ProvisionException: No repository found at jar:file:/home/fmdkdd/eclipse/dropins/epsilon-1.2-emf-src.jar!/.
#+END_EXAMPLE

(the Error log view just pretty prints the content of .metadata/.log ... without
giving you the ability to copy lines; and why is there a hidden file in a hidden
folder?  Grmpf).

Maybe, the 1.2 JAR of Epsilon is not OSGi compliant or whatever.  Let's try to
dropins the 1.4.

Hmm nope, same error.

Okay then let's install them from inside Eclipse.  Version 1.2, preferably.

That does get rid of the build errors and MANIFEST errors.  Now, to run.

I have worrying warnings in my .log though:

: org.eclipse.core.runtime.CoreException: Executable extension definition for "class" not found.

But, I can launch the recursive Eclipse.  So, lessons learned:

Required bundles are actually runtime requirements?

** Explaining the discrepancy with the 2014 demo                   :emfviews:
According to H, the previous engineers might have already started some
refactoring in the goal of simplifying Viewtype creation.  In the demo, we see
an Ecore file being created along the Eviewtype.  That's something H did not
want, since we could just register the selected filters in the XMI itself.

In the current version, there is no Ecore file being generated, and the
XMI contains the line:

: <linkedElements elementRef="//Process" modelRef="http://www.obeonetwork.org/dsl/togaf/contentfwk/9.0.0" name="Process" estructuralFeatures="isAutomated"/>

which corresponds to what I've cliked on in the last step of the wizard.

In emfviews.core.Viewtype, there is a serialize method that added the
filtersMetamodel line that was in the demo.  It isn't called anymore by the
wizard.

Looking through the history of this file and in the commit history, all I can
see is that there was a first version of emfviews (0.1 I presume) with a vastly
different Wizard.  Then there was a version 2 (0.2), where the wizard was
changed to basically what I have inherited.

In any case, we don't have a definitive reference of source of truth for "how it
should work" other than examples and videos.  Possibly outdated.

Better brush up my knowledge of Eclipse plugins and EMF.

** A clean Eclipse                                                  :eclipse:
I've found the minimal Eclipse experience:

From [[http://download.eclipse.org/eclipse/downloads/][this page]], go to the latest release, and grab the "Platform runtime binary"
for your arch.

Very snappy Eclipse.  No crap like Mylin and EGit installed by default.

Then, I can pass a configuration folder on the command line:

: eclipse -configuration ~/eclipse-configs/test/configuration

From there, I can install new software from inside eclipse, and they will be
installed at eclipse-configs/test/plugins.  The eclipse-configs/test folder
becomes the new home folder for eclipse.

No pollution between configurations.

So that's good for reproductible environments (except, you know, the manual step
of actually provisioning Eclipse with the new packages).

But now if I need to install the Java dev tools for every
configuration... or even update them... that's going to be a pain.  Ideally, I
read somewhere that p2 was able to pool from a common bundle.  So I should be
able to download all this stuff in just one place, then let Eclipse get the
plugins from one pool.

But I really don't want Eclipse to load /all/ the plugins in the pool.  Even if
it's "lazy-loading" them, it's still taking ages AND I have no say in how
what plugins are /actually/ loaded even if I don't use them.

* [2017-04-11 mar.]
** Trying bundle pools in Eclipse                                   :eclipse:
For speeding up provisioning, and making updates more sane.

I tried to use Oomph, which is actually the Eclipse Installer, the default
download provided by Eclipse.  In the advanced mode, you can select the Eclipse
Platform, and additional projects.  Except, these are pulled from master and put
/into/ your workspace; not additional plugins.  The use case this solved is when
you want to contribute to some Eclipse project.

It also works with any Github project, so I guess you could use Oomph to
somewhat easily provision an Eclipse to work on EMFViews.  You just have to say
"pick the MDE product, then add EMFviews, done".

But in my case I don't want all the cruft.

What you should be able to do is run Oomph, install an Eclipse platform, and
that Eclipse will be setup to get its bundles from the bundle pool.  Except
Oomph fails to install Eclipse platform.

Well, so much for saving bandwidth.

** Installing EMF tools in the platform                             :eclipse:
So installing the EMF sdk feature is apparently not enough to run the tutorial.
I lack emf.edit.  Even though they are part of EMF Core, according to [[http://www.eclipse.org/modeling/emf/][the
website]].

Maybe I'm not pulling from the correct update site?

Adding the update site mentioned on [[http://www.eclipse.org/modeling/emf/updates/][this page]] does not work.  After adding it to
the available sites list in the preferences, and loading up the preferences
again, the site has mysteriously disappeared.

But, there is an update site with URL:

: http://www.eclipse.org/modeling/updates/

disabled by default.  Enabling it and going through "Install new software",
selecting it...

waiting a long time...

Now it has added a bunch of other sites (what?)

And I can install EMF... 2.7.  From 2012.

Gosh, why is this so hard?

On [[http://www.eclipse.org/modeling/emf/downloads/][this page]], it seems I can /download/ an update site containing EMF.  So let's
try to add this ZIP as a local update site.

It does not disappear from the list when I leave it there.

And it's lightning fast when I go into "Install new software".  And it's version
2.12

But EMF Edit is grayed out, since it's already installed.

Removing... installing from this local update site...

EMF edit plugin is marked as loaded in Eclipse, but it's marked as not found in
the plugin.xml dependencies.

Let's remove everything EMF related, and try to load it as a dropin.

There's no dropins folder in my test configuration.  AAArgh.

There's one in eclipsen/platform though.

Hmm, maybe I only have the /runtime/, and not the SDK.  That's another download
on the page.  Let's try that.

Well, the SDK feature seems to only add documentation and source.  No
difference.  Other than that, still not finding emf.edit.

Ah, it works!  [[https://www.eclipse.org/forums/index.php/t/134617/][This thread]] was golden.  Apparently, for building plugins, you
need to setup the Target Platform correctly.  And, for some reason, even though
in my two workspaces they target platforms are set up correctly, they do not
find the same plugins.  One finds 190 plugins, the other 316.

Trying to clean up my test configuration now... trying to install things in
dropins, but that's a BadIdea.  Stuff's missing.  "Install new software" works
when I pull from the default update sites.

Installing PDE, JDT and EMF is enough to be able to run the tutorials.

** Eclipse plugin tutorial                                   :eclipse:plugin:
Following "Eclipse 4 plug-in development by example", by Alex Blewitt, Packt
Open Source.

Plugins which add to the UI or require the UI to operate conventionally have
'ui.' in their package name.

MANIFEST.MF file is for dependencies (OSGi-related stuff).  While the plugin.xml
file is for describing extensions and extension points.

Having extensions described as XML speeds up plug-in loading: you don't have to
execute any code of the plugin (though you do need to parse XML).

** Some links on building plugins with Maven+Tycho           :eclipse:maven:
https://zeroturnaround.com/rebellabs/building-eclipse-plug-ins-with-maven-3-and-tycho/
http://www.vogella.com/tutorials/EclipseTycho/article.html

* [2017-04-12 mer.]
** Eclipse plugin tutorial (cont.)                           :eclipse:plugin:
Clock tuto.

Some issue with Display.getDisplay() that crashes when launching Eclipse with
multiple monitors.  Did not happen when hot loading the code.

How are you supposed to get the current display then?

* [2017-04-14 ven.]
** Eclipse plugin tutorial (cont.)                           :eclipse:plugin:
Re: error from last time.  [[http://stackoverflow.com/questions/33157856/getting-swterror-not-implemented-multiple-displays-with-simple-code-sample][Found someone]] who raised the error on SO.  No answer,
no fix.

From what I gather, you /shouldn't/ use Display.getDisplay, since it creates a
new display (that you need to dispose of).

(Also, the error has nothing to do with multiple /monitors/, but multiple
Display objects as understood by Eclipse.)

If I use Display.getCurrent instead, I get null back, since no display has been
created when Activator.start is called.  Another suggestion is to use:

: PlatformUI.getWorkbench().getDisplay()

this also fails on startup with:

: java.lang.IllegalStateException: Workbench has not been created yet.

Again, it seems the plugin is started very early in the process.  One workaround
would be to create the tray item as soon as the workbench started.  This is
[[https://wiki.eclipse.org/FAQ_Can_I_activate_my_plug-in_when_the_workbench_starts%253F][possible]].

Yep, this works nicely.

** Resource management in JFace and SWT
SWT has manual resource management: when create instance of Color or Image, you
are supposed to .dispose() of them when you don't need them anymore.  That way,
SWT, releases the associated native objects.

JFace has resource registries to deal with the allocation and disposal of resources.

* [2017-04-19 mer.]
** Nearly done with the Eclipse Plugin book                  :eclipse:plugin:
Lots of learning were had.

Chapter 9 touches automated testing with JUnit.  Nothing fancy; plugins just
need to run with a special JUnit configuration.

More interesting is the UI testing with SWTBot, to simulate click and go through
the UI programmatically.  It's fun seeing Eclipse launch and crunch through
dialogs at inhumane speeds.

Although, even if the JUnit bar fills with green, I get a bunch of Exceptions in
the host console after the tests are run.  Presumably, SWTBot is too fast for
Eclipse, and does not take care of disposing some resources properly when
exiting then client instance.

: org.eclipse.swt.SWTException: Failed to execute runnable (org.eclipse.swt.SWTException: Widget is disposed)

** Building the plugins with Maven+Tycho                      :maven:plugin:
Following Chapter 10 of the book.  Apart from writing XML files, it's rather
smooth.

: mvn clean package

seems to poll the Eclipse update sites on each build, which takes a loooooooong
time.  You can avoid that check with the ~--offline~ flag:

: mvn --offline clean package

Hmm can't seem to run the SWTBot tests using Maven.  Might be that the book is
slightly outdated, as it was tested with Tycho 0.18, whereas we now have Tycho
1.0.0.

[[http://www.vogella.com/tutorials/EclipseTycho/article.html][This tutorial]] is fresher.

Still have troubles loading requirements for the test... It seems the client
Eclipse launched by Maven is really barebones (good): there are basically no
views!

I should at least get the Clock View that the tutorial plugins adds.

From what I understand, Maven /should/ obey the plugin dependencies in the
MANIFEST file.  But the runtime target configuration is different.

Maybe try to test with a barebones run configuration in Eclipse itself.

* [2017-04-21 ven.]
** Getting Maven+Tycho to run the tests                        :maven:plugin:
Trying with a barebones run configuration in Eclipse.

Only adding "Required plugins".  Cannot validate due to a not very talkative
error:

: org.eclipse.e4.ui.workbench.swt [9]
: Unresolved requirement: Require-Capability: osgi.extender; filter:="(&(osgi.extender=osgi.component)(version>=1.2)(!(version>=2.0)))"

Maybe it's a [[https://bugs.eclipse.org/bugs/show_bug.cgi?id=494913][bug]]?  Anyway, adding org.eclipse.equinox.ds and clicking "Required
plugins" (for the requirements of equinox) fixes it.

Another way to do it is to add equinox.ds to the dependencies in the MANIFEST,
saving it, and and clicking "Required plugins".  But at this point I don't know
if these dependencies should be declared in the MANIFEST itself...!

Now:

: org.eclipse.core.runtime.AssertionFailedException: null argument:Could not find IExtension for application: org.eclipse.ui.ide.workbench

I know that it works if I select all plugins in the Target Platform, so the
question is: which plugin is missing?

: org.eclipse.ui.ide.application

seems to do it.  At least the client Eclipse runs, but the tests all fail, and
the console is full of:

: Event Admin service is not available, unable to publish event org.osgi.service.event.Event

Adding org.eclipse.equinox.event to the dependencies solved it.  Thanks [[https://www.eclipse.org/forums/index.php/t/293382/][thread]].

Now only the third test fails:

: org.eclipse.swtbot.swt.finder.exceptions.WidgetNotFoundException: Timed out waiting for tree item General

Because when SWTBot does File->New->Project, the dialog is empty.  There's no
General folder.  There's only "Project", no categories.  Hmm, then I can just
use "Project" without adding another dependency.

It works.. not.  It creates the project, but the assertion fails.  Somehow it
runs too fast and does not wait for the project to be created.  Adding a
bot.sleep does it, but there's a nicer way with wait conditions.

Okay so now, back to Maven.

: Tests run: 3, Failures: 0, Errors: 0, Skipped: 0

Actually, before that, I had to remove one testUI that's just too brittle, the
one testing against the String value of the SWTBot shells.

But, it works.

** General best-practices for Eclipse plugins               :eclipse:plugins:
In Mastering Eclipse Plugin Development, by the same author as the tutorial book
I finished this week, there's a chapter Designing Modular Applications with some
pointers on best practices for Eclipse plugins.

How semantic versioning works for Java.  Like semver, adding a method is a minor
increase, changing or removing API is a breaking change.  In Java, adding
methods to interfaces is a breaking change, since classes that implement this
interface have to be modified.  Interfaces can be @noimplement, and adding
methods to these is only a minor version increase.

There's also the @since annotation which is rather useful.

There are tools for looking at the API of your releases and suggesting the
correct semantic version increases, like the API baseline in Eclipse.  Maven can
also do it.

** State of EMFViews                                               :emfviews:
We have no tests.  But, there are 15 examples in the examples/ folder.  Do they
still all work?

I've already asserted that the project is not in the state of running the ECNA
demo from 2014.  (Might not be too far, but things have changed since the demo
at least).

The examples are mostly models: XMI, Ecore, UML files.  Cloc is in fact totally
unable to give me a count, as it ignores basically all files except one in this
folder.

More interesting is when I cloc the plugins directory:

#+BEGIN_EXAMPLE
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
Java                           223          12613          19404          36444
Assembly                         3              0              0           6062
XML                             14             44             18            978
-------------------------------------------------------------------------------
SUM:                           240          12657          19422          43484
#+END_EXAMPLE

That seems like a lot of Java.  And assembly, strangely.  But many Java files
are in fact generated.  Let's look at only emfviews.* plugins.

#+BEGIN_EXAMPLE
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
Java                            58           1268           3376           4856
XML                              7             11              2            149
-------------------------------------------------------------------------------
SUM:                            65           1279           3378           5005
-------------------------------------------------------------------------------
#+END_EXAMPLE

Less daunting.  Many files are smallish to tiny:

: cloc --by-file plugins/fr.inria.atlanmod.emfviews.* | cut -c180-190

#+BEGIN_EXAMPLE
588
272
234
204
188
179
173
167
154
153
139
134
129
126
126
113
110
106
98
93
83
82
77
77
74
73
70
65
60
58
56
55
54
51
50
48
43
38
37
31
24
24
21
20
20
19
16
15
15
15
15
15
15
12
11
11
10
10
10
9
7
7
6
5
5
#+END_EXAMPLE

The largest one is emfviews.editor.editors.ViewtypeEditor, which looks all hand
coded and not generated.

[Still have issues with Epsilon missing; have to install 1.2 from the update
site; ecore, ecore development tools, emf; and also UML2 extenders from the Neon
update site... it builds!]

Now let's look at the dependencies.

* [2017-04-24 lun.]
** Dependency graph for EMFViews                                   :emfviews:

[[file:doc/emfviews-plugin-dependencies.svg]]

All the beige boxes are plugins, eggs don't have a plugin.xml, and all are
included in emfviews.feature.

Most depend on the VirtualLinks model, as expected.

Even though it's included in the feature, emfviews.util has no plugin.xml, and
contains only public static methods.

Curiously, virtuallinksocldelegate does not depend on virtuallinksdelegator,
because it does not implement the interface IVirtualLinksDelegate, even though
virtuallinksepsilondelegate does.

In fact, I'm not sure the OCLDelegate is plugged into anything.  The
VirtualLinksDelegator declares an extension point that is used by the
EpsilonDelegate, but not the OCLDelegate.

There are also two plugins (cream) not declared by the feature.  They don't
compile due to change to emfviews.core, so not sure what their role is.

Other plugins: vpdl.dsl.* and monoge.dsl.*.  VPDL is the View Point Description
Language mentionned in the EMFViews paper.  It seems monoge.dsl.* fits the same
role; all of it was added in one commit title "Added DSL for metamodel
extension".  It might do more.  Hard to say.  Most of it is generated by Xtext.
Have to lookup how Xtext works.  On a cursory examination, monoge.dsl.* seems
less fleshed out than Vpdl.

** Point with H                                                    :emfviews:

- Merge emfviews.util in emfviews preferably
- Merge virtuallinksdelegator in vlink-mm
- monoge.dsl is the DSL for the second paper, vpdl.dsl is the DSL for the first
  paper.  Both are useful, but will be tackled later.
- Check for dead code, duplicates.
- Remove unnecessary dependencies that come from transitivity (if you include
  emfviews, you don't need to include vlink-mm explicitly).

* [2017-04-25 mar.]
** Cleaning up warnings                                            :emfviews:
Mostly generics missing, unused vars, and other niceties.

Could not get at everything, since I don't understand the code fully yet.

* [2017-04-26 mer.]
** Still cleaning up and formatting                                :emfviews:
The autoformatter of Eclipse is helpful, but for wrapping especially, multiple
rules can apply, and I'm not sure of the priority between them.  At least, the
process is deterministic (I hope).

Some things are plain weird in the code.  Pretty sure I am looking at dead code
sometimes, but Eclipse cannot tell me that because these are all plugins, and
public methods are part of the API.

Trying to eliminate the dead code...  Eclipse can tell me if a method is used
in the workspace with Ctrl+Alt+H.  That's helpful.  Then, let's say method M is
unused and I remove it.  M called A.  Now A is unused, and Eclipse tells me so.
But if A was the sole caller of B, Eclipse does not immediately tell me by
transitivity that B is also unused.  A bit annoying.

Wait no.  That's not the good approach.  If B subclasses A, and B is given as an
A somewhere, Eclipse can't know that the overridden methods in B will be
called.

Checking for unused constructors I think should be safe.  Static methods as
well.  And methods that are not overridden.

The case of emfviews.elements.MergeElementsImpl is curious: I can't find any
calls for its constructor, but there are references to /casts/ to this class.
Maybe the instances are created through reflection somewhere, but grepping
around does not help.  Also, it seems it's only partially implemented, as
presumably emfviews.rules.MergeRule is strongly related, and most of the methods
there return null.

So, I'm not sure what's truly dead code and what was just forgotten.  Observations:

In emfviews.ui.CreateViewWizard.performFinish, I have a
EMFViewsFactory.createEView call that's seemingly unused.  Instead, we write
directly to a file in the code that follows.  Same thing with the createViewtype
constructor in the factory; it's called nowhere, and in turn one Viewtype
constructor is never called.  In Viewtype.serialize, there's a bunch of stuff
that's eerily similar to EView.serialize.

That was a part that didn't seem to work when I tried to replicate the ECNA
demo... definitely a hot point.

* [2017-04-28 ven.]
** Looking at the examples                                         :emfviews:
Okay so from what I can see, the examples all contain model files (ECore, XMI,
UML).  No code.

Crucially, the only instructions to use the examples are in videos.  Some
examples contain already-created viewtypes and views.

Opening a viewtype file with the viewtype editor throws an exception:

#+BEGIN_EXAMPLE
java.lang.NullPointerException
	at org.eclipse.emf.common.util.URI$URIPool$PlatformAccessUnit.setValue(URI.java:865)
	at org.eclipse.emf.common.util.URI$URIPool.intern(URI.java:1949)
	at org.eclipse.emf.common.util.URI.createPlatformResourceURI(URI.java:2680)
	at fr.inria.atlanmod.emfviews.core.Viewtype.loadFilterMetamodel(Viewtype.java:171)
	at fr.inria.atlanmod.emfviews.core.Viewtype.doLoad(Viewtype.java:160)
	at org.eclipse.emf.ecore.resource.impl.ResourceImpl.load(ResourceImpl.java:1518)
	at fr.inria.atlanmod.emfviews.editor.editors.ViewtypeEditor.createViewtypeTreeEditorPage(ViewtypeEditor.java:118)
	at fr.inria.atlanmod.emfviews.editor.editors.ViewtypeEditor.createPages(ViewtypeEditor.java:445)
#+END_EXAMPLE

The ER2015 video shows that you can also open the viewtype as an ECore model.
Doing that also throws an exception:

#+BEGIN_EXAMPLE
java.lang.NullPointerException
	at org.eclipse.emf.ecore.resource.impl.ResourceImpl$4.getChildren(ResourceImpl.java:522)
	at org.eclipse.emf.common.util.AbstractTreeIterator.hasAnyChildren(AbstractTreeIterator.java:97)
	at org.eclipse.emf.common.util.AbstractTreeIterator.hasNext(AbstractTreeIterator.java:85)
	at org.eclipse.emf.ecore.presentation.EcoreEditor.createModel(EcoreEditor.java:1278)
	at org.eclipse.emf.ecore.presentation.EcoreEditor.createPages(EcoreEditor.java:1339)
	at org.eclipse.ui.part.MultiPageEditorPart.createPartControl(MultiPageEditorPart.java:363)
#+END_EXAMPLE

This one is more concerning, since it's not tied directly to any code in our
plugins.  Might be that we implement some interface incorrectly.

** Trying to open an eviewtype with the editor                     :emfviews:
So the first NPE was due to missing plugins.  This line in Viewtype:

: EPackage contributingEcoreModelPackage = EPackage.Registry.INSTANCE.getEPackage(modelURI);

was returning null.  The modelURI came from the eviewtype file:

#+BEGIN_EXAMPLE
contributingMetamodels=smartEAintegration/metamodels/contentfwk.ecore,http://www.omg.org/spec/BPMN/20100524/MODEL-XMI,http://www.omg.org/spec/ReqIF/20110401/reqif.xsd
#+END_EXAMPLE

In Viewtype.loadContributingMetamodels, we split on this property value, and for
each model, we make a copy of it and in the copy remove attributes and
classifiers.

But this was null, since the plugins were not in the registry.  This should be a
better error.

Anyway, I could have added the model plugins in the dropins folder of my
Eclipse, but I wanted them to run only on the target configuration.  Going into
run configurations, you cannot add arbitrary plugins that are not already loaded
in the current Eclipse: the plugins can only be a subset of the workspace +
target platform.  But you can change the target platform.  And there, you can
add arbitrary plugins.

Adding the model plugins and their requirements did the trick.

Now, another NPE:

#+BEGIN_EXAMPLE
java.lang.NullPointerException
	at fr.inria.atlanmod.emfviews.editor.editors.ViewtypeEditor.createViewtypeTreeEditorPage(ViewtypeEditor.java:143)
	at fr.inria.atlanmod.emfviews.editor.editors.ViewtypeEditor.createPages(ViewtypeEditor.java:445)
#+END_EXAMPLE

This is because of that line:

: treeViewer
          .setInput(((Viewtype) viewtypeResource).getResourceSet().getPackageRegistry().values());

Namely, viewtypeResource.getResourceSet is null.  [[*What if I provide filtersMetamodel myself?][Wait a minute]].  I got to the
same conclusion two weeks ago.  But then, I have no idea why the resourceSet is
null.

All I know is Viewtype extends ResourceImpl, but does not override that method,
so ResourceImpl returns its resourceSet.  Which, from what I see in the code, is
only ever set by basicSetResourceSet.  Which is never called anywhere.

However, we do have a virtualResourceSet in Viewtype.  Could that be it?

Oh wow, adding:

#+BEGIN_SRC java
  @Override
  public ResourceSet getResourceSet() {
    return virtualResourceSet;
  }
#+END_SRC

does seem to work.  I do have a property editor, a tree viewer, and the text
source view.

I wonder how this ever worked... other than changes in the API.

Anyway, success!

* [2017-05-02 mar.]
** Eclipse non-determinism                                 :emfviews:eclipse:
Launching Eclipse, trying to open an eviewtype file with the editor that I've
fixed Friday... only the source tab works.  The other two are blank.  Friday
this was working...

What changed?

After opening other files... the tabs have mysteriously appeared.  Okay, what's
going on here?  Some lazy loading of plugins?  Then our views are silently
failing if we fail to create them?

Okay so starting Eclipse again... Opening up
~EAview_Test/1_viewtype/myEAviewpoint.eviewtype~ haha!  Blank tabs.  At least
it's consistent this time.

(Note: for some reason, when switching tabs, the eviewtype file is marked as
dirty even though we didn't change its contents)

After I open the XMI file in the same folder, the tabs appear.  When opening the
XMI file I noticed a pause, so it most probably did load something.

** Fixing Eclipse tooltip background                                :eclipse:
The background for Javadoc tooltips is black, with white text, and crucially,
dark blue links.  That's a hard contrast, but crucially, the links are difficult
to read.

It seems Eclipse inherits the value from GTK.  It's true that the tooltips in
Firefox are also white on black.

So what do I have to change?

Adding a ~/.config/gtk-3.0/gtk.css file with:

#+BEGIN_SRC css
.tooltip .info {
  background-color: #f5f5bf;
  color: #000;
}
#+END_SRC

This changes the tooltip color in Firefox, and in Eclipse when I hover buttons
in the toolbar, but /not/ the Javadoc tooltips.

[[https://bugs.eclipse.org/bugs/show_bug.cgi?id=501742][This bug]] seems relevant; the issue has been fixed in the Oxygen pre-release.
But what if I don't want to switch?

There's a Javadoc background color preference in Eclipse->Appearance.  It's for
the Javadoc view, not the Javadoc tooltip.  Curiously, there's no setting for
the foreground, which is white by default, with dark blue links again making
selecting a good background color difficult.

Using ~SWT_GTK3=0~ does have an effect: Eclipse seems to switch to the awful
GTK3 theme, where every widget is large.  The Javadoc tooltips are readable then
(black on light grey), but the links are missing since the SWT browser fails to
instantiate.  That's not a solution.

Changing gtk2 preferences has no effect.

I see that the commit fixing the bug just changes one line in the JDT UI
plugin.xml:

#+BEGIN_SRC diff
       <colorDefinition
             label="%JavadocBackgroundColor.label"
             categoryId="org.eclipse.jdt.ui.presentation"
-            value="COLOR_INFO_BACKGROUND"
+            defaultsTo="org.eclipse.ui.workbench.HOVER_BACKGROUND"
             id="org.eclipse.jdt.ui.Javadoc.backgroundColor">
#+END_SRC

My understanding is that ~COLOR_INFO_BACKGROUND~ is picked up from GTK3, but
that's clearly not the case here as the setting is ignored.  Would have to dig
into the source.

: git clone https://git.eclipse.org/r/jdt/eclipse.jdt.ui
: rg --hidden COLOR_INFO_BACKGROUND

Oh hey:

#+BEGIN_SRC java
eclipse.jdt.ui/org.eclipse.ltk.ui.refactoring/src/org/eclipse/ltk/internal/ui/refactoring/RefactoringStatusDialog.java
87:			Color foreground= parent.getDisplay().getSystemColor(SWT.COLOR_INFO_FOREGROUND);
88:			Color background= parent.getDisplay().getSystemColor(SWT.COLOR_INFO_BACKGROUND);

eclipse.jdt.ui/org.eclipse.jdt.ui/ui/org/eclipse/jdt/internal/ui/infoviews/AbstractInfoView.java
390:			fgColor = display.getSystemColor(SWT.COLOR_INFO_FOREGROUND);
401:			bgColor= display.getSystemColor(SWT.COLOR_INFO_BACKGROUND);
#+END_SRC

getSystemColor then.  Trying to get the values returned for that by Eclipse, I
do get black for background, and white for foreground, even with the gtk.css
file.

In the Display class, there are two functions that set ~INFO_BACKGROUND~ from
GTK: ~gtk_css_default_theme_values~ and ~initializeSystemColors~.

The first looks like it's reading the CSS file for the current theme:

#+BEGIN_SRC java
case SWT.COLOR_INFO_FOREGROUND:
if (OS.GTK_VERSION >= OS.VERSION(3, 20, 0)) {
  tSelected = cssOutput.indexOf ("tooltip * {");
} else {
  tSelected = cssOutput.indexOf (".tooltip {");
}
selected = cssOutput.indexOf ("@define-color tooltip_fg_color");
if (tSelected != -1) {
  if (OS.GTK_VERSION >= OS.VERSION(3, 20, 0)) {
    COLOR_INFO_FOREGROUND = gtk_css_parse_foreground(themeProvider, "tooltip * {");
  } else {
    COLOR_INFO_FOREGROUND = gtk_css_parse_foreground(themeProvider, ".tooltip {");
  }
  return "parsed";
} else if (selected != -1) {
  color = simple_color_parser(cssOutput, "@define-color tooltip_fg_color", selected);
  if (!color.isEmpty()) {
    break;
  }
}
#+END_SRC

Looks like it's not really parsing the whole CSS, just looking for specific
strings and getting the colors.

The search for the background color is slightly different:

#+BEGIN_SRC java
case SWT.COLOR_INFO_BACKGROUND:
			tSelected = cssOutput.indexOf ("tooltip.background {");
			selected = cssOutput.indexOf ("@define-color tooltip_bg_color");
			if (tSelected != -1) {
				COLOR_INFO_BACKGROUND = gtk_css_parse_background(themeProvider, "tooltip.background {");
				return "parsed";
			} else if (selected != -1) {
				color = simple_color_parser(cssOutput, "@define-color tooltip_bg_color", selected);
				if (!color.isEmpty()) {
					break;
				}
			}
			break;
#+END_SRC

It's not picking up the background-color property.  initializeSystemColors is
the one who calls the code above, with the logic:

#+BEGIN_SRC java
if (OS.GTK_VERSION >= OS.VERSION(3, 14, 0)) {
			String colorInfoForeground = gtk_css_default_theme_values(SWT.COLOR_INFO_FOREGROUND);
			if (!colorInfoForeground.isEmpty()) {
				if (colorInfoForeground != "parsed") {
					rgba = gtk_css_property_to_rgba (colorInfoForeground);
					COLOR_INFO_FOREGROUND = toGdkColor (rgba);
				}
			} else {
				styleContextGetColor (context, OS.GTK_STATE_FLAG_NORMAL, rgba);
				COLOR_INFO_FOREGROUND = toGdkColor (rgba);
			}
		} else {
			styleContextGetColor (context, OS.GTK_STATE_FLAG_NORMAL, rgba);
			COLOR_INFO_FOREGROUND = toGdkColor (rgba);
		}
#+END_SRC

I don't know how what CSS Eclipse gets back from GTK, but the ones I have in the
theme are separated into multiple files, with a main.css that includes other
files with @import directives.  Since the code above is grepping for
@define-color, I'm guessing it's looking at the raw CSS files sitting on my
drive.  In that case, it will only match the @define-color line, which goes
through ~simple_color-parser~.

#+BEGIN_SRC java
String simple_color_parser (String output, String value, int index) {
	/*
	 * This method takes a color value (rgb(...), #rgb, an X11 color, etc.)
	 * and makes sure it's input we can handle. We can handle rgb/rgba values,
	 * X11 colors, or colors in the format #rgb or #rrggbb.
	 *
	 * We cannot handle shade/gradient functions or references to other colors.
	 * Because of this we strip out values that start with "@" and check
	 * non rgb values against X11 named colors.
	 *
	 * The following would be invalid input:
	 *
	 * shade(@bg_color, 0,7)
	 * or
	 * define-color error_bg_color @bg_color
	 */
	if (output != null && value != null) {
		int position;
		String color;
		position = index + value.length() + 1;
		color = output.substring(position);
		// Check for rgb color case
		if (color.startsWith("#") || color.startsWith("rgb")) {
			return color;
		} else if (!color.startsWith("@")) {
			// Check for an X11 color
			String [] cut = color.split(";");
			if (colorList.contains(cut[0])) {
				return color;
			}
		}
	}
	return "";
}
#+END_SRC

This function, again, seems rather brittle; it will break if there is more than
one space before the actual color value given to a @define-color prop.  Since
this is clearly a flavor of CSS used by GTK, maybe they have a stricter syntax
than CSS.  Or maybe all CSS files on the web are actually non-compliant, but web
browsers are lax in parsing?

Anyway, my ~tooltip_bg_color~ has a hexadecimal value, so it should return it.
And then... since we are not returning "parsed", the color is converted to RGBA,
which calls delegates to native code.

#+BEGIN_SRC java
GdkRGBA gtk_css_property_to_rgba(String property) {
	/* Here we convert rgb(...) or rgba(...) properties
	 * into GdkRGBA objects using gdk_rgba_parse(). Note
	 * that we still need to remove the ";" character from the
	 * input string.
	 */
	GdkRGBA rgba = new GdkRGBA ();
	String [] propertyParsed = new String [1];
	propertyParsed = property.split (";");
	OS.gdk_rgba_parse (rgba, Converter.wcsToMbcs (null, propertyParsed[0], true));
	return rgba;
}
#+END_SRC

The comment suggests that this method only works on rgb and rgba color values,
not hexadecimal.  Changing the values in the gtk-main.css to rgb color values
has no effect.

(I wish I could add a breakpoint into the code I'm seeing Eclipse).

Maybe it's using default colors somehow?  These are the calls to get the default
foreground and background colors:

: styleContextGetColor (context, OS.GTK_STATE_FLAG_NORMAL, rgba);
: getBackgroundColor (context, OS.GTK_STATE_FLAG_NORMAL, rgba);

The first one calls into native code.  The second draws a surface and look at
the color in it.

Oh wait: I /can/ put breakpoints into the code.  That's going to be much
simpler.

Okay so the CSS Eclipse is looking at to determine the colors is... not the one
I was modifying.  It's much larger, and has rule declarations instead of only
@define-color calls and @import statements.

Unfortunately, the debugger is unable to give me the full value.  In the part
I've managed to extract, I don't see any comments.  That might indicate the file
was generated.

So this file apparently contains a ~.tooltip~ declaration, since that's a hit
for the code in Display.  When it gets to it, here is what it finds:

#+BEGIN_SRC css
.tooltip {
  border-bottom-left-radius: 5px;
  border-bottom-right-radius: 5px;
  border-top-left-radius: 5px;
  border-top-right-radius: 5px;
  box-shadow: none;
  color: rgb(255,255,255);
  padding-bottom: 4px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 4px;
  text-shadow: 0 1px rgb(0,0,0);
}
#+END_SRC

That's coherent with the white foreground.  For background:

#+BEGIN_SRC css
tooltip.background {
  background-clip: padding-box;
  background-color: rgba(0,0,0,0.8);
  border-bottom-color: rgba(255,255,255,0.1);
  border-bottom-style: solid;
  border-bottom-width: 1px;
  border-image-repeat: initial;
  border-image-slice: initial;
  border-image-source: initial;
  border-image-width: initial;
  border-left-color: rgba(255,255,255,0.1);
  border-left-style: solid;
  border-left-width: 1px;
  border-right-color: rgba(255,255,255,0.1);
  border-right-style: solid;
  border-right-width: 1px;
  border-top-color: rgba(255,255,255,0.1);
  border-top-style: solid;
  border-top-width: 1px;
}
#+END_SRC

So the question now is: where is this CSS coming from?  It's not from the theme
I've specified, and not from the user CSS file.  Maybe it's a file used by
Eclipse?

It is requesting the "Adwaita" theme by name.

Okay so I've dumped the CSS that Eclipse gets from GTK to disk.  I still have no
clue how it's constructed, and how it sets the color values for tooltips.

Looking at the GTK documentation:

#+BEGIN_EXAMPLE
In addition, certain files will be read when GTK+ is initialized. First, the
file $XDG_CONFIG_HOME/gtk-3.0/gtk.css is loaded if it exists. Then, GTK+ loads the
first existing file among XDG_DATA_HOME/themes/theme-name/gtk-VERSION/gtk.css,
$HOME/.themes/theme-name/gtk-VERSION/gtk.css,
$XDG_DATA_DIRS/themes/theme-name/gtk-VERSION/gtk.css and
DATADIR/share/themes/THEME/gtk-VERSION/gtk.css, where THEME is the name of the
current theme (see the “gtk-theme-name” setting), DATADIR is the prefix
configured when GTK+ was compiled (unless overridden by the GTK_DATA_PREFIX
environment variable), and VERSION is the GTK+ version number. If no file is
found for the current version, GTK+ tries older versions all the way back to
3.0.

In the same way, GTK+ tries to load a gtk-keys.css file for the current key theme, as defined by “gtk-key-theme-name”.
#+END_EXAMPLE

If we actually look in these folders, the gtk.css for Adwaita is empty, since
it's the default theme.  Presumably, all is implemented in the code.

I don't have ~XDG_CONFIG_HOME~ set, but I suspect the user file is still getting
read, since it's modifying the tooltip colors for other parts of Eclipse.

Okay, what about pointing to a custom theme?  Will it follow the CSS then?

Creating a Foo theme and setting as default, with this gtk.css:

#+BEGIN_SRC css
.tooltip {
  color: rgb(91, 91, 91);
}

.tooltip.background {
  background-color: rgb(230, 230, 230);
}
#+END_SRC

In Eclipse, the CSS dump for Foo is as huge as Adwaita's.  Tooltip values are
the same as well:

#+BEGIN_SRC css
.tooltip {
  border-bottom-left-radius: 5px;
  border-bottom-right-radius: 5px;
  border-top-left-radius: 5px;
  border-top-right-radius: 5px;
  box-shadow: none;
  color: rgb(255,255,255);
  padding-bottom: 4px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 4px;
  text-shadow: 0 1px rgb(0,0,0);
}
#+END_SRC

Something is fishy with this GTK function, and I can't find a way to influence
the values.

Looks like I have to switch to Oxygen.  Oh well.

Oxygen fixed it... but not the colors in the Content Assist.  I understand
that's a fresh commit that should go in the release.

* [2017-05-03 mer.]
** Fixing the empty tabs in the EMF Views editor                   :emfviews:
So: loading an eviewtype, I get a MultiPageEditor with 3 pages.  Page 2 and 3
are blank.  After I load the XMI file in the same folder, the two pages have
content.

Trying to add a very simple page to the MultiPageEditor with a single button:
switching to this page, the button exists.

Trying to add a more involved page with a ScrolledForm and a TreeViewer:
switching to this page, it is blank.

However, using setActivePage to point to this new page, the page is indeed
constructed and visible, and all the other pages are blank, except the first
one.

Using setActivePage to point to the properties page: it is visible on launching
Eclipse and after switching to and back from other pages.  First page is
visible, other two pages are not.

The common factor in the invisible pages seem to be that they use a
ScrolledForm.

I just need a Control to put into a page.  A ScrolledForm is a Control, but a
Composite is a Control as well.

Trying to put a button and a TreeViewer in a Composite: grey page.

If I add a layout to the composite:

: comp.setLayout(new GridLayout());

Now it's displayed.  Same thing with a ScolledForm actually: adding the
setLayout to its Composite returned by getBody will make the widgets visible
when switching to the page.

But creating the ScrolledForm through the FormToolkit, that doesn't work.

Need to look into forms and editors.

* [2017-05-05 ven.]
** Eclipse forms and editor bug                            :eclipse:emfviews:
Reading: https://www.eclipse.org/articles/Article-Forms/article.html

This is a good (albeit dated) resource on what Eclipse Forms are useful for.  It
seems PDE uses them extensively for editing the plugin.xml file for instance.
So now I understand what the EViewType editor tries to emulate.

Since we are trying to build a multi-page form, it seems the preferred way is to
extend FormEditor rather than MultiPageEditor.

Found [[http://git.eclipse.org/c/platform/eclipse.platform.ui.git/plain/examples/org.eclipse.ui.forms.examples/src/org/eclipse/ui/forms/examples/internal/rcp/FreeFormPage.java][an example]] of using FormEditor and multiple FormPage.

Doing that seems to fix the blank page bug.  I just converted the code that
created the forms to FormPage inner classes.  It's not as clean as I'd like,
since there seem to be weird explicit dependencies between models and views.
Considering that all three pages must stay in sync, it would seem that changing
the model and listening to changes would be cleaner.

But, I also don't know if we want to keep the editor in this current form.  So
that fix should do it for now.

** More dead code                                                  :emfviews:
Looking around, I see there are two classes that are not used: FormComposite, a
small utility class to adapt a composite with a toolkit, and Overview, which
seem to be the properties view as it existed before.

Of importance, the Overview class takes care of using text strings pooled from a
text file for the UI, so they can be translated.  But I can't seem to succeed in
including the page in the editor, so...  Dead code.

* [2017-05-09 mar.]
** Fixing the TreeViewer in the Viewtype editor                    :emfviews:
So, first of all, there was the issue of dirtying the state of the editor when
switching to the TreeViewer.

While trying to understand what the page is for, it hit me that a few things are
not working properly: some boxes should be ticked, and selected elements should
be expanded as well.

In addition, I see that basically all objects can be expanded in the TreeViewer,
even when they don't have any children.

Luckily, I've already done the TreeViewer tutorial from the book.

So, we are reusing the ContentProvider from our Viewtype creation wizard.
Basically, we delegate to the EMFContentProvider to display models, except we
add another layer of EPackage.

Rewriting the ContentProvider getChildren and hasChildren to be in sync fixes
the issue with empty children showing a twistie.

But we expand/tick/reveal calls still have no effect.

Debugging a reveal call indicates that it returns null internally.

So I can get the treeViewer to reveal/tick/expand elements when I build the
elements array myself.  I guess what we get from viewtype.getHiddenAttributes
are not elements related to the input of the tree.

Wait a minute.  The tree is not complete in the view... Looks like the models we
display are the one from the virtual resource set of the viewtype... so the
items are already filtered out.  The code is trying to tick the exact same
items, so of course that shouldn't work.

Not removing the items in the resource set... well, that still doesn't tick
them.  But I think that's because the virtual resource set contains clones of
the models, and the hidden attributes are objects of the base resource set, so
maybe they cannot be equal to one another.

Ah!  Yes, that's it.  After putting the originals in the virtual resource set
and not the copies, the ticks appear.  But the containing classes are not
expanded.

Which is weird, because we construct the array of containing class based on the
hidden attributes.  So if the hidden attributes are ticked because the tree
contains the same objects, it should also contain the right classes.

Hmm maybe we are changing the expanded elements at another point.  We do, in
pageChange.

But commenting that does not help.

Wait, after adding breakpoints to step through the code that sets the expanded
state internally in AbstractTreeViewer, it does work!  Shenanigans.  I guess the
hot code replace can get confused by my changes, without warning me.  This is
unfortunate.

Also, the code was trying to set the expanded state on a tree item below the top
level.  According to the documentation, this should work: expanded states are
saved even if you close the parent, so when you reopen the parent, the expanded
children should still be open.

However, if I manually do it in the tree, that's not the behavior I observe.
Expanding a child, and toggling its parent: the child is closed.  Looks like
closing an item closes its children recursively.  What works is to reveal the
elements.

Okay so now, I still haven't solved the mysterious dirtying of the editor when
opening the Contents page.

There's an editorDirtyStateChanged which I can override.  But it doesn't get
called.

Okay, found it.  The AttributeSelectionAdapter is changing the model every time
we select something.  That's overkill, and wrong.  We should only update the
model when there are actual changes in the tree: that is, when we check/uncheck
elements.

But I'm not fixing it right now.  Better focus on cleaning up the core of
EMFViews: the virtual model.  I'm leaving notes to know that these parts of the
code are busted.

** Notes on EMFViews core                                          :emfviews:
What is referred to as the Weaving model in the paper is our VirtualLinks
package.  It's a collection of links between models and metamodels.  For the
moment, we have two kind of links: associations and filters.

Associations are virtual references.

Filters hide attributes from models.

We don't have virtual attributes or virtual classes, but that might be something
to improve upon (the Extension paper was a step in that direction).

The virtual models are realized by the emfviews.core package.  There we have
Viewtype (a viewpoint in the paper), View and EView.

We have two levels: the viewtype describes the links between contributing
metamodels, and the view describes how to construct a virtual model from
contributing models (which are instances of the contributing metamodels of the
viewtypes).

Practically speaking, when opening an EView file with an ECore editor, EMF will
display the virtual model.  This is achieved by registering EMFViewsFactory as
the parser class in an extension point.

This class then creates an EView and Viewtype, depending on the file extension.
These two files extend ResourceImpl, so they can be transparently used as
resources by EMF.

How being a resource helps in displaying a model, I still have to find out.

H pointed out some redundancies in the EView file: we specify the ECL file, but
we don't need it.  It's already in the EViewtype, which is also specified, and
in any case we only need the XMI describing the weaving model.  The ECL file is
used to generate the weaving model, but once we have one, we don't need the ECL
anymore.  And in any case, we can also provide the weaving model XMI manually.

Same thing in the EViewtype: the examples have an ECore file that contains the
hidden attributes.  These should be part of the XMI, which already register the
association virtual links.

* [2017-05-10 mer.]
** Renaming VirtualLinks package and freshening up the model       :emfviews:
Annoyingly, we had a package named fr.inria.atlanmod.emfviews.virtualLinks.  Note
the camelCase.

Since this package is generated by EMF, better to update the model directly.
I'm not quite sure what to make of the namespace URI we have:

: http://inria.fr/virtualLinks

But I know that changing it will break existing serialized XMI files, so that's
probably a bad idea for the time being if I want to run the examples, without
having to change the namespace there as well.

** Testing out containment references in EMF                            :emf:
In a language where objects are allocated on the heap, I did not understand the
use of containment references.

Turns out, a containment reference and vanilla reference both include a list of
target objects.  The list implementation are different classes, but I don't see
anything vastly different about them.

According to the EMF bible, an object B that has a container can only be in one
container at the time.  Changing its container will remove it from the previous
container.  That's neat.

But doesn't that apply to bidirectional references with a single multiplicity at
one end as well?

Looks like it does.  So, a containment reference is equivalent to a
bidirectional reference with multiplicity 1 at one end.  One difference is that
to make a bidirectional reference in ECore, you have to create two references.

But the major difference since to lie in the serialization.  Contained targets
will be serialized in the same resource as the container, whereas vanilla
references are serialized in different resources.  In other words, it does what
you expect when you consider storage.  A quick test reveals that indeed,
contained objects are saved in the same file as their parent, whereas with
vanilla references you are dealing with multiple files.

So, for simplicity, and simplified management of serialized resources,
we should prefer containment references.

** Trying to use the MoDisCo model browser                         :emfviews:
Because the model browser provided by ECore is very basic, and does not follow
references.  The MoDisCo browser does.

However, trying to open the examples XMI with it, I am greeted with familiar
errors concerning unknown features from the TOGAF metamodel:

: !MESSAGE org.eclipse.emf.ecore.xmi.FeatureNotFoundException: Feature 'belongsTo' not found. (platform:/resource/1_EAdata/models/1_travelAgencyEA.xmi, 26, 270)

I slayed this dragon previously, and I kept my custom TOGAF plugin.  Re-using
it...

Okay, it works.  I can follow recursive links in the model.  It's... not very
impressive, or convincing, but it works.  The Modisco browser is dog slow
however.

** Following the code                                              :emfviews:
After the EmfViewsFactory delegates to either the EView or Viewtype constructor,
what happens?

Well, nothing at that point: the constructor returns a resource.  That resource
is only loaded when its doLoad method is called.  That's where the magic
happens.

In Viewtype.doLoad, we parse the eviewtype file, load the models, and create the
virtual resource set.

The correspondenceModelBase seem unused at this point.  The Viewtype only uses
it in serialize()... except this function is never used.  The actual
serialization happens in the doSave method of the resource.

First we load the filter metamodel: the ECore file that should not be here.

Then we load the contributing metamodels.  We get the corresponding package, and
each package is cloned and kept in contributingEPackages.

Then, we first filter out any attributes as specified by the filter metamodel.
If the package matches one in the filters, we remove every structure from common
classifiers.

There are two issues currently: we loop through all filtered packages, for
each contributing package.  That's unnecessarily quadratic.

Second issue: we only seem to care about EClass classifiers with an unchecked
downcast.  So I'm pretty sure that's an exception if the filter metamodels
contains an Enum or Datatype.

Hmm, it crashes, but not where I expected.  Actually, I see other downcasts in
the same method, and they seem unsafe, but no warnings.  Need to check that as
well.

*** TODO Check copyright/contributions
Copyright is attributed to Inria in some places, Atlanmod in others (or should
it be AtlanMod?).

Copyright years are 2013,2014 tops.

Copyright notice should maybe be generated for VirtualLinks.

Copyright notice should be included in all packages.

AtlanMod should be the provider of all packages.

*** TODO Investigate dependencies
Some EMF plugins depends on other EMF plugins, some depend on EMF packages
directly.  The book I read on Eclipse plugin development recommended to depend
on packages.

*** TODO Change the logo background to be transparent
This is unnerving.

* [2017-05-12 ven.]
** Downcasts in Java are "safe"                                        :java:
Because there will be runtime checks...  I was under the impression the compiler
would complain, but it's just in case of unchecked cast with generics.  Because
of type erasure, the compiler cannot insert a runtime check (a List is still a
List), so the warning is to make sure you know what you are doing.

Otherwise, Java assumes you do know what you are doing with straight downcasts
from A to B, even though the compiler only knows that this downcast /could/
work (if B :< A).

That's disappointing.

Is there a linter out there that could at least pick up downcasts so I could
review them?  FindBugs [[http://findbugs.sourceforge.net/bugDescriptions.html#BC_UNCONFIRMED_CAST][appears to]].

The Eclipse plugin is a bit rough, but it does report the stupid downcast from
my test code.  However, it does not report the troubling downcasts in EMFViews.
So, more trouble than it's worth.

** Mysterious crash when loading funky ECore file                  :emfviews:
So adding other classifiers (EEnum, EDataType) to the Ecore file containing our
filters and opening the Eviewtype with the ECore editor results in 3 thrown NPE.

The puzzling part is that, in all of the stack traces, our code is not on the
stack.  Maybe we implement something wrongly.

Adding a breakpoint shows Viewtype.getContents is called and returns the null in
question.  But since the null value is used by ResourceImpl, that's where the
NPE is thrown.

Anyway, we only set virtualContents after doLoad() has completed.

Hmm, I see!  Stepping through again, and in fact ResourceImpl.load wraps our
doLoad with a try/finally, but no catch.  So we do throw a cast exception due to
the presence of other classifiers!  But that was masked by ResourceImpl.
Sneaky.

** EMFViews archeology                                             :emfviews:
So, H found the original demo paper along with the initial prototype
implementation of EMFViews (then called VirtualEMF).  The novel idea at the time
was to have a /virtual/ model, that composed multiple contributing models.  The
virtual model is lazy: attributes are proxies to the concrete models, and the
virtual attributes are synthesized on-demand.

In the code, you can find a VirtualModel class that's absent from the current
version.  That's because at the time, only models were virtualized, not
metamodels.  But, the same virtualization approach can be applied to metamodels,
since they can be viewed as models as well; hence EMFViews.  In EMFViews, we
have Viewtype which should be the equivalent of VirtualModel for metamodels, and
View, which would be closer to the original VirtualModel.

Looking at the rest of the code, everything in emfviews.elements seem very
similar to the first version.

In emfviews.rules, the MergeRule was severely cut.

In emfviews.core, the MetaModelManager was mostly changed.  The
VirtualLinkManager was slightly changed, and that's it (other than added/removed
files).

** Further code investigation                                      :emfviews:
Now I'm in Viewtype.loadCorrespondenceModel.  The correspondenceModel is the XMI
file that describe the VirtualLinks: it gives us the info we need to compose the
contributing models (and in this case, metamodels).

There are two kinds of links actually: Filter and Association.  But filter links
are not currently used in this path of code; the filters are specified in a
separate ECore file which is used in the loadFilterMetamodel phase.

So the code is concerned only with Association links.  For each Association, we
synthesize an EReference with the Association attributes (source, target,
lower and upper bounds) and add it to the EClass in which it resides (in the
virtual packages we created earlier).

Ultimately, in Viewtype.setVirtualContents, we turn the EPackage from our
virtual resource set into a VirtualContents object (which is just an EList).  I
had looked at VirtualContents before: it's a curious implementation of an EList
from a list of lists, which only purpose seem to be to simulate a flat list:

#+BEGIN_SRC java
public E get(int index) {
    if (index >= 0) {
      for (List<E> l : subLists) {
        if (index < l.size()) {
          return l.get(index);
        } else {
          index -= l.size();
        }
      }
    }
    throw new IndexOutOfBoundsException();
  }
#+END_SRC

I'm assuming this is done because getContents requires an EList.  But, then, why
not flatten the lists once and for all?  The VirtualContents list seem to be
read-only, since the set method is implemented by a call to super
which... throws UnsupportedOperation.

** Open questions                                                  :emfviews:
- Is Viewtype creating a truly virtual metamodel?  It doesn't seem to do any
  demand-loading, but maybe that's behind the scenes.  Should compare with what
  View/EView does for models, or what VirtualModel did in the first prototype.

- There are still a bunch of files in the core, are they used by View/EView or
  not?  MergeRule, TranslationRule, etc.

* [2017-05-15 lun.]
** Is Viewtype proxying metamodels?                                :emfviews:
To me it seems that no, it just plain clones them into the virtual resource
set.  This is done in loadContributingMetamodels:

#+BEGIN_SRC java
EPackage contributingEcoreModelPackage = EPackage.Registry.INSTANCE.getEPackage(modelURI);

Copier copier = new Copier();
EObject copy = copier.copy(contributingEcoreModelPackage);
copier.copyReferences();
EPackage copiedPackage = (EPackage) copy;
EcoreUtil.remove(copiedPackage);
contributingEpackages.add(contributingEcoreModelPackage);
#+END_SRC

Regardless of whether there are filters, we clone the packages.  Then, if there
are filters, we remove the attributes from these copies.

: eClassWithItemsToHide.getEStructuralFeatures().remove(theAtt);

Then, if there are associations, we add EReferences to these copies:

#+BEGIN_SRC java
EReference theR = EcoreFactory.eINSTANCE.createEReference();
theR.setName(association.getName());
theR.setLowerBound(association.getLowerBound());
theR.setUpperBound(association.getUpperBound());
theR.setEType(theTargetEClass);
...
theSourceEClass.getEStructuralFeatures().add(theR);
#+END_SRC

So, is this different from how View/EView work?

** Investigating View/Eview                                        :emfviews:
Stepping through the code.  When we load an eview file, we trigger EView.doload.

First thing is to read the file, and create a Viewtype resource from the
compositionMetamodel line.  We are creating a whole new Viewtype (and copying
packages), just for the EView.  If a Viewtype is a virtual metamodel, we should
be able to locate it from the registry, and create it only if it does not exist.

I'm wondering if the EView/View split is the half-finished result of trying to
abstract the common parts of EView and Viewtype into a common abstract class.
But at the moment, EView is the sole subtype of View.

In EView, we then load the View.contributingMetamodels.  This merely register
the metamodels in the virtualResourceSet of View.  But this virtual resource set
is different from the one held by Viewtype.  At this point, the metamodels are
not modified.

Then we create a MetamodelManager.  This one populates a bunch of Maps.  A map
of composition classes keyed by their names; these are taken from the contents
of the constructed Viewtype.  Then a map of all the EClass of the contributing
metamodels, again keyed by their names; these are taken straight from the
classifiers of the contributing metamodels.

Then a map of concrete to virtual classes.  That's interesting:

#+BEGIN_SRC java
for (List<EClass> lcec : contributingClassesByName.values()) {
  for (EClass cec : lcec) {
    List<EClass> lvec = compositionClassesByName.get(cec.getName());
    for (EClass vec : lvec) {
      if (vec.getEPackage().getNsURI().equals(cec.getEPackage().getNsURI())) {
        this.concreteToVirtualClass.put(cec, vec);
        mapFeatures(cec, vec);
      }}}}
#+END_SRC

The "virtual EClasses" (vec) that are put into the map are pulled from
coompositionClassesByName, and used as values keyed by the corresponding class
in contributingClasses.

mapFeatures does the same mapping, but for structural features, recursively:

#+BEGIN_SRC java
private void mapFeatures(EClass concEC, EClass virtuEC) {
  for (EStructuralFeature feature : concEC.getEStructuralFeatures()) {
    EStructuralFeature vf = virtuEC.getEStructuralFeature(feature.getName());
    if (vf != null) {
      this.virtualToConcreteFeature.put(vf, feature);
      this.concreteToVirtualFeature.put(feature, vf);
    }}}
#+END_SRC

Now we have a bidirectional map.

Lastly, there may be additional features in the virtual classes (created by the
associations), so we also record them in a map of virtualAssociations, but only
if they were not present in virtualToConcreteFeatures:

#+BEGIN_SRC java
for (List<EClass> lec : compositionClassesByName.values()) {
  for (EClass ec : lec) {
    for (EStructuralFeature sf : ec.getEStructuralFeatures()) {
      if (virtualToConcreteFeature.get(sf) == null)
        if (virtualAssociations.get(sf.getName()) == null) {
          List<EStructuralFeature> sfs = new ArrayList<>();
          sfs.add(sf);
          virtualAssociations.put(sf.getName(), sfs);
        } else {
          virtualAssociations.get(sf.getName()).add(sf);
        }}}}
#+END_SRC

After that we are back in EView, and that's it for the metamodels.  Now we
loadContributingModels:

#+BEGIN_SRC java
protected void loadContributingModels(List<String> contributingModelsPaths) {

  for (String modelURI : contributingModelsPaths) {
    virtualResourceSet.getResource(URI.createPlatformResourceURI(modelURI, true), true);
  }

}
#+END_SRC

Which just seems to force the loading of each model, without doing anything with
the returned resource (why?).

If there is a correspondenceModelBase we... don't do anything with it (yet)?  We
get the correspondence XMI, create a VirtualLinksDelegator for the
correspondenceModelBase, and let the delegate create the links:

#+BEGIN_SRC java
if (properties.getProperty("correspondenceModelBase") != null) {
  IWorkspace workspace = ResourcesPlugin.getWorkspace();
  java.net.URI linksModelURI = workspace.getRoot()
      .findMember("/" + properties.getProperty("correspondenceModel")).getLocationURI();
  try {
    VirtualLinksDelegator vld =
        new VirtualLinksDelegator(properties.getProperty("correspondenceModelBase"));

    vld.createVirtualModelLinks(org.eclipse.emf.common.util.URI
        .createURI(linksModelURI.toString()), getContributingModels());
#+END_SRC

In this case, it creates an EclDelegate.  In
EclDelegate.createVirtualModelLinks, we open the ECL file and first parse the
aliases in the header.

Here is a sample ECL file from the examples:

#+BEGIN_EXAMPLE
//alias_ea=http://www.obeonetwork.org/dsl/togaf/contentfwk/9.0.0
//alias_bpmn=http://www.omg.org/spec/BPMN/20100524/MODEL-XMI
//alias_reqif=http://www.omg.org/spec/ReqIF/20110401/reqif.xsd

rule detailedProcess
match s : ea!Process
with  t : bpmn!Process
...
#+END_EXAMPLE

I think the intent here is pretty clear: to define ~ea~, ~bpmn~ and ~reqif~ as
aliases for the metamodels in the header.  Still, it would be better to have ECL
support these kinds of declarations rather than hack a parser with indexOf
calls.

[H: usually you'll run ECL with a launch configuration file, specifying the
aliases.  Here it's inlined.  Maybe there is away to provide a launch
configuration at runtime, but it's not really important.]

In any case, we populate two maps keyed by the aliases: one to the resource of
the metamodel, and one to the package URI.  No provisions are made if we don't
find a corresponding resource.

After that, we close the ECL file because we let ECL parse the rest.  Then we
add instances of EmfModel to the model repository of the Ecl module.

Then, we executet the ECL module, and iterate on the resulting MatchTrace in
order to create virtual links for each matching trace:

#+BEGIN_SRC java
for (Match match : matches) {
  if (match.isMatching()) {
    EObject left = (EObject) match.getLeft();
    EObject right = (EObject) match.getRight();

    Association vAsso = vLinksFactory.createAssociation();
    vAsso.setName(match.getRule().getName());
    vAsso.setAssociationTypeName(match.getRule().getName());
    vAsso.setLowerBound(0);
    vAsso.setUpperBound(1);

    LinkedElement lSource = vLinksFactory.createLinkedElement();
    lSource.setModelRef(left.eClass().getEPackage().getNsURI());

    lSource.setElementRef(left.eResource().getURIFragment(left));
    vAsso.setSourceElement(lSource);

    LinkedElement lTarget = vLinksFactory.createLinkedElement();
    lTarget.setModelRef(right.eClass().getEPackage().getNsURI());
    lTarget.setElementRef(right.eResource().getURIFragment(right));
    vAsso.getTargetElements().add(lTarget);

    virtualLinks.getVirtualLinks().add(vAsso);
    virtualLinks.getLinkedElements().add(lSource);
    virtualLinks.getLinkedElements().add(lTarget);
  }
}
#+END_SRC

After that, we save the populated virtualLinks to the XMI file.

So, it seems we always recreate the XMI file from the ECL.

[H: that may not be ideal, but models can be updated, so you usually want your
view to synchronize with these changes by default.  Here we run the ECL query
again.]

Back in EView, we now create a VirtualLinkManager, given the correspondence
model URI (the XMI).  The manager merely holds a reference to both the EView and
the VirtualLinks instance from the XMI.

Then the VirtualLinkManager is initialized, which creates a LinksProjector.
There, for each Assocation in the XMI, we get a virtual element from the
VirtualLinkManager corresponding to the source element of the association, and
we link the target elements to it:

: vElement.setVirtualAssociation(virtualFeature, EStore.NO_INDEX, targetElements);

After that, we set the virtual contents of our EView resource, by translating
each package of the contributing models to virtual elements.  Creating virtual
element happens in VirtualLinkManager.getVirtualElement:

#+BEGIN_SRC java
public EObject getVirtualElement(EObject e) {
  VirtualElement vElem = virtualLinks.get(e);
  if (vElem == null) {
    vElem = new ReproduceElementImpl(virtualModel, e);
    virtualLinks.put(e, vElem);
  }
  return vElem;
}
#+END_SRC

and ReproduceElement uses a ReproduceRule, which implements an EStore... and
that's probably where the secret virtualization sauce lies.  But it already
looks like there is much more happening in EView/View concerning virtualization,
and I didn't see any copying taking place.

So my premature answer is: the Viewtype is not virtualized as the Views are.
Which was kind of the point of EMFViews.  That should be fixed in priority.

* [2017-05-16 mar.]
** Reading the EMF bible                                                :emf:
To get a clearer picture of the concepts at hand.

Questions still open after reading the relevant chapters:

- Can we read a UML model and access it using the
  EPackage/EClass/EAttribute/... interfaces?

- Is demand-loading and demand-creating for resources lazy, eager, or something
  else?  Specifically, the createResource and getResource methods accept a
  boolean argument: does it forces resolution or rather delays it?

- It seems, at least for references, that EMF already does some
  auto-proxification.  What is the mechanism we use in EMFViews
  (ReproduceRule?), and how does it compare?

  If we use "Dynamic EMF" as it's called in the book to create our view
  packages, would we not benefit from proxification?

And an observation:

EMFViews add copies of contributing model packages to a registry local to the
virtual resource set of a Viewtype.  But then, the EView does not tap into this
virtual resource set, so there's duplication here.

* [2017-05-17 mer.]
** EMFViews uses an EStore                                         :emfviews:
A VirtualElement inherits from an EStoreEObjectImpl, which is an EObject
implementation backed by an EStore.  Then, our translation rules are all
different EStore implementation.

From what I gather, this is where the actual magic for models happen (and this
was part of the initial implementation back in 2011).

When we load a model, the very last step of EView.doLoad is to set the virtual
contents:

#+BEGIN_SRC java
for (Resource r : contributingModels) {
  ArrayList<EObject> oneOftheSublists = new ArrayList<>();
  oneOftheSublists.add(translateToVirtualElement(r.getContents().get(0)));
  sublists.add(oneOftheSublists);
}

this.virtualContents = new VirtualContents<>(this, sublists);
#+END_SRC

This populates lists with virtual elements, which are obtained from the virtual
link manager:

#+BEGIN_SRC java
public EObject getVirtualElement(EObject e) {
  VirtualElement vElem = virtualLinks.get(e);
  if (vElem == null) {
    vElem = new ReproduceElementImpl(virtualModel, e);
    virtualLinks.put(e, vElem);
  }
  return vElem;
}
#+END_SRC

That's where reproduce elements are instantiated.  (And, interestingly, only
reproduce elements; MergeElement and FilterElement do not seem to be created
anywhere)

A reproduce element is a virtual element, so an EStoreEObjectImpl, and holds a
concrete EObject called the concrete element.  The idea is to pass through
access to the concrete element using the EStore interface.

At the end of creating a reproduce element, this is what happens in init:

#+BEGIN_SRC java
this.eProperties().setEResource(vModel);
this.concreteElement = concreteElement;
this.eSetClass(eClass);
this.eClass();
setTranslationRule(ReproduceRule.INSTANCE);
eSetStore(this.getTranslationRule());
#+END_SRC

We create a reproduce rule, which implements EStore, and will capture get/set
calls on this virtual object.  That's why, in ReproduceRule.get:

#+BEGIN_SRC java
public Object get(InternalEObject object, EStructuralFeature feature, int index) {
  ReproduceElementImpl vElement = (ReproduceElementImpl) object;

  View vModel = (View) vElement.eResource();
  if (vModel.getMetamodelManager().isVirtualAssociation(feature)) {
    return vElement.getVirtualAssociation(feature, index);
  }
  EStructuralFeature cFeature = vElement.getConcreteFeature(feature);
  Object value = vElement.getConcreteElement().eGet(cFeature);
  ...
  return value;
#+END_SRC

We ultimately return the concrete value.  But not in every case:

#+BEGIN_SRC java
if (feature instanceof EReference) {
  if (feature.isMany()) {
    value = new VirtualModelList<>(object, feature, Arrays.asList((List<EObject>) value));
    if (index != NO_INDEX) {
      value = ((VirtualModelList<EObject>) value).get(index);
    }
  } else {
    value = vModel.translateToVirtualElement((EObject) value);
    if (value instanceof FilterElement) {
      value = null;
    }
  }
}
#+END_SRC

If the requested structural feature is an ERef, and it's many, we return a
virtual list.  Ultimately, inside this virtual list, we will call
getVirtualElement.  If the ref has a single multiplicity, then we can directly
return the virtualElement.

In essence, we perpetuate the virtualization recursively.

It seems to be this part of the code is mixing concerns.  There is a test for
FilterElement here, to mask the value if it should be filtered.  But then we
also have the same test in the VirtualModelList.  Why does the virtual list
repeats this instead of delegating to single virtual elements?

Besides, it seems to me we should have a clear mapping from the Ecore model to
the virtual model, defined for all classifiers and features.

** Using a code coverage tool to find hot/cold code        :emfviews:eclipse:
Following T's recommendation, I used the EclEmma plugin, which is based on
JaCoCo.

Installation was painless.  The plugin supports coverage for running client
Eclipse application, which is my use case.

So now I can answer with certainty that, opening EView and EViewtype files with
a model browser and the viewtype editor, the following classes are never used:

- MergeRule, MergeElementImpl
- FilterElement

In other classes, besides what I already identified as unused, it seems we have
no examples using the sourceAttribute and targetAttribute of a Association.
Maybe there are superseded by sourceElement/targetElements.

* [2017-05-19 ven.]
** How are virtual model attributes filtered out?                  :emfviews:
H raised an interesting question: if an attribute is filtered out in the virtual
metamodel, it is also filtered out in the virtual model.  But how does that
happen?

Does EMF just disregard attributes that are not in the metamodel?  Do we also
need to filter the attributes from the model?

I'm guessing it's the former.  If I comment out the filtering attributes part in
the metamodel, they should appear on the model.

Yes, they do.

Hmm, when the attributes are present in the metamodel, we add them to the maps
of virtual to concrete features in MetamodelManager.  When the attributes are
absent, they are absent from the maps as well.  That's a hint.

* [2017-05-22 lun.]
** Writing tests for EMFViews before refactoring                   :emfviews:
I've got a couple of easy refactorings ahead, related to the EView and EViewtype
files.  But, before that, I want to write some tests to ensure I don't break any
functionality doing so (at least, any functionality we care about).

One problem with writing tests is the way Viewtype and View are written as
resources, you have to provide files through URI, otherwise you cannot construct
them properly.

We could refactor Viewtype and View so that the resource-specific code is
extracted, and calls into a model-specific part that does not have to deal with
files.  But that would be refactoring in order to write the tests for the
/other/ refactoring...

I'll try to write the tests passing files as URI first.

Okay, hit a snag: I'm using URI.createPlatformResourceURI to pass a
workspace-relative filename to the EViewtype file, and it doesn't work.
Presumably, because when I run the code there is no workspace!

So rather I should just use relative paths.  This works:

: URI.createURI("models/foo.eviewtype")

and this will look up the "models" directory in the current project, so it's
relative.  Hopefully that slash is portable as well.

Grmbl, now Viewtype tries to load the filters metamodel.  But it also uses
URI.createPlatformResourceURI, which in turn will use the resource factory
registry to find out how to create an Ecore.  But running in the tests, this
factory is empty:

: System.out.println(Resource.Factory.Registry.INSTANCE.getExtensionToFactoryMap().isEmpty());
: true

I guess I can populate it myself in the tests.

: Resource.Factory.Registry.INSTANCE.getExtensionToFactoryMap()
:     .put("ecore", new EcoreResourceFactoryImpl());

Now I have to find the correct path to set in the Eviewtype file so that it
loads my Ecore model from the right directory.

At the moment, it fails to find it.  I'm in
PlatformResourceURIHandlerImpl.createInputStream.

Amusingly, after prefixing my URI with 'platform:/resource', this method removes
the prefix

: String platformResourcePath = uri.toPlatformString(true);

Ultimately, it calls EcorePlugin.resolvePlatformResourcePath on this suffix,
which merely looks into its getPlatformResourceMap for the root project in order
to produce a platform-specific file URI...

Let's do this:

: EcorePlugin.getPlatformResourceMap().put("foo", URI
: .createURI("file:///home/fmdkdd/proj/emfviews/tests/fr.inria.atlanmod.emfviews.test/models"));

Yeah, it works!  Is there a way to make it relative at least?

In the end, the file URI calls new File(), passing everything to the right of
':'.  Ah, but that's only the URI for the base folder, and EMF uses it to
resolve the resource path below, and this cannot be relative:

#+BEGIN_SRC java
 public URI resolve(URI base, boolean preserveRootParents)
    {
      if (!base.isBase())
      {
        throw new IllegalArgumentException("resolve against non-hierarchical or relative base");
#+END_SRC

Hardcoded it is then.

Then:

: IllegalStateException: Workspace is closed.

Raaaah, we have code in Viewtype.loadCorrespondenceModel which queries the
workspace.  To construct absolute file URIs, again.  This time, to load the XMI
file of the correspondence model.

At this point, I have three options:

1. Fuck it, and not write tests before doing the changes.  That's not totally
   satisfactory; and I will need to write tests anyway for other changes down the
   line.
2. Make the slightest modifications to Viewtype so that it let us provide the
   proper URI
3. Run the tests as a plugin, so that a workspace is loaded


Now that I think of it, 3 would solve the previous problem as well.

Okay, that's better.  Now I can run the JUnit test using a headless Eclipse, but
still loading a workspace.  In fact, I did have to configure a "test" workspace
where I added the ECL, XMI and Ecore files needed by the viewtype to load.

I'd rather have the tests add these files to the test workspace... or even have the
Viewtype code to load them from anywhere.  But anyhow.

Managed to test the presence of features and absence of filtered features.  Now
to be a bit more thorough.

Hmm, hit a snag when trying to check the models.  All I get are
ReproduceElementImpl instances, so I can't cast them to EClass/EPackage to get
their contents or names.

I could cast to ReproduceElementImpl... but then I wouldn't be testing the
virtual access.

So maybe I'm missing something, and we should access these objects through the
EStore interface.  At least, EMF is able to construct a tree viewer from these
contents, so I should be able to inspect these as well.

*** DONE Fix the URI scheme in Eview/Eviewtype
CLOSED: [2017-06-07 mer. 17:26]
We mix platform URLs with http for finding packages of metamodels.  This is
confusing and complicates the code (there are couple instances of duplication
based solely on different URI schemes).

* [2017-05-23 mar.]
** How does the basic Ecore editor goes through our ReproduceElementImpl? :emfviews:
Because that's how I probably need to iterate on them as well.

In EcoreEditor.createModel, our reproduce rule is called by an iterator
resource.getAllContents(), which goes through all the properties.

The actual text is provided by label providers that are given by adapter
factories... EcoreItemProviderAdapterFactory is where the mapping is done from
Ecore objects (ERef, EClass, etc.) to the actual classes that do the work.

For EObjects, it uses the ReflectiveItemProvider.  Setting a breakpoint at
getText there and opening items in the tree confirms that this is the place.

#+BEGIN_SRC java
public String getText(Object object) {
  EObject eObject = (EObject)object;
  EClass eClass = eObject.eClass();
  String label = format(capName(eClass.getName()), ' ');

  EStructuralFeature feature = getLabelFeature(eClass);
  if (feature != null)
  {
    Object value = eObject.eGet(feature);
    if (value != null)
    {
      return label + " " + value.toString();
    }
  }
  return label;
}
#+END_SRC

It's just using eClass().getName().  Okay, let's try that.

It works!  I managed to test the presence of reproduced elements and virtual
associations, and the absence of filtered elements.  But somehow, I've gotten
some values from eClass.getName(), and some others from casting to an EReference
and using getName():

: assertEquals("ReqIF", l.get(1).eClass().getName());
: assertEquals(e.eClass().getName(), "Process");
: if (c instanceof EReference && ((EReference) c).getName().equals("detailedProcess"))

and I don't understand quite why I need to go to the eClass for some, but not
for others.  For the EReferences, eClass() returns EReference... which I guess
is expected.

In the model, I iterate over the /contents/ of the the BusinessArchitecture
object (presumably, a list of structural features).  For each, I can test:

: assertEquals(e.eClass().getName(), "Process");

This is what the Ecore reflective editor gives me: the name of the Eclass.  But
a "Process" in the model also has a name.  The editor gets it from
getLabelFeature and eGet above.

getLabelFeature search for a plausible feature to use as a label: if it's a
"name" attribute or if it's a String.

So I can do:

: e.eGet(e.eClass().getEStructuralFeature("name"))

to get the name of each Process instance.  Similarly, to get the detailedProcess
ref:

: e.eGet(e.eClass().getEStructuralFeature("detailedProcess"))

At first I was surprised with the results:

#+BEGIN_EXAMPLE
fr.inria.atlanmod.emfviews.elements.ReproduceElementImpl@72b10258 (eClass: org.eclipse.emf.ecore.impl.EClassImpl@6113a26f (name: Process) (instanceClassName: null) (abstract: false, interface: false))
null
null
null
null
null
null
null
null
null
null
#+END_EXAMPLE

Only one reference went somewhere, the others null?

But then I opened the model in Modisco, and it turns out that, yes, only the
first process "Booking a trip" has a detailedProcess that leads somewhere.  All
the others are empty references.

Interestingly, even though it's a reference, we don't get an empty list when
there are no elements, or a list when there is only one element.  We get a list
only for two or more elements.

** Removing the correspondenceModelBase in the EView file          :emfviews:
This should be the simplest task on the list.

With the test written and the coverage tool, I can see that: 1) we never
actually do anything with the correspondenceModelBase in Viewtype (the code is
commented out), and 2) removing the correspondenceModelBase in Eview does not
fail the test.

That's because, if we don't specify a model base, we'll just use the existing
correspondence model.

On the other hand, if we do specify the model base, we rewrite the
correspondence model every time.  Except, when the file does not exist (?!).
Putting an empty file there works, which is a bit... meh.  The limitation is due
to getting the URI from finding the file first, rather than just constructing
the URI without looking if there's a file there.

Using createPlatformResourceURI fixes it.

** Getting filters from the virtual links XMI instead of the Ecore :emfviews:
This is slightly more involved.  At the moment, we filter the elements at the
metamodel level, getting the filters from an Ecore file.

For each contributing metamodel, we:

- copy it to our virtual resource set
- remove any feature matching a filter
- load the correspondence model and add associations

What we should do instead is to get the filters from the virtual links model,
and use them to remove features from the metamodels.

So, for each contributing metamodel, we should:

- copy it to our virtual resource set
- load the correspondence model
- remove any feature matching a filter
- add associations

Although, since we specify the elements to be filtered in a different format, we
must change the matching accordingly.  In the Ecore, we did the matching
structurally.  But in the virtual links metamodel, we have only 3 pieces of data
to match the filter element reference:

: <linkedElements elementRef="//Process" modelRef="http://www.obeonetwork.org/dsl/togaf/contentfwk/9.0.0" name="Process"/>

Clearly, we are losing the hierarchical component here.  Though, maybe
"elementRef" is actually intended to be XPath?  If so, I think it means
"any node named Process", which is not more information than "name", but at
least if it's used as XPath you could be more precise than that I guess.

It seems the element ref is used by the links projector in getReferencedObject:

: r.getEObject(elementRef);

So, no XPath then.  Well, regardless, I think I'll go with a pretty basic scheme
to begin with:

: <linkedElements elementRef="contentfwk.BusinessArchitecture.drivers"
:                 modelRef="http://www.obeonetwork.org/dsl/togaf/contentfwk/9.0.0"
:                 name="drivers"/>

This should give enough information to filter the correct element without ambiguity.

Writing the search was a bit more tricky than expected, due to it being a tree.
Using a queue did it.  Not the most readable code, but it works.

Test pass, elements are filtered on the metamodel and the model.

* [2017-05-29 lun.]
** Small morning refactorings                                      :emfviews:
Removing the unnecessary HashMap as arguments to load and save for a resource.
EMF handles null arguments just fine.

On a side note, there are multiple cases of calling load with an explicit input
stream, but the suggested way to load a resource is to first give it an URI,
then call load(null).

In fact, sometimes we call load with an explicit input stream, /and then/ set
the URI.

Now to rename a few things for better coherence with the paper:

- Viewtype to Viewpoint.  That was a pain, as we had also many example files
  with the eviewtype extension, and files that referred to them...  These
  examples might not be even working anymore for all I know, but hey, coherence.

- "Correspondence model base" to "Matching model".  I guess "correspondence" was
  alright, although different than the paper, but "matching" is shorter.  And we
  had many thing beginning with 'c' already, it was getting Confusing.

At this point I discover projectile-replace: much faster!  Though it doesn't
seem to save files automatically, there is projectile-save-project-buffers;
launching magit-status also prompts to save them individually.  Still leaving
the Java refactorings to Eclipse, since I have more faith in its correctness.

- "Correspondence model" to "Weaving model".  The correspondence model is
  actually just an instance of the virtual links metamodel.  But I suppose
  there's no reason to couple the two, so "weaving" will do (besides, it's
  shorter).

- "Composition metamodel" to "viewpoint", since that's what it is.

- "Contributing (meta)models" to "contributing".  Hmm, this one will wait for
  the unification of EView and Viewpoint, where we will have a single
  "contributingModel" line.  For coherence with the other properties in an
  EView/EViewpoint file, the suffix should stay, at least for the time being.


These morning refactorings went well in the afternoon I guess.

* [2017-05-30 mar.]
** Making Viewpoint a virtual metamodel                            :emfviews:
At the moment, to construct the Viewpoint, we merely clone the contributing
packages into a virtual resource set, then remove filtered attributes and add
references corresponding to virtual associations.

The idea is to use what we have in View already to construct a Viewpoint, by
feeding it Ecore as a metamodel.  First difficulty is that EView refers to a
Viewpoint.  If we use EView to represent a Viewpoint at the metamodel level,
then it would still need a Viewpoint.

What is the Viewpoint used for anyway?

In EView.doLoad, we load a full Viewpoint from the "viewpoint" property.  We
then use this viewpoint to get a reference to the matching model and the list of
contributing metamodels.

This list of contributing metamodels is used by View to populate the virtual
resource set of the View/EView (the attribute is declared by View, instantiated
by EView, and populated by View.loadContributingMetamodels).

It seems similar to what Viewpoint is doing, except we are /not/ copying the
packages we put in the virtual resource set of View.

So at this point, we have a virtual resource set with the original packages,
/and/ a Viewpoint with its own virtual resource set containing clones of the
exact same packages, albeit modified by virtual links.

How are both used?  First, the content of the virtual resource set of View is
only used by View.getContributingModels, that is: to produce a list of the
resources contained by the resource set (but we filter out Ecore resources for
some reason).

This list is used by other getters of View, and also in doSave.  Most
interestingly, its contents are passed unfiltered to the constructor of a
MetamodelManager in EView.doLoad, along with the viewpoint.  Another point of
use of interest is View.setVirtualContents: the (filtered) list is used to
populate the virtual contents with translated virtual elements.

The viewpoint of EView is /only/ used to pass to the MetamodelManager.  So it
seems this is where the link between concrete metamodels, virtual metamodels,
models and virtual models happen.

In fact, the third argument to the constructor of MetamodelManager takes a
reference to the EView instance that created it... the EView holds a reference
to the MetamodelManager, and that's the only class where the manager is
instantiated.  Seems to me they are rather coupled.

MetamodelManager holds maps of concrete to virtual features as I have [[*Investigating View/Eview][previously
covered]].

It uses the EView reference in only one place, this test:

#+BEGIN_SRC java
 if (virtualModel != null && virtualModel.getResourceSet() != null
        && virtualModel.getResourceSet().getPackageRegistry() != null
        && virtualModel.getResourceSet().getPackageRegistry().values() != null
        && virtualModel.getResourceSet().getPackageRegistry().values().size() > 0) {
      Collection<Object> listOfVirtualMMPackages =
          virtualModel.getResourceSet().getPackageRegistry().values();
#+END_SRC

Written in a rather defensive style, this list of virtual metamodel packages is
the same thing that we pass in the first argument to the constructor... which
the constructor collects into a list of EPackage: contributingMetamodels.

Now, coverage for the test I've written tells me that the test returns false
anyway, because getResourceSet() returns null.  So at the moment, we don't do
anything at all with the EView reference.

We do use the other two arguments: the list of (unaltered) contributing
metamodel packages is put into contributingMetamodels, and the viewpoint is used
to populate the compositionClassesByName, where we put the altered metamodel
classes.

Then, in buildMaps, we iterate over each EClass from the contributing
metamodels, and if we find an EClass of the same name, belonging to the same
package, in the map of composition classes (from the Viewpoint), then we add it
to the concreteToVirtualClass map, and iterate on their features.

It's the same thing for mapFeatures: for each concrete feature in the
contributing EClass, if it also exists (by name) in the virtual EClass (the
EClass from the viewpoint), then we add the feature to two maps:
virtualToConcreteFeature and concreteToVirtualFeature.

I'm puzzled by two things: why we use names for comparisons, and why we don't
just iterate on the virtual metamodel.

Using names is brittle, and leads to the redundant checks for classes that
belong to the same package.  Also, is there any guarantee of name uniqueness in
EMF?  It looks like there is: adding a feature or class with the name of an
existing one will fail the validation.  So that's a safe assumption.  We can use
names, but it might be best to have them qualified.

Iterating on the virtual metamodel: since we will only add classes and features
present in the virtual metamodel, and we will add all of them (save for
associations), it might make more sense to iterate on them to start with, and
just get the corresponding class/feature from the qualified name in the
contributing metamodels.

Or, do a parallel descent in the trees.

* [2017-05-31 mer.]
** Re: How are virtual model attributes filtered out?              :emfviews:
Coming back to [[*How are virtual model attributes filtered out?][How are virtual model attributes filtered out?]].  I've established
that when attributes are absent in the viewpoint, they will be filtered in the
models.  But where is the connection taking place?

Stepping into an eGet call to find out where it plugs into our code.

Interesting: an eGet(EStructuralFeature) call is delegated to another eGet,
which looks up the feature ID and delegate to an eGet(int).  But in that one,
the feature ID is turned into ... an EStructuralFeature!  This the exact same
object given to the first eGet call in my debug trace.

After a while, we end up in ReproduceRule.get, and since I'm testing a virtual
association feature, in ReproduceElementImpl.getVirtualAssociation.  In this
case, it's a single reference, so this just virtualizes the target element.

Note: we cache virtualized elements in a map, but EStore also has his own cache
(see isCaching).

In eGet, if the feature is absent from the metamodel (filtered out), then EMF
raises an exception.

When we iterate on the contents of BusinessArchitecture using eContents, we
iterate on the structural features of the eClass.  So this just looks up in the
Viewpoint.

Since EMF uses the structure of the metamodel to iterate on the actual values of
the model, when they are filtered at the metamodel level, they do not appear in
the virtual model.

However, does this mean that there is a way to access these values in the model
if you know the feature name?

* [2017-06-02 ven.]
** Writing a test for modifying models                             :emfviews:
Since we have a virtual model, it should reflect changes in the models, right?
I'm not sure we support that yet, but I figure that there's nothing in the code
that should prevent it.  Caching, maybe.

I've written a small test, and it looks like changing the model does /not/
propagate the changes to the virtual model.

What blocks it?

: vea_labels.get(0).eGet(label_name)

I would expect the first ~get~ to return a proxy to the concrete object, and the
eGet would be delegated to the concrete object.

What's happening: we end up in ReproduceRule.get, where we get the concrete
feature, and the concrete element:

#+BEGIN_SRC java
EStructuralFeature cFeature = vElement.getConcreteFeature(feature);
Object value = vElement.getConcreteElement().eGet(cFeature);
if (feature instanceof EReference) {
  if (feature.isMany()) {
    value = new VirtualModelList<>(object, feature, Arrays.asList((List<EObject>) value));
    if (index != NO_INDEX) {
      value = ((VirtualModelList<EObject>) value).get(index);
#+END_SRC

Since the concrete element is a reference with >1 multiplicity, we create a
virtual list.  Hmm, that means we create a /new/ virtual list every time the a
reference is requested.  Maybe that's how EMF does it as well, providing an
immutable list.  But that does not seem necessary, since it's just a proxy in
this case, we could instantiate it once and save it for further calls, since it
will only delegate to a concrete EList.  Anyway.

We create the virtual list, and if the index is ~> -1~, we return the correct
value, otherwise we return the whole list.

Inside the virtual list, we walk the sublists to get the concrete element:

#+BEGIN_SRC java
EObject concreteEO = (EObject) l.get(k);
EObject virtualEO = virtualModel.getVirtualLinkManager().getVirtualElement(concreteEO);
#+END_SRC

Here, my concrete element is the Label EClass, and the virtual element is a
ReproduceElement containing the concrete element.

Finally, we translate it (again?) to a virtual element before returning
it... hmm.

#+BEGIN_SRC java
return (E) virtualModel.translateToVirtualElement((EObject) l.get(index));
#+END_SRC

Oh, I see.  The first part of the code only wants to find out the true index of
the concrete element, since we can have filter element that should be hidden.
Then, once we have the index, we translate the concrete element to a virtual one
and return it.

So: the ~get(0)~ call returns the Label EClass wrapped in a ReproduceElement.
So far so good.  Except, the concrete element is not the same Label instance as
the one in the concrete model.

Which kind of make sense: to construct the virtual model, we loaded the
resource.  To construct the model, we also loaded the resource from XMI, a
second time.  There's no reason for the instances to be the same.

So what's happening is we are modifying a label instance in memory, but it's
completely disconnected from the instance kept in the virtual model.

The virtual model makes no guarantee to hook into every instance of the model to
watch for changes.  I guess we could use the notifying architecture of EMF..

Our view would be updated if we saved the changes to the model, and recreated
the view... but that's not really an update anymore.

But let's follow the ~eGet~ call.  We end up in EStoreEObjectImpl.dynamicGet,
where:

#+BEGIN_SRC java
Object result = eSettings[dynamicFeatureID];
if (result == null) {
  // actually get the result and cache it
}
return result;
#+END_SRC

Oh oh.  So it /is/ caching values for us.  Here it finds the "Software kind" in
its cache and returns it.  If we remove the cached value, it goes to
ReproduceRule.get, where... the concrete value is "Software Kind".

So, yeah.  The above.  We are dealing with separate instances: the virtual model
is completely disconnected from the model, since they are loaded as separate
resources.

I'm not even sure that the virtual model /should/ reflect changes in this way.
At the very least, I would expect that changes to the underlying concrete models
held by the virtual model are reflected in the virtual model.

But we need to access the contributing models.  Let's do that.

: java.lang.ClassCastException: org.eclipse.emf.ecore.impl.DynamicEObjectImpl cannot be cast to contentfwk.EnterpriseArchitecture

Hmm that's interesting, I could cast to concrete instances when loading the
model myself, but when they are loaded by EView, they are dynamic objects...

Okay, okay, let's make it all dynamic access.

: org.junit.ComparisonFailure: expected:<[foo]> but was:<[Software Kind]>

Of course.  I'm guessing the EStore caching is the culprit here.  If I bypass
it...  Yep!  The test passes.  The change is reflected to the virtual model.

And adding:

#+BEGIN_SRC java
protected boolean eIsCaching() {
  return false;
}
#+END_SRC

to our ReproduceElementImpl is sufficient to turn caching off definitely.

But I'm not sure in what scenario one would peek at the contributing models this
way, rather than taking them straight from the resource.

The notifying approach is more promising, /if/ you can subscribe to changes from
/every instances/ of the same model, which I don't think you easily can.

Maybe hooking into the resource factory or something.  But that's out of scope.

** Accessing a filtered feature in the models                      :emfviews:
Filtered features are removed from the metamodel, but they don't seem to affect
the models in any way.  There /is/ code for skipping instances of FilterElement
in the virtual model list, but we don't construct any instances of these at the
model level.

So, in theory, we should be able to access the content of a filtered feature.
But maybe EMF does not have any mechanism to let us do so.

Wrote a test.  If you give eGet a feature object, it will convert it to a
feature ID using the eAllStructuralFeatures array.  That array is built from the
metamodel, so again, it will fail to find the feature.

I'm not sure there's a way around it.  But I'm unclear on where the eClass for a
model is coming from.  When you load a model, how do we instruct EMF to use the
filtered metamodels?  More questions...

* [2017-06-06 mar.]
** Following the trail while it's hot                              :emfviews:
Picking up where I left things last time.

The eClass for our view is inside the eProperties of the ReproduceElementImpl.
That field comes from EStoreEObjectImpl, and the class is set in
ReproduceElementImpl.init:

:    this.eSetClass(eClass);

The init method is called by the two constructors, but one constructor is
seemingly unused.  So we are left with:

#+BEGIN_SRC java
public ReproduceElementImpl(View vModel, EObject concreteElement) {
  super();
  EClass tempEClass =
      vModel.getMetamodelManager().translateToVirtualEClass(concreteElement.eClass());
  this.init(vModel, concreteElement, tempEClass);
}
#+END_SRC

The eClass used by the reproduce element is looked up in the maps built by the
metamodel manager.  That's where we assign the virtual metamodel with the
filtered features to the virtual model.

Trying to add the feature to the eClass using:

: vba.eClass().getEStructuralFeatures().add(f);

I get a nice array index out of bounds exception, since the eSettings array used
to lookup the feature is not extended when we add the feature as above.

I'm not sure it's something you'd want to do anyway.  But it doesn't work.

So I'll assume that we cannot access features filtered by the virtual
metamodel.  Good thing.

On the other hand, following the code I was reminded of something interesting:
when we filter a reference, what happens to its opposite (it if exists?).  I'm
guessing: nothing, so EMF will probably complain if we try to reach the
opposite.

Let's try it.

Hmm, inconveniently, none of the features we already filter have opposites.
Let's make a new, minimal, test.

Created a minimal ECore metamodel.  Now I need a model.  Here is the code to
generate it:

#+BEGIN_SRC java
String mmURI = "/viewpoint-test/metamodels/minimalref.ecore";
EPackage p = (EPackage) (new ResourceSetImpl()
    .getResource(URI.createPlatformResourceURI(mmURI, true), true).getContents().get(0));

EFactory f = p.getEFactoryInstance();
EObject a = f.create((EClass) p.eContents().get(0));
EClass bClass = (EClass) p.eContents().get(1);
EObject b1 = f.create(bClass);
b1.eSet((EStructuralFeature) bClass.eContents().get(0), a);
EObject b2 = f.create(bClass);
b2.eSet((EStructuralFeature) bClass.eContents().get(0), a);

Resource r = (new ResourceSetImpl()).createResource(URI
    .createPlatformResourceURI("/viewpoint-test/models/minimal.xmi", true));
r.getContents().add(a);
r.getContents().add(b1);
r.getContents().add(b2);
r.save(null);
#+END_SRC

Now the eviewpoint and eview files.  Do I need multiple metamodels in the
eviewpoint?  I think I will trigger the extension part of the code in Viewpoint
if I don't.  But it doesn't matter for filters, since these are applied
regardless.

Ugghh, spent 10 minutes debugging a typo in the modelRef of a linkedElement from
the weaving XMI...  Some validation of these files could be helpful.

Now I need an ECL file.. even though I'm only using filters, so technically I
don't need it.  Wait.  I don't need the ECL file for the view... if it doesn't
exist we'll just skip it.  But I do need an XMI for the view as well... an empty
one will do.

Okay, I can load the view without errors.  Now, I just need to check that the
reference is filtered, that it's opposite is not, and then get the opposite of
the opposite to see what happens.

Oh, interesting.  If I filter out a containment reference: A contains a number
of B, then the view will only contain A.  Since there is no way to access the B
anymore, I cannot access the opposite ref.

Let's try a non-containment then.

Strangely, ~view.getContents()~ returns a list with only one reproduce element,
for the A class from the metamodel.  Even though I haven't filtered anything
yet!

Okay, in View.setVirtualContents, we do:

: oneOftheSublists.add(translateToVirtualElement(r.getContents().get(0)));

Except, with the files I have created for this example, r.getContents() returns
the list [A, B, B], and get(0) returns just the A instance.  So that's why only
A appears in the virtual model contents.

Now, in the working example, getContents returns [EnterpriseArchiteture].  This
is coherent with the model XMI, where everything is wrapped in an
EntrepriseArchitecture tag.  Same for the other ReqIf and BPMN model: they are
wrapped in ReqIf and BPMN tags.

Hmm.  Actually, they are wrapped because that's how the model are made: they
each contain a class with containment references where everything should go.

But that means the View code will only work with such models.  Why not take
everything contained by the resource rather than just the first object?

Seeing as we already have a VirtualContents class that takes sublists...

Okay, made the change.  It shouldn't affect the existing examples since the
behavior is the same for resources containing only one element.  Now we just
don't ignore the other ones.

Wrote a test.  The filtered reference is not available on the metamodel.  Its
opposite is still present.  I think that's acceptable, if we say that views are
"lightweight", that they do not enforce EMF invariants.

However, we can get a hold of the filtered reference by the getEOpposite method
on its opposite.  That's weird.  I would expect it to return null, given that
it's filtered at the metamodel level.  Where is this getEOpposite call looking
for it?  Maybe it's cached?

But more worrying, I would expect to be able to follow the opposite reference.
That currently does an NPE in EStructuralFeatureImpl.getSettingsDelegate.

#+BEGIN_SRC java
EReference eOpposite = getEOpposite();
if (eOpposite != null)
{
  eOpposite.getEContainingClass().getFeatureCount();
}
#+END_SRC

First of all, why is this code even there?  It accesses the feature count... and
does nothing with it.

Regardless, getEContainingClass returns null... but that's not even a containing
reference.  Weird.

* [2017-06-07 mer.]
** EOpposite is set when the model is loaded                   :emfviews:emf:
That's the first answer.  When loading the XMI, bidirectional references set
their opposites to each other.

So when we filter out one part of the reference, the other still has its
eOpposite field set.

Second problem was the null EContainingClass.  It should return the class
containing the feature.  But since the feature is filtered out, it has no
containing class anymore.

Actually, bypassing the code:

#+BEGIN_SRC java
if (eOpposite != null)
{
  eOpposite.getEContainingClass().getFeatureCount();
}
#+END_SRC

we do get access to the feature value, and the test succeeds.  So this bit is
problematic.

I really don't know why it's there.  I understand that this getter is memoized,
so calling it is a way to force the computation of whatever underlying data it
returns.  But if you need to force the computation, it's because you are peeking
under the sheets of the interface; ergo, doing something you shouldn't.  Or, you
don't really need to force the computation, and this code is useless.

Doing a bit of git spellhunking...

: http://git.eclipse.org/c/emf/org.eclipse.emf.git/
: git://git.eclipse.org/gitroot/emf/org.eclipse.emf.git

Found [[orgit-rev:~/proj/org.eclipse.emf/::22137e7][one commit]] from 2005 (!) where the code was updated, but the strange ~if~
was already there:

#+BEGIN_SRC diff
       if (eOpposite != null)
       {
-        eOpposite.getEContainingClass().getEAllStructuralFeatures();
+        eOpposite.getEContainingClass().getFeatureCount();
       }
#+END_SRC

Before that, I get the initial git commit from 2004 when the repo was created by
splitting from a previous CVS probably (there are .cvsignore files lying
around).  From what I can find, the CVS repository is now unavailable, so I
won't get any history beyond that.

** Cleaning up URI loading                                         :emfviews:
This was getting annoying to deal with a dummy workspace just to have the test
resources.

Asked G about it, he suggested I use createURI instead of
createPlatformResourceURI, to avoid being tied to the workspace.  We tried it
together, and using relative paths with createURI will load the files from the
current plugin.

Had to make changes in multiple places where we previously used platform URI, or
worse, findMember on the workspace.  Now it's more homogeneous.

He also suggested we pass URI to the EMFViews core instead of strings, so that
we leave the problem of creating and resolving them to the client.  I agree, and
would even go as far as passing resources directly.

*** TODO Investigate duplication of model loading from XMI
Since in Viewpoint we pass around URI strings when we really want to deal with
EPackages, there may be some duplication where we load a package from the XMI
instead of getting an EPackage directly, or getting it from a registry.

That seems like unnecessary work, and a potential source of bugs (since we have
clones of models lying around, so strict equality wouldn't work).

* [2017-06-09 ven.]
** Trying out Eclim for controlling Eclipse from Emacs        :emacs:eclipse:
Out of the box I set the bar a little too high for Eclim, since I'm running
Eclipse 4.7 M6, and only 4.6 is supported at the moment.

There is a development branch for 4.7 on the Git.  I try it out, follow the
build from source guide.  But it fails to build on my Eclipse config, since I
have a separate configuration folder and platform folder.

Ok ok.  Maybe I should try a plain Eclipse to see if it's even worth the
trouble.  I'm afraid it doesn't have useful stuff like Javadoc on hover (maybe
using Eldoc?).  Let's see.

It installs with 4.6.  Now I run the eclimd daemon from inside Eclipse
(View->Eclimd).

Not that slow.  You can get Javadoc for a type, not on hover, but with a
binding.  Auto-completion seems to work, although you don't have the Javadoc for
completion items.

I think the way windows are created and (not) disposed for each function is more
annoying than helpful.  I'll keep my current setup.

** Using Eview for metamodels                                      :emfviews:
To use EView for the metamodel level, we would need to provide a viewpoint.  The
viewpoint is used to populate the maps in the metamodel manager; essentially,
its role is to assert what features are present on the viewpoint.

If the Eview is used for the metamodel, its viewpoint should be the Ecore
metamodel itself.  Maybe I can try building a test around that.

* [2017-06-12 lun.]
** Made a class diagram of emfviews.core                          :emfviews:
A bit hairy.  PlantUML uses GraphViz behind the scenes, so the layout engine
quickly shows its limits when you get a dozen of boxes.

Regardless, it helps to see the whole picture.

I think I want a sequence diagram of Viewpoint.doLoad, and most importantly,
EView.doLoad.  Probably an object diagram of everything created by
EView/Viewpoint for the minimal example.

* [2017-06-16 ven.]
** More diagramming                                                :emfviews:
I fleshed out the class diagram a bit, fixing the layout so it's more readable.
Dependency arrows are in a light color so they don't drown the rest of the
information.

I wrote a small JS bookmarklet to add interactive highlighting of the outgoing
edges from a node, so we can make sense of all the information that's in it.

** Review of tools for creating UML class diagrams                      :uml:
I tried other tools for UML diagrams; small tools like Dia or yEd give fine
control over the layout, but you have to do everything graphically.  yEd has
advanced layout and grouping features, with collapsing.

Can't easily export PlantUML output to something these tools eat, unfortunately.
PlantUML has XMI export, but it's only for the classes, not the arrows.

Large tools like Modelio/Papyrus/UML Designer are slow and cumbersome for
drawing just a class diagram if you don't need all their facilities for
generating code from it.  One of them froze when I tried to zoom out on the
nearly empty canvas.

I think yEd is decent enough for quickly whipping something up, and the
auto-layout features does a better job than GraphViz.  Too bad it's proprietary.
Though you can export to GraphML which is an open format.

Trying out the ObjectAid plugin for generating class diagrams directly from the
code.  It does a decent job at that, it can even show dependencies between
classes.  I think it's picking up constructor calls; but not casts.  The
auto-layout feature avoid nodes overlapping, but seems to largely ignore edges,
so it's overall useless.  Also, you cannot control what hide/show individual
attributes or links.  The diagram is serialized to XML by default, so it's
reusable, but there is no export to a common graph format, only images (and no
SVG).

** Exciting use case for EMFViews                              :emfviews:uml:
That gets me thinking.  The output of automated tools for generating class
diagrams will always be too rich, will contain too much information.  Usually,
you want to filter out this information to focus on a specific functionality, to
understand the project piece-wise.  That's why I started out by mapping the
project manually, because I knew that I wanted to do a partial representation of
the project, even with some simplifications.

So, usually, you want /views/, not the whole picture.  Using EMFViews to create
partial views of programs would be a terrific use case (and using it on itself
would be nice).  We might also want to aggregate info from multiple models: like
a model of which Java statetement was executed by some test (impact analysis),
combined with a MoDisCo model to create a view of a class diagram where you only
see the class/methods/attributes used by a specific test.

Interestingly, MoDisCo has entries for each cast and constructor invocation in a
project, but if you want to know that a specific type is used as argument to a
method or indirectly through the return value of a method (i.e.,
a.getB().foo()), it seems you have to extract it yourself.

* [2017-06-20 mar.]
** Converting PlantUML to GraphML                              :plantuml:uml:
I've finished this big object diagram, giving me a concrete view of what objects
are created when we call EView.load.

It's a bit cumbersome to navigate, since GraphViz made a bit of a mess of it.
It's about twice as large as the class diagrams, and it's only a /very/ simple
example (there are no virtual associations involved, and only one contributing
metamodel).

I was thinking of looking if yEd was able to get a better layout out of it.
Since this is the second diagram I made where I wish there was an easy way to
import PlantUML files into yEd, let's shop for solutions.

There is a [[https://github.com/Kesin11/plantuml_class_digram_parse][PlantUML parser there]].  It's in Perl, and doesn't seem to parse
much.  Class diagrams, classes and relationships.  But doesn't look like it
parses labels on relationships.  Also, I don't know Perl.

There is [[https://github.com/leungwensen/plantuml-parser][one in JS]].  This one looks rather complete.  There's a PEG grammar file
that looks exhaustive.  But it doesn't seem to work as-is.  Calling plantUMLParser.parse:

#+BEGIN_EXAMPLE
	      peg$startRuleFunctions = { start: peg$parsestart },
	                                        ^

ReferenceError: peg$parsestart is not defined
#+END_EXAMPLE

Apparently, the parser is lifted from [[https://github.com/bafolts/plantuml-code-generator][plantuml-code-generator]].  That project
looks more fleshed out (there's a README, at least).  And in fact, that's where
the PEG grammar is coming from.

Trying it out.. it looks like loading the parser generated by pegjs does the
trick.  On a simple string, it looks for the "@startuml" line, so that's a good
start.

On my object diagram though:

#+BEGIN_EXAMPLE
/home/fmdkdd/proj/plantuml-to-graphml/plantuml.js:2247
      if (peg$c91.test(input.charAt(peg$currPos))) {
                             ^

TypeError: input.charAt is not a function
    at peg$parsenoise (/home/fmdkdd/proj/plantuml-to-graphml/plantuml.js:2247:30)

#+END_EXAMPLE

:(

Hmm, wait a minute.  Maybe I'm not passing a String?  I was using fs.readFile
from node, and I forget to pass the encoding.

Haha, yes!

#+BEGIN_EXAMPLE
/home/fmdkdd/proj/plantuml-to-graphml/plantuml.js:3299
      throw peg$buildException(
      ^
SyntaxError: Expected "@startuml", [ \t], [\n], [\r\n] or end of input but "'" found.
    at peg$buildException (/home/fmdkdd/proj/plantuml-to-graphml/plantuml.js:361:14)
#+END_EXAMPLE

Doesn't support comments?  Looks like it does, but not before the ~@startuml~
tag.  PlantUML doesn't care... fixing.

#+BEGIN_EXAMPLE
/home/fmdkdd/proj/plantuml-to-graphml/plantuml.js:3317
      throw peg$buildException(
      ^
SyntaxError: Expected [^\r\n] but "\n" found.
    at peg$buildException (/home/fmdkdd/proj/plantuml-to-graphml/plantuml.js:361:14)
#+END_EXAMPLE

What?  Grmbl grmbl.  If you don't tell me /where/ in the input you failed to
match. that's going to be tedious.

There's a ~--trace~ option I can pass to pegjs.. but it's still not telling me
where it failed.

plantuml-code-generator uses pegjs 0.9, and there's a 0.10 with "improved error
messages".  Let's see.

Still doesn't tell me the location in the input where it failed to match...
Ahah!  Catching the syntax error in a try/catch, it does include location
information.  The default toString does not report it.

Okay, so it's not taking '/' or '.' in attribute names.  Pretty strict.  I guess
it's because the grammar is used to /generate/ code from the PlantUML file, so
you don't want anything funky in your identifiers.  But it ultimately depends on
the language, so...

After making it more lax with object names / members names, it appears it's not
parsing names of relationships either.

At this point, I know that I can use PegJS to make a more generic parser... or I
could write my own... or I could just use the PlantUML parser and add a GraphML
exporter in there.

I think I'd rather have it as a standalone tool than an addition to PlantUML.

[[http://graphml.graphdrawing.org/primer/graphml-primer.html#Graph][GraphML has a spec]], under CC-by.  Doesn't look too fancy, just XML.  A good POC
would just output nodes and edges, ideally with attributes (as a label inside
the node?  Just have to check out what yEd exports...)

#+BEGIN_EXAMPLE
    <node id="n0">
      <data key="d4"/>
      <data key="d5"/>
      <data key="d6">
        <y:UMLClassNode>
          <y:Geometry height="120.48000000000002" width="262.56000000000006" x="318.71999999999997" y="-10.240000000000009"/>
          <y:Fill color="#FFFFFF" transparent="false"/>
          <y:BorderStyle color="#C0C0C0" type="line" width="1.0"/>
          <y:NodeLabel alignment="center" autoSizePolicy="node_width" configuration="CroppingLabel" fontFamily="Dialog" fontSize="13" fontStyle="plain" hasBackgroundColor="false" hasLineColor="false" height="21.1328125" horizontalTextPosition="center" iconTextGap="4" modelName="internal" modelPosition="c" textColor="#000000" verticalTextPosition="bottom" visible="true" width="262.56000000000006" x="0.0" y="3.0">Viewpoint</y:NodeLabel>
          <y:UML clipContent="false" constraint="" omitDetails="false" stereotype="" use3DEffect="false">
            <y:AttributeLabel>-contributingEPackages: List&lt;EPackage&gt;</y:AttributeLabel>
            <y:MethodLabel>#doLoad()
#doSave()
+getContents()
+getResourceSet()</y:MethodLabel>
          </y:UML>
        </y:UMLClassNode>
      </data>
    </node>
#+END_EXAMPLE

Looks like yEd has its own additions to the GraphML format... That was more or
less expected, since the GraphML doesn't have much in it, and yEd adds a bunch
of information.

** Observations from the diagram                                   :emfviews:
The EPackage has three clones lying around.  One is the original, one is the
filtered copy, but then one is used by the VirtualLinkManager to map virtual
classes.  There's probably a better way to do it.

I'm not sure that have not duplicated other stuff as well.  Should redo the
experiment by noting the objects id to be sure.

* [2017-06-21 mer.]
** Is it possible to use neato instead of dot with PlantUML?       :plantuml:
Not out of the box.  There is an argument ~-graphvizdot~, but passing
~/usr/bin/neato~ has no effect.  There is also some "layout strategy" option in
the parser, but it's not connected to anything.

So, the PlantUML -> GraphML route still stands.

* [2017-06-22 jeu.]
** Getting a heap dump from Eclipse                            :eclipse:java:
If I wanted to generate an object diagram mechanically, I would first need to
get a heap dump.

Looks like there are at least [[https://stackoverflow.com/questions/25168490/java-eclipse-create-heap-dump-on-breakpoint][two ways]] to do that.  jvisualvm comes with Java,
and you can plug into an existing process.

So I put a breakpoint in Eclipse, and obtain a heap dump in the HPROF format.
JVisualVM allows me to browse this dump more or less like the Eclipse debugger
can (there's fewer "nice" short string format).

There's also ~jhat~, a bundled command, that creates a local server to browse an
HPROF dump.  It's not very useful for browsing through, since it doesn't show
attribute values.

Most tools dealing with heap dumps seem targeted to people who want to find
memory leaks, understandably.  Like [[http://www.eclipse.org/mat/][MAT]].

JVisualVM accepts plugins though, so we could imagine an export to DOT.

Another way to obtain an HPROF dumb, without jvisualvm:

: jmap -dump:format=b,file=/tmp/foo.hprof PROCESS-PID

This can then be opened in MAT for browsing.  Though I'm not seeing values for
inherited attribute in MAT... but unless JVisualVM was doing some peeking in the
runtime process when displaying the HPROF, the info is probably there.

[[https://web.archive.org/web/20121221115642/http://java.net/downloads/heap-snapshot/hprof-binary-format.html][Here's some documentation]] on the HPROF binary format.  But it might not be the
freshest info.  Looking at the source of jmap or jvisualvm is also an option.

** Some concerns about creating object diagrams mechanically       :emfviews:
First, heap dumps are huge, and you are concerned with only a fragment of it at
one time.  MAT reports nearly 500k objects in the heap for the small test I
built the object diagram from.  In the object diagram I made, there are ~50
objects.  Whitelisting interesting objects should be the default.

Second, heap dumps are instantaneous and cannot give any temporal information
individually.  That's relevant for keeping track of objects collected by the GC.
In the object diagram I made, an instance of LinksProjector is created by
VirtualLinkManager to setup virtual associations, but no reference to it is held
so it is collected when the method ends.  Depending on when the heap dump is
captured, you might or might not see this object.  Taking multiple dumps can
help, but is not guaranteed to be correct, unless you are able to register all
allocations.

In fact, the JVM can also perform escape analysis and decide to allocate objects
directly on the stack.  In that situation, I'm not sure the object would appear
in a heap dump at all.  So, again, heap dumps cannot give you a full list of
created objects.

Third, you might want to enrich the visualization of some types or values.  In a
heap dump, objects are just nodes that points to other objects.  But depending
on the type, you might want to make them distinguishable.  Lists, arrays and
hashmaps are easier to recognize as tables.  In the object diagram, I've made
simplifications for URI objects to just their string value.

* [2017-06-26 lun.]
** Wrote a test for virtual associations                           :emfviews:
Using minimal metamodels and models.  The main difference with the minimal
filters test is that this time we need a view ~weaving.xmi~ that's not empty.

Providing the viewpoint weaving.xmi is not enough.  It's used to construct the
metamodel of the virtual model, but not to populate it.  The weaving.xmi of the
view is used by the LinksProjector to create the virtual associations.

One difficulty to create this view weaving model was to find the correct
~elementRef~ values.  A first clue is in LinksProjector.getReferencedObject,
where this field is used:

: referencedElement = r.getEObject(elementRef);

It calls Resource.getEObject, which accepts an URIFragment from EMF.  Here are
examples of URI fragments from the three-model-composition test (where they are
generated by ECL):

#+BEGIN_EXAMPLE
  //@architectures.1/@processes.0
  _48wAUN6xEeCbzp_EHZybUg
  //@architectures.0/@strategicElements.0
  rmf-19428170-0b70-4b81-9fd2-0cdec5778a49
  //@architectures.0/@strategicElements.1
#+END_EXAMPLE

Some are "structured fragments", that reflect the path to follow in the
resource.  It's a mix of structural features names and indices for contents
lists.  Others are "IDs", which look like hashes.

To find the correct fragment, one can load the models in a resource, and call
~Resource.getURIFragment(eObject)~ on the EObject in question.

Turns out, with the simple models that I have, the fragment ~/0~ works.

Now, it's brittle to use resource-dependent indices; I'd rather use a qualified
name, but since this field is supposed to be created by ECL... maybe there's no
easy way to construct a qualified name there.

Hmm, using ~/~ as fragment also works in this case...  I cannot find
documentation on the URI scheme used by EMF.  All I found was a comment by Ed
Merks saying that it was "XPath-like", but not XPath.

Regardless, after discussing with H., I think we are aiming towards qualified
names for this field, as they are more readable.  Just have to check that ECL
can generate them.

* [2017-06-28 mer.]
** Rethinking the virtual links metamodel                          :emfviews:
We have some attributes that are never used, some attributes that are used in
bad ways, and things we cannot express.

The requirements are:
- We can have any number of contributing models
- We can transpose concepts and properties from contributing models in the view
  so they appear in the view
- We can filter concepts and properties from contributing models so they do not
  appear in the view
- We can create new concepts, properties and associations in the view that do
  not exist in the contributing models

Additionally, we can choose between "blacklist" and "whitelist" modes: blacklist
mode, the default (and current behavior), is when the view contains all the
concepts of the contributing models, and you can filter some concepts /out/
explicitly.  In whitelist mode, no concepts are included by default, and you
must filter them /in/ explicitly.

We could also want the same feature at the concept level, to filter out/in
properties for each concept separately.

After sketching a few alternatives and iterating with H., we converged [[file:doc/virtuallinks-metamodel3.svg][on one
design]] that ticks all the boxes, and should be a definite improvement over what
we had so far.

Some notes:
- It supports blacklisting/whitelisting at the view level, but not for
  individual concepts.  You don't lose expressive power, but only convenience
  and maybe performance by having to exclude or include a bunch of properties.

- The fact that synthetic elements (New*) are also instances of LinkedElement
  (through VirtualElement) lets use them as target for other synthetic
  elements.  So a NewProperty can be added to a NewConcept, rather than being
  locked to concepts belonging to the contributing metamodels.

  This is also means there is a risk of circular dependencies (if A and B are
  new elements, and A refers to B and B refers to A).  This can happen for,
  e.g., associations: Assoc A1 from A to B has opposite A2, A2 from B to A has
  opposite A1.  Cannot set the opposite fields before the two associations are
  created.

  We'll see how best to resolve that once we get to translate the metamodel to
  EMF code.  Using the order of elements from the resource looks like a decent
  first approach.

- The model is quite orthogonal, but lacks constraints for invalid situations.
  For instance, NewAssociation ~opposite~ field should target either
  NewAssociation (if VirtualElement) or have an FQN that points to an
  EReference.

  Likewise, a NewConcept can have sub- and super-concepts, but you can create
  non-sensical hierarchies like A < B and B < A.

* [2017-06-30 ven.]
** Switching to the new virtual links metamodel                    :emfviews:
Wrote the metamodel in Ecore, generated the code.  Errors everywhere.

Disregarding the UI and Editor plugins for now, to focus on the core and on the
tests.

Apart from renaming things to follow the new metamodel, the code is already
simpler in a few places, when fetching the new element from the metamodel or
model notably.

For now I'm just trying to get back the previous functionality, and disregarding
the additional functionality like creating associations between virtual
elements, and synthesizing new elements (for which I didn't have tests anyway).

One difficulty is that we cannot use fully-qualified names to target model
elements.  The code in EMFViewsUtil.findElement is not working for models
because it tries to get the "name" feature on the EClass, but model objects have
their metamodel class as EClass, and these don't necessarily have a name.  We
/could/ write a findModelElement function that first gets the structural feature
corresponding to the FQN, and then try to eGet this feature in the model.  That
wouldn't be sufficient if we wanted to target EClass instead... so the approach
has to be different.  Besides, one named feature on the metamodel may correspond
to multiple model objects, in the case of lists.

For the moment, I'm sticking to EMF URI fragments for the model level.  That's
one discrepancy between how we use the virtual links metamodel for viewpoints
and for views.

Currently passing 2 tests out of 5... the three-model-composition is larger, so
harder to fix.  It generates the weaving model for the view through ECL, so I
have to fix that as well.

Fixing ECL...  looks like it's still missing something.  Will investigate next
week.

* [2017-07-03 lun.]
** Installing Eclipse Oxygen                                        :eclipse:
Release version.  I was on milestone 6 previously.

As usual, since it's easier to add plugins than to remove them from Eclipse, and
a lower number of plugins makes for a healthier Eclipse, I went with the Oxygen
platform download:

http://download.eclipse.org/eclipse/downloads/drops4/R-4.7-201706120950/

Then, I added:

- JDT
- PDE
- EMF SDK (EMF is already included)

EMFViews requires also OCL, Epsilon and Epsilon EMF integration.

I used the update site for Epsilon 1.2, since migration to version 1.4 is still
a TODO.

After that, and after making sure the target platform and run configuration are
coherent, I was able to run the tests.

** Fixing tests for the new metamodel                              :emfviews:
We actually have 4/5!  Three-model-composition runs into an NPE.  Investigating.

We fail in EStructuralFeatureImpl:

#+BEGIN_SRC java
EClassifier eType = getEType();
Class<?> instanceClass = eType.getInstanceClass();
#+END_SRC

eType is null.  Since a feature is a typed element, it /shouldn't/ be null.

Found it: the setEType in Viewpoint is null... because targetElem is null.
That's because of a wrong path in the viewpoint weaving model (BPMN instead of
bpmn2).

Will have to make sure we find the element before creating associations...

Still, too bad EMF doesn't inform us that the EReference is invalid before...
That probably stems from the fact that we create the EReference piece-wise, so
we can violate invariants before we are done initializing it, and there's no way
to tell EMF we are done.  The builder pattern solves this.

Added some checks.  Found a bug in another concreteElement for reqif (reqif10 is
the name of the root), which was not tested.  Early errors pay off!

* [2017-07-04 mar.]
** Writing tests for adding concepts                               :emfviews:
Some unresolved questions.

Where do new concepts go?  If they subtype an existing concept from a
contributing package, they could go in that package.  But if they subtype none?
Or if they subtype two concepts from separate packages?

After discussing with H., we settled on putting all new concepts in a specific
package that has the name of the viewpoint.  Now the viewpoint must have a name.

In testing for this feature, there's a bit of non-determinism: the order of
packages in the virtual contents of the viewpoint is undefined.

Actually, the order stems from the order of the underlying hashtable used by the
package registry were we put these packages.

It would make sense to specify the order.  One order that seems obvious is to
take the order of the contributing models, plus the extra virtual package after
these.

I added support for new properties as well.  We now have basic functionality as
provided by the new metamodel.  I should go over what was possible in the
previous version to make sure we have not lost any expressive power.

* [2017-07-05 mer.]
** Handling errors in EMF models                                   :emfviews:
Wrote a small test.  EMF does not seem to do any validation of metamodels
created dynamically.  However, calling Diagnostician.INSTANCE.validate can give
you a list of diagnostics: if two attributes are the same name, then it's not
OK.

The code that does the actual validation is in ecore.util.EcoreValidator.

** Handling errors in Viewpoint                                    :emfviews:
There are many ways in which loading a weaving model and executing it can fail:

#+BEGIN_SRC java
EObject parent = tryGetEObject(p.getParent());
if (!(parent instanceof EClass)) throw new InvalidLinkedElementException(String
    .format("Parent of new property '%s' should be an EClass", p.getName()));
EClass parentClass = (EClass) parent;

String n = p.getName();
if (n == null) throw new ViewpointException("New property name is null");
if (!n.matches("[a-zA-Z][a-zA-Z0-9]*")) throw new ViewpointException(String
    .format("New property name '%s' should be non-empty, start with a letter, and contain only letters or digits",
            n));

for (EStructuralFeature f : parentClass.getEAllStructuralFeatures()) {
  if (n.equalsIgnoreCase(f.getName())) throw new ViewpointException(String
      .format("New property name '%s' is already taken in class '%s'", n,
              parentClass.getName()));
}
#+END_SRC

In many respect, the weaving model is a language, and Viewpoint.loadWeavingModel
is an interpreter.  These ViewpointException are thus semantic errors, stemming
from an invalid usage of the language.

Now, the problem is that mixing exceptional code with the happy path is hard to
read.

Solutions:

1. Do the validation in a separate class, before going down the happy path.

   That amounts to writing an interpreter that visits the whole model,
   simulating operations taken by the viewpoint, but not actually creating any
   side effects, just throwing exceptions if the invariants are not obeyed.

   Pros: happy path is totally separated from exceptional code.  All possible
   errors are treated on the side.

   Cons: duplication of visiting code, duplication of logic (to check if a
   NewConcept has a unique name, you have to collect all the new concepts),
   duplication of work (validation is interpreting once, and the happy path is
   interpreting a second time).

   Not sure if we can actually catch /all/ errors without creating the
   side-effects on Viewpoint.  So there might still be exceptional code in the
   happy path.

2. Keep the validation in the happy path, but as one-liners.

   E.g., call ~validateName()~ on the name before using it.  The code in
   validateName takes care of all exceptional cases, and is hidden away.

   Pros: exceptional code is kept to a minimum in the happy path.  No
   duplication of visiting code/logic/work.

   Cons: If we do any work optionally, then we may never raise an error for an
   invalid usage.  That's like every dynamic programming language, where code
   that is never executed is never checked.


Will think more about this on the way home.

* [2017-07-07 ven.]
** Dealing with exceptional cases in Viewpoint                     :emfviews:
I've adopted solution 2.  It's the simplest, and with an utility function like
this:

#+BEGIN_SRC java
  private ViewpointException EX(String msg, Object... args) {
    return new ViewpointException(msg, args);
  }
#+END_SRC

You can write:

#+BEGIN_SRC java
if (model == null)
        throw EX("Model '%s' of concrete element cannot be found in package registry", modelURI);
#+END_SRC

Which is minimal noise.

In concert with the Optional type, we can write this:

#+BEGIN_SRC java
EObject obj = EMFViewsUtil.findElement(model, path)
          .orElseThrow(() -> EX("ConcreteElement '%s' cannot be found in model '%s'", path,
                                modelURI));
#+END_SRC

I've thought about moving the different types of errors in ViewpointException,
like so:

#+BEGIN_SRC java
static ViewpointException INVALID_NAME(String msg, Object... args) {
  return new ViewpointException("Invalid name '%s'", String.format(msg, args));
}
#+END_SRC

But that's creating an interface that I have no use for at the moment.  Maybe
when I write tests for failures, that would be easier to check against an error
type (from an enum?) than the exact string message.

* [2017-07-10 lun.]
** Reviewing features cut in the new weaving metamodel             :emfviews:
From what the previous code did for the "Extension" case in Viewpoint.

| Previous          | Now    |
|-------------------+--------|
| refine            | yes    |
| generalize        | yes    |
| add property      | yes(1) |
| filter property   | yes(2) |
| filter class      | yes(2) |
| add constraint    | no     |
| filter constraint | no     |
| modify property   | yes(3) |
| add reference     | yes(4) |
| filter reference  | yes(2) |

1) we don't support all primitive types yet, but that's a matter of minutes.
2) superseded by ElementFilter
3) indirectly through filter+add, but are they really equivalent?
4) the previous code supported specifying containment reference

So we are not missing anything major.

Now, the most interesting aspect of the new weaving metamodel is the ability to
target virtual model for properties, concepts and associations.  Let's focus on
that.

The "VirtualLink"/"VirtualElement" objects that are in the weaving model can be
thought of as "instructions" or "construction orders": that's how they are used
by the Viewpoint.  From a NewConcept, we will create an EClass.  The two objects
are different, but the NewConcept stands in for the EClass, and there's a clear
mapping from one to the other.

So, we can collect the EObject created from a VirtualLink in a map, and look up
this map whenever we want to target a VirtualElement.

** Solving circularity in the new weaving metamodel                :emfviews:
As already noted, creating a NewAssociation R1 with an opposite R2, R2 has to
exist at the time R1 has its opposite set, otherwise it cannot work.

Currently, since new associations are created in the order given by the XMI,
this cannot work.

Solutions:

1. Delay setting the opposite value of new references until after all references
   are created.

   That fixes the circularity for opposites, but not for other cases.  Are there
   other cases?  I'm not sure.

2. Do two passes: one to create virtual elements and populate the mapping, and a
   second one to set their EMF fields.

   Solves all potential circular dependencies.  Also removes the order
   constraint of the XMI elements.  Slightly more code complexity.

3. A fully virtual metamodel would not have this issue, as it would be lazy
   anyway.  The value for the opposite would be resolved only when code queries
   it (e.g., the Diagnostician).

   Might be the solution we end up adopting anyway.


Circular references could happen for concepts if, e.g., C is new and has D as
subconcept, and D is new.  D effectively has C has superconcept, but we don't
need to repeat it.  If we did, then we would end up with a circular dependency.

I'll go with #2, in the interest of correctness.

Done.  Code complexity was not increased much.  A bit of additional implicit
control flow dependency between methods (have to populate the synthetic element
map first).  Might be mitigated by making the map an explicit argument of the
build* methods.

* [2017-07-11 mar.]
** Implementing whitelisting                                       :emfviews:
In blacklisting, when you filter an element, you are implicitly filtering every
element under it, since we are dealing with trees.

So it makes sense that in whitelisting, when you filter an element /in/, it
should implicitly include all the elements that are /above/ it in the tree, but
not the elements below.  Otherwise, you cannot access the element, and you would
have to explicitly filter /in/ everything above it.

In practice, we can delete everything that is not filtered.

Idea: go through all contributing packages and their contents, recursively, and
create a list of (path, objects) tuples, where path is the qualified name to
access the objects.  Then, remove all tuples from the list where path is a
prefix to to an element filter.  Then, delete all objects that remain.

But Java has no tuples.  Okay, I guess I can generate the list on the fly: for
each EObject, determine its path, if it's not a prefix of any element filter,
add the object to the list.  Delete all objects in the list.

If an EObject is not a prefix of an element filter, we can add it to the
elements to delete /and not descend to the children/.  But if it is a prefix, we
have to descend until it's a match, and add the children to the elements to
delete.

Well, no, actually we can't short-circuit, because one of the children may be
explicitly whitelisted as well.  So we can delete only objects whose path is not
a prefix of any element filter.  That simplifies the code, but increases time
complexity.

* [2017-07-12 mer.]
** Whitelisting complexity is slightly worse than blacklisting     :emfviews:
Whitelisting is O(len(model)*len(filters)), where len(model) is the total number
of elements in the model graph, across all contributing packages.

Blacklisting loops on the filters, but it calls findEObject, which calls
findElement, and that is O(len(models)) as well.  So the two complexities are
equal.

Except than in practice, findElement can shortcut, while whitelisting in
applyFilters never shortcuts.

But, whitelisting works now.  The better approach would certainly be to be fully
lazy.  Will think about that.

** Weaving metamodel variation                                     :emfviews:
In the last meeting, it has been brought up that the new weaving metamodel was a
bit confusing: new associations sources and targets point to linked elements,
but these can be new association, which does not make sense.

At the moment it's a runtime error, but it could be enforced by the metamodel.
We could use OCL constraints, but just having a separate type for valid targets
would work.

However, while we could do that for virtual elements, we cannot do that for
concrete elements, since we only have a path to link to them.

Unless it's possible to link directly to the concrete elements... by loading the
weaving model alongside the contributing models in the same resource set?

Wrote a test... it's definitely possible to add EObjects directly into the
weaving model, save and load that resource.  No need for findElement, and it
should work with any EObject, not just named objects.

Made [[file:doc/virtuallinks-metamodel6.graphml][another class diagram]].  But I'm not sure it's worth it, since it might tie
too much to Ecore.

Here is a [[file:doc/virtuallinks-metamodel7.graphml][slight variation]], where we only add "Concept" and "Association" and
remove "VirtualElement".  That way, we are sure that synthetic elements cannot
be bogus.  But we still have no clue for concrete elements, since we only have
the FQN string.

* [2017-07-18 mar.]
** Virtualizing the viewpoint                                      :emfviews:
Currently, Viewpoint is not truly virtual: we use the weaving model to build the
viewpoint at construction time, and it can never change.

The upside is that it's simple: you deal with stuff that's not changing.  And
it's probably more efficient that way: the virtual contents are built ahead of
time.

The downsides are that, for large metamodels, it might not be that efficient to
iterate over all of their features ahead of time, if only a few of them are ever
used.  And we lose the ability to mutate the viewpoint when the metamodels
change...

But how often to metamodels change?  And we can even subscribe to their changes?

Also, if an EAttribute changes name, or is deleted, but it was the target of a
NewAssociation in the weaving model, now the weaving model is invalid.  Should
the viewpoint subscribe to changes to the weaving model as well?

H. reminded me that the point of EMFViews is to have a /lightweight/ solution to
combine multiple models.  Not only to create new metamodels and models from
existing ones—model transformation tools can do that already—but to combine
huge models quickly.

Another concrete use case that surfaced is for NeoEMF: using EMFViews to control
the visibility of features for users with different access privileges.

*** Use case 1: lightweight combination of models
Now, for the first use case.  Let's say you have a few metamodels and want to
create a viewpoint.  You:

1. Create an .eviewpoint file
2. Specify the weaving model
3. Create the viewpoint

Now, let's assume the viewpoint contents are built at creation, and cannot be
updated.  That's the current state.

**** Updating a metamodel
You update the metamodels.  You remove a concept, or rename a concept.  The
viewpoint doesn't change.  You have to recreate it.

- Rebuild viewpoint

This discards the viewpoint and rebuilds it, using the .eviewpoint file and
weaving model once again.

If that concept was referenced by the weaving model and you did not propagate
the changes to the weaving model, then the viewpoint will emit an error.  Then
you need to:

- Update weaving model
- Rebuild viewpoint

Otherwise, the viewpoint will now reflect the new versions of the metamodels and
weaving model.

**** Updating the weaving model
Now you update the weaving model, by adding another contributing model.  The
viewpoint still does not pick up this change.  You have to:

- Rebuild viewpoint

Every time you change the weaving model.

*** Use case 2: access control
You define a viewpoint for access control on an existing metamodel:

1. Create an .eviewpoint file
2. Specify the weaving model (whitelist)
3. Create the viewpoint

It looks like the same thing.  Actually, in use case 1, the actions are probably
interactive, and ultimately map to actions the core API.  In use case 2, the
actions are probably direct calls to the API.

*** When the viewpoint is virtual
One approach is to make the viewpoint fully lazy.  Currently, the only entry
point to a Viewpoint is the ~getContents~ method of the resource.  And
currently, we return a virtual contents object that has been built at loading
time.

If, instead, we build the virtual contents object each time ~getContents~ is
called, then we are guaranteed to always reflect the latest states.

*** Let's focus on virtualizing the viewpoint without considering updates for now
After debating with H., we just want to have the Viewpoint emit virtual
elements, as is the case currently for EView.

How to handle updates in the viewpoint or weaving model is then a separate
concern that we might tackle later.

** Trying to wrap the objects returned by Viewpoint                :emfviews:
In the VirtualContents.  Questions abound.

First of all, most of the tests that deal with Viewpoint should be changed to
use the reflective EMF API.  Now, we know that we have EPackages, but we should
have VirtualElements that may or may not be packages.

But, do we need to wrap recursively the contents of EPackage?

* [2017-07-19 mer.]
** Virtualizing viewpoint: first approach                          :emfviews:
The point is to avoid copying the contributing models, and just proxy.

For each contributing package:

- create a new VirtualEPackage that delegates to the EPackage

That should be transparent right?

Looks like the Diagnostician is not happy, as it assumes it can convert to
InternalEObject... but these are not.

Too bad.  Let's toggle it off.

All my tests pass... though they shouldn't, since here I am modifying the
original metamodels.  But the tests never check for that.

Hmm, but this approach won't work.  Or at least, it works at the metamodel
level, but we should use the same architecture for the model level.  There, we
will have no other choice but to be a VirtualEObject.  If VirtualEPackage is a
subclass of VirtualEObject, then it works.

One of the disavdantage of this approach is that implementing EObject/EPackage
has a lot of methods.  To filter classifiers according to the blacklist for
instance, there are at least two relevant methods:

- getEClassifiers()
- getEClassifier(String)

But, one could also obtain the list of classifiers through the reflective API of
eInvoke, eGet, etc.  That's a lot of holes to patch.

The EStore approach has only two methods: get and set.  Easier to make sure it's
correct.

Otherwise, there's the DynamicEObject approach.

** Virtualizing viewpoint: using EStore                            :emfviews:
Running into weird exception of impossible casts from EPackageImpl$2 to
EObject...

That's an anonymous inner class to EPackageImpl, but where is it coming from?

#+BEGIN_SRC java
 public EList<EClassifier> getEClassifiers()
  {
    if (eClassifiers == null)
    {
      eClassifiers =
        new EObjectContainmentWithInverseEList.Resolving<EClassifier>
          (EClassifier.class, this, EcorePackage.EPACKAGE__ECLASSIFIERS, EcorePackage.ECLASSIFIER__EPACKAGE)
        {
          private static final long serialVersionUID = 1L;

          @Override
          protected void didChange()
          {
            eNameToEClassifierMap = null;
          }
        };
    }
    return eClassifiers;
  }
#+END_SRC

It's this newEObjectContainment thingy.

Oh wait, it's returning a /list/ and the EStore.get should return a lone
element.  Yes, that works.

Now I think I really need to update the tests to use the reflective API.... but
it's really verbose.  Or there is another way: I could just write an utility
method that would test if a Viewpoint matches an expected format.  We would just
descend on the TreeIterator given by Viewpoint.getAllContents, without having to
lookup features/classifiers by name.

** Adapting the tests                                              :emfviews:
Oh hey, Viewpoint.getAllContents() returns an empty array.

* [2017-07-20 jeu.]
** Adapting the tests                                              :emfviews:
Update on yesterday: Viewpoint.getAllContents() returns what I need.  But I
don't know what's the best way to test it.

It's a tree iterator.  I could check it against a simple Iterator, but that
would be checking against an infix walk of the contents, so you lose some
information.

Also, there's the matter of /how/ to describe the expected structure.  Using
only names is too restrictive.  It seems I need to be able to check, for each
level, the value of some feature.  So I need a tree of "feature: value".

I wish I was in Lisp... making and iterating over a tree would be trivial.

Using the reflective API is verbose, even with shortcuts.  Before I wrote this:

#+BEGIN_SRC java
EClass A = (EClass) ((EPackage) l.get(0)).getEClassifier("A");
assertEquals(0, A.getEStructuralFeatures().size());

EClass B = (EClass) ((EPackage) l.get(1)).getEClassifier("B");
assertNotNull(B.getEStructuralFeature("b"));
#+END_SRC

Now I have to write this:

#+BEGIN_SRC java
EObject A = (EObject) ecall(l.get(0), EcorePackage.EPACKAGE___GET_ECLASSIFIER__STRING, "A");
assertEquals(0, ((List) eget(A, "eStructuralFeatures")).size());

EObject B = (EObject) ecall(l.get(1), EcorePackage.EPACKAGE___GET_ECLASSIFIER__STRING, "B");
assertNotNull(ecall(B, EcorePackage.ECLASS___GET_ESTRUCTURAL_FEATURE__STRING, "b"));
#+END_SRC

When all I really want to do is to test the resource against an expected result
of:

#+BEGIN_EXAMPLE
- minimalA
  - A
- minimalB
  - B
    - b
#+END_EXAMPLE

* [2017-07-21 ven.]
** Adapting the tests for the reflective API on Viewpoint          :emfviews:
I went with a few helper methods to reduce the boilerplate, and now the tests
use the reflective API /and/ are more readable.

The "check an expected structure" seemed to cumbersome.

Here is yesterday's example:

#+BEGIN_SRC java
EObject A = getClassifier(l.get(0), "A");
assertEquals(0, getFeatures(A).size());

EObject B = getClassifier(l.get(1), "B");
assertNotNull(getFeature(B, "b"));
#+END_SRC

It made other places much neater:

#+BEGIN_SRC diff
-    @SuppressWarnings("unchecked")
-    EList<EObject> ea_labels =
-        (EList<EObject>) ea.eGet(ea.eClass().getEStructuralFeature("labels"));
-    @SuppressWarnings("unchecked")
-    EList<EObject> vea_labels =
-        (EList<EObject>) vea.eGet(vea.eClass().getEStructuralFeature("labels"));
+    EList<EObject> ea_labels = eList(ea, "labels");
+    EList<EObject> vea_labels = eList(vea, "labels");
#+END_SRC

Looking at the whole diff, all changes are as long or shorter than the previous
code.  Crucially, we don't have way fewer casts.  So I guess it's a win.

Now, does it help with the EStore backing for Viewpoint?

** Using EStore implementations for Viewpoint contents             :emfviews:
So the tests do not magically all pass using this reflective API, unfortunately.

Most of them fail trying to cast Boolean to EObject... That's weird.

: getClassifier(l.get(0), "A")

Okay, so that's because getClassifier calls o.eInvoke on an operation that we
got from o.eClass.  On regular EMF classes, that works, but on our virtual
element, the eInvoke call never goes back to our EStore implementation and
returns ~eIsProxy~ in EObjectImpl instead.

Does that mean that eInvoke does not work on EViews?  If so, that would probably
be a nail in the coffin for this approach.

Hmm, there are no operations on EViews.. for objects of the minimalref model.
Maybe it's because there is no generated code for this metamodel?

What about TOGAF?  Still empty.  eClass() always seems to be an EClassImpl, so
eOperations is a feature, but it's empty.

Hmm, I was under the impression that some operations were generated for
getter/setters of attribute, but it seems it's not the case.  Operations are a
separate type of element in the metamodel.  So the Ecore metamodel has
operations, but TOGAF doesn't seem to have any.

Okay, so the question remains: does eInvoke works for ReproduceElements created
by EView?  I could load Ecore as a model and try it out...

The nsURI is: http://www.eclipse.org/emf/2002/Ecore

Hmm, that's interesting.  No errors when creating the EView, but the contents
are empty.

As far as I can tell, in View.loadContributingModels, we do
virtualResourceSet.getResource() and as a side-effect the resources attributes
of virtualResourceSet contains the contributing models resources.

However, when given a model as HTTP, it /finds/ something and returns an
EResourceFactory (instead of, say, an XMIResourceImpl), but the resources
attributes remains empty.

Since this resources attribute is used to build the contents of the View, the
contents will be empty.

Getting the returned value and explicitly adding the resource to the
virtualResourceSet seems to fix it.

Finally: I do get a ReproduceElement around an EPackage.  Calling
eClass().getEOperations on this reproduceElem gives me...

: [org.eclipse.emf.ecore.impl.EOperationImpl@56c0a61e (name: getEClassifier) (ordered: true, unique: true, lowerBound: 0, upperBound: 1)]

But, using my getClassifier helper gives me the same Boolean to EObject class
cast exception.

So: it doesn't work on the EView ReproduceElement either.

I should write a self-contained test with an EStoreEObject backed by an EStore
and try an eInvoke on it.  But my guess is that it doesn't work out of the box,
and I don't know yet if there is a way to make it work.

* [2017-07-24 lun.]
** Finding out if EStore supports eInvoke or not               :emfviews:emf:
If it doesn't, I'll have to check DynamicEObject, or write our own EObject
implementation.

Writing simply:

#+BEGIN_EXAMPLE java
    EStoreEObjectImpl o = new EStoreEObjectImpl();
    o.eInvoke(EcorePackage.Literals.EPACKAGE___GET_ECLASSIFIER__STRING,
              ECollections.asEList("Foo"));
#+END_EXAMPLE

(I found a new way to get the EOperation)

Violates an assertion in BasicEObjectImpl: "The operation 'getEClassifier' is
not a valid operation".

Setting the class:

#+BEGIN_EXAMPLE java
    EStoreEObjectImpl o = new EStoreEObjectImpl();
    o.eSetClass(EcoreFactory.eINSTANCE.createEPackage().eClass());
    o.eInvoke(EcorePackage.Literals.EPACKAGE___GET_ECLASSIFIER__STRING,
              ECollections.asEList("Foo"));
#+END_EXAMPLE

returns ~false~.  A boolean.  Probably eIsProxy again.

After debugging, yes, we end up in EStoreEObjectImpl.eInvoke with the
operationID of 1, and it happens to be the ID of ~EOBJECT___EIS_PROXY~, so we
call that method and return false.

Am I using this wrong?  It looks like using eInvoke on EStoreEObject is used to
call /EObject operations/.  And I want it to /delegate/ eInvoke on a backing
object.  But EStoreEObject has no notion of a backing object.

It /does/, however, delegate attributes get and set to the backing store.

Using eGet on the EStoreEObject ends up calling dynamicGet(featureID), which
delegates to eStore().get().

Hmm.  But we could extend EStoreEObjectImpl to delegate eInvoke to the backing
store... but we would have to extend the store interface as well.  And,
ultimately, it seems that extending EStoreEObject is not the proper solution: an
EStoreEObject /delegates/ to a an EStore, but a VirtualElement aims to be
/transparent/.

Is DynamicEObject a better fit?

** Exploring DynamicEObject                                    :emfviews:emf:
Doing:

#+BEGIN_SRC java
DynamicEObjectImpl o = new DynamicEObjectImpl();
o.eSetClass(EcoreFactory.eINSTANCE.createEPackage().eClass());
o.eInvoke(EcorePackage.Literals.EPACKAGE___GET_ECLASSIFIER__STRING,
          ECollections.asEList("Foo"));
#+END_SRC

throws UnsupportedOperationEx "eInvoke not implemented for getEClassifier"

Promising!  We end up in BasicInvocationDelegate.dynamicInvoke, which will only
invoke the given operations if it is an EObject operation:

#+BEGIN_SRC java
 if (eOperation.getEContainingClass() == EcorePackage.Literals.EOBJECT)
 {
   switch (eOperation.getEContainingClass().getEAllOperations().indexOf(eOperation))
   {
     case EcorePackage.EOBJECT___ECLASS:
       return target.eClass();
     case EcorePackage.EOBJECT___EIS_PROXY:
       return target.eIsProxy();
 ...
#+END_SRC

Otherwise, it throws.

So it looks like we need to define an InvocationDelegate that does something
else.

An invocation delegate is a property /of an operation/.  It's a class that tells
the operation how it should execute itself given the arguments.  So we go from:

#+BEGIN_EXAMPLE
obj.eInvoke(operation, args)
eInvocationDelegate(operation).dynamicInvoke(obj, args)
BasicInvocationDelegate.dynamicInvoke(obj, args)
switch (operation id)
  ECLASS => obj.eClass()
  ECONTAINER => obj.eContainer()
  ...
#+END_EXAMPLE

It's a bit convoluted.

Invocation delegates are created through annotations... or we can set them
directly on the operation.

But I don't want to set them per operation.  I just want to delegate the eInvoke
call to a concrete object, which will know how to handle it.

So it looks like we'll have to build our own EObject implementation.  However,
DynamicEObject can serve as a reference of what is actually useful in there.
There is very little actual logic in that class, so it should serve as a guide.

I think we could have a DelegatingEObject class that just delegates to a
concrete element.  Then we could extend this Delegating class as VirtualEObject
which adds additional control related to views (potentially filtered, etc.).

* [2017-07-25 mar.]
** Using a DelegateEObject for virtualizing Viewpoint              :emfviews:
Have to make some adjustments.

We used a ResourceSet to hold the cloned and modified packages, and got them
back through registry.getEPackage.  However, getEPackage returns null when the
objects we put there are not instances of EPackage.

When using a DelegateEObject instead of cloning the EPackages, this cannot
work.  Instead, a simple Map from the package URI to the package object
(actually, a delegate) works.

Now, most of the tests pass; there is the issue that applyFilters just deletes
stuff from the original packages through the delegate.  We can't do that
anymore.

Instead, we should /hide/ elements.  Having a VirtualEObject class that extends
DelegateEObject and has a ~filtered~ field would work.  However, at the moment
we use DelegateEObject only at the package level, but we should make sure that
every object inside this delegated package is also delegated.

One way would be to iterate on the contents of each EPackage at first, and wrap
them in DelegateEObject.  Another would be to do it lazily at runtime, using
membranes.

Let's try to do it at construction time.

Hmm, I need the original structure of the package to guide me.  What if cloned
the packages, but then replaced every object inside by a delegate to itself?

ArrayStoreException

Argh, seems I can't just store anything I want here.  Okay, fine, let's go with
the membrane approach then.

First hole to plug is eContents: this gives access to non-Delegate object, and
we should change that.

Defining a MembraneEObject (for lack of a better name), that constructs a
MembreEList and wraps objects in the get(index) method.

Objects are saved in a Map to avoid creating duplicate wrappers (and to help
preserve reference equalify).

Problem: calling EcoreUtil.delete on a MembraneEObject doesn't work, because
they are casted as InternalEObject.

Well, good thing we don't need actually want to delete these objects then.  But
wait, filters don't work that I understand, but creating new elements should
work.

Hmm, that's because we are casting to EClass in order to be able to access and
modify getESuperTypes.  Can't we use the reflective API?

: ((EList<EClass>) sub.eGet(EcorePackage.Literals.ECLASS__ESUPER_TYPES)).add(klass);

It's not pretty, but we can.

Hmm, wait.  The addSubConcept is more interesting.

#+BEGIN_SRC java
EObject sup = findEObject(e, registry);
if (!(sup instanceof EClass))
  throw EX("Superconcept '%s' of new concept '%s' should be an EClass", e, c.getName());
klass.getESuperTypes().add((EClass) sup);
#+END_SRC

If sup is a MembraneObject, then we can't cast it to EClass obviously.  We could
use his delegate object... but should we?

It seems we have to, because we can't add anything other than an EClass a super
type.  MembraneEObject does not implements EClass, and if we start duplicating
Membranes to include MembraneEClass, MembreEReference, etc., then we are not
sharing code with the View level anymore.

What happens if we use the delegate?  It means we mix virtual classes and
original classes.  And it adds a way to get to original classes from synthetic
elements in the virtual package.

Hmm I think I see a solution:

     delegates to
 D_1 ------------> C_1
                   ^
                   |
 D_2 ------------> C_2

C_1 is the original concept, C_2 is a new subconcept.  Since we deal with
delegates, we can't subclass D_1 with C_2 directly.  However, we can subclass C_1
with C_2, and then expose a delegate D_2 to C_2.

That also solves the question of whether to expose synthetic elements directly
or through delegates.

* [2017-07-26 mer.]
** On yesterday's solution to the subconcept problem               :emfviews:
There's one downside: if the inheritance link is added to the concrete elements,
then we are modifying the existing model, rather than modifying the view.

The alternative is then to create the inheritance link between the delegates:

     delegates to
 D_1 ------------> C_1
 ^
 |
 D_2 ------------> C_2

But then, the delegates need to hold extra information.  Basically, we need to
have a VirtualClass object that implements EClass.

Trying the VirtualClass/VirtualProperty approach.  First VirtualProperty:

#+BEGIN_SRC java
public class VirtualEAttribute implements EAttribute {

  protected VirtualProperty virtualProperty;
  protected EAttribute backing;
#+END_SRC

It implements EAttribute to be transparent for EMF.  But it holds the
VirtualProperty instance it is supposed to represent.  The backing EAttribute is
used to hold all the relevant EAttribute fields, rather than duplicating them.

But now, this object cannot be cast to InternalEObject when doing:

: parentClass.getEStructuralFeatures().add(attr);

Well, we don't want to do that anymore anyway.  The VirtualEAttribute already
has a parent that is held by its VirtualProperty.  The more interesting question
is what happens when we interrogate our VirtualEAttribute?

We fail at:

#+BEGIN_SRC java
   EObject f = getFeature(A, "newProperty");
   assertNotNull(f);
#+END_SRC

~f~ is null, because we can't find the feature on A.  So in order to make this
work, we have to implement VirtualEClass as well, and create these.

Similar problem with VirtualEClass:

: virtualPackage.getEClassifiers().add(klass);

Cannot cast to InternalEObject.  Seems EMF doesn't want us to mix our homebrewed
virtual EObjects with regular EPackage.

Have to implement VirtualEPackage then.

Or maybe we can work around this restriction, since the only thing that we do
with our new concepts is to expose them in our VirtualContents.  If instead of
priving a package, we provide a list EClass, it could work.
