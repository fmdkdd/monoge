# -*- eval: (add-hook (quote org-babel-after-execute-hook) (function org-redisplay-inline-images)); -*-
#+STARTUP: inlineimages entitiespretty

* [2017-04-03 lun.]
** Notes on setting up for the MoNoGe project              :eclipse:emfviews:
Eclipse 3.8 does not start with Java 9 from Oracle.

: sudo update-alternatives java

-> java8-openjdk was installed on the machine

Installed Eclipse 4.6 anyway.

Additional dependencies for the project:

- ATL
- Xtext
- Epsilon (http://download.eclipse.org/epsilon/updates/)

First two can be installed from Eclipse (Help -> Install modeling components).
Epsilon has an update site.

For running the ECNA2014 demo, I will probably need:

- BPMN 2
- RMF (ReqIf)

Short-term tasks:

- Re-run the examples from the video with the source code
- Learn EMF
- Draw a diagram of how EMFViews works, structurally (what plugin does what, how
  they fit together, etc.)

** Importing EMFViews into Eclipse                         :eclipse:emfviews:
For some reason, when importing the plugin projects, Eclipse thinks the packages
in the src/ folder begin with src.  Save for the very first one.

I just need to open the project properties, go to build path, and click apply.
The src folder is already the build path, but the setting is not applied
correctly.

Then, I also need to add the "plugin dependencies" library for each project.

Doing that for all plugins...

There are still errors from missing dependencies.  Why is there no "one-button"
install?

Finally, some discrepancies in version numbers for our own plugins, that could
be tracked to the way different versions of the plugins have been merged into
the repository.

Only 7 errors left out of >1000 thousands when I started.  The remaining errors
I have to look into the APIs of the libraries to see if some things have changed
since 2013.

* [2017-04-04 mar.]
** Inria provided infrastructure                                   :atlanmod:
R told me about continuous integration server provided by Inria.  They run on
jenkins, and we can have a decent amount of VMs apparently.

They also host a GitLab service, so we could host repositories there as well.

** Understanding the code                                          :emfviews:
There's a feature.xml that seem to describe the EMFViews feature, with juicy
info like dependency with version numbers:

: <import plugin="org.eclipse.uml2" version="4.0.0" match="greaterOrEqual"/>
: <import plugin="org.eclipse.emf.ecore.xmi" version="2.7.0" match="greaterOrEqual"/>
: <import plugin="org.eclipse.uml2.uml" version="4.0.0" match="greaterOrEqual"/>

Not all dependencies have explicit version numbers, but that's a start.

There's also a discrepancy in version numbers: in the feature.xml file, all the
plugins have the version "0.2.0.qualifier", but in the MANIFEST.MF for the
EMFViews plugin, we ask for 1.0.0 versions:

: Require-Bundle: org.eclipse.emf.ecore,
:  org.eclipse.emf.ecore.xmi;bundle-version="2.7.0",
:  fr.inria.atlanmod.emfviews.vlink-mm;bundle-version="1.0.0",
:  org.eclipse.core.resources;bundle-version="3.8.1",
:  fr.inria.atlanmod.emfviews.virtuallinksdelegator;bundle-version="1.0.0",

I understand we also have code generated from ECore models (vlink-mm) which is
checked in the repo.  Since the code is generated from the model, it might make
more sense to not check it in, and regenerate after a ~build~ step.

Other discrepancy: copyright attribution on the feature.xml file is for Inria
Rennes Bretagne Atlantique, while the rest of the plugin, you can see individual
contributor names.  Which is it?

* [2017-04-05 mer.]
** Exploring Eclipse                                     :eclipse:
Specifically, I'm interested in understanding how and where Eclipse saves
preferences for a project: dependencies, how to build, etc.

That's important when putting these things into a repository.

For instance: when I "Run as.. Eclipse Application" an Eclipse plugin like the
one from this [[http://eclipsesource.com/blogs/tutorials/emf-tutorial/][EMF tutorial]], it runs the new Eclipse under another workspace.
The path of this new workspace is specified by the "Eclipse Application" run
configuration.

By default, it is:

: ${workspace_loc}/../runtime-EclipseApplication

So it creates a folder above the current workspace.  Since my workspace is in
$HOME, it creates $HOME/runtime-EclipseApplication.  But, for the purpose of the
tutorial, the files I create in this new workspace should reside inside the
repository.  So, I rather want:

: ${project_loc}/../runtime-EclipseApplication

so it creates the folder at the same level of the project folder I am running.

Now, where is this preference saved?

: $ rg --hidden "project_loc" ./eclipse/ eclipse/ workspace/ proj/monoge/emftuto/
: workspace/.metadata/.plugins/org.eclipse.debug.core/.launches/Eclipse Application.launch

In my workspace.  I guess it makes sense since "Run as..." is an Eclipse thing,
so it should be an Eclipse pref.

But, if you checkout the repository, you would have to manually replicate the
"Run as..." setting in order to get my examples working.

One solution is to export the workspace preferences and put it in the
repository.  So at least, if you use Eclipse, you can import these settings as a
one-click solution.

** Speeding up Eclipse                                              :eclipse:
Even on a powerful and recent machine, Eclipse is quite slow and feels
unresponsive.  Things to set in eclipse.ini to ([[https://www.eclipsecon.org/europe2015/sites/default/files/slides/Boosting%2520the%2520Performance%2520of%2520your%2520Eclipse%2520IDE_0.pdf][allegedly]]) speed it up:

#+BEGIN_EXAMPLE
-server
-Xms512m
-Xmx2g
-Xmn512m
-Xverify:none
-XX:+AggressiveOpts
-XX:+UseParallelGC
#+END_EXAMPLE

Removing unnecessary stuff also helps: Git control, startup plugins,
auto-updates, etc.

Curiously, in the Modeling Tools distribution, there is another EGit plugin that
you also have to remove.

** Trying to replicate the ECNA2014 demo                           :emfviews:
I've got the emfviews plugin building without errors in Eclipse.  Run
as... application (I guess?)

Now to create an EAdata project, add the travelAgencyEA.xmi.

Uhoh, error:

: http://www.obeonetwork.org/dsl/togaf/contentfwk/9.0.0' not found

Hmm hmm.  Maybe I need [[https://github.com/ObeoNetwork/TOGAF-Designer][this plugin]]?

Getting the source, adding the plugin (subfolders) to the EMFViews workspace,
and adding them as a dependency for the EMFViews plugin, running "Run as.."
again.  I can check in Help -> Installation details of the recursive Eclipse
that EMFViews and TOGAF plugins are both present.

After that, opening the travelAgencyEA.xmi again yields:

: org.eclipse.emf.ecore.xmi.FeatureNotFoundException: Feature 'belongsTo' not found. (platform:/resource/EAdata/models/1_travelAgencyEA.xmi, 26, 270)

A bunch of them.  'isOwnedByUnit', 'communicatedWithFunctions',
'providesEntities', 'containers', 'labels'.

And some IllegalValueExceptions:

: Value 'BusinessService[TRANSIENT]' is not legal. (platform:/resource/EAdata/models/1_travelAgencyEA.xmi, -1, -1)

Could it be a mismatch between the TOGAF version I'm using and the one used in
the demo?

Grepping around the TOGAF repo, I can see hits for these strings, especially in

: plugins/org.obeonetwork.dsl.togaf.contentfwk/model/contentfwk.aird

#+BEGIN_EXAMPLE
1277:      <ownedDiagramElements xmi:type="diagram:DEdge" xmi:id="_eyUzMP63Ed-AK7xgn-H1PA" name="[0..1] isOwnedByUnit - [0..*] ownsFunctions" sourceNode="_ugYTwJ-9Ed-hg-_nMagkzg" targetNode="_zjeaMJ-9Ed-hg-_nMagkzg">
1280:        <semanticElements xmi:type="ecore:EReference" href="contentfwk.ecore#//Function/isOwnedByUnit"/>
2562:        <lines xmi:type="table:DLine" xmi:id="_BsDvz6AREd-mRqry0T_xvQ" label="EReference : isOwnedByUnit">
2563:          <target xmi:type="ecore:EReference" href="contentfwk.ecore#//Function/isOwnedByUnit"/>
2564:          <semanticElements xmi:type="ecore:EReference" href="contentfwk.ecore#//Function/isOwnedByUnit"/>
2567:            <target xmi:type="ecore:EReference" href="contentfwk.ecore#//Function/isOwnedByUnit"/>
#+END_EXAMPLE

I can also see these strings in the EA.ecore metamodel, and these looks like
definitions.

Maybe the example was self-sufficient after all.  But how to link the XMI file
to an ECore metamodel?

Further investigation.  If I load up the TOGAF contentfwk model (one I found to
load it is to create a dummy ECore file, and Right click -> Load resource, and
look for the obeo URL near the bottom).

I can see the mismatch.  The TOGAF contentfwk has, in the Function class:

- communicatesWithFunctions
- isOwnedByOrganizationUnit

whereas the EA.ecore has:

- communicatedWithFunctions
- isOwnedByUnit

So the errors make sense.  And, it would seem that I need to point the XMI files
to use my EA.ecore metamodel.

Looking for help.. G helpfully answered my questions:

You can't link an XMI to an Ecore as-is.  Eclipse has a global registry of
metamodels, and there is no way to Right-click on an Ecore file, and add it to
the registry.

(Unless, possibly maybe, through some ATL plugin, but I don't have it installed
right now)

The alternative is to generate plugin code from the Ecore metamodel, and "Run
as.." again.

So:

In the base Eclipse workspace (where the EMFViews plugin resides), create an
empty EMF project add the EA.ecore file to the model folder.  Create a genmodel
file, and generate the model code.

Then, launch the recursive Eclipse.  All open projects will be loaded as
plugins, so no need to add this project (or TOGAF) as dependency to EMFViews.

This time, I can load the XMI without troubles.

The two other XMI files require BPMN2 and ReqIf10.  At this point, I'm
remembering that the emfviews repo had a "dropins" folder containing JAR files
for BPMN, ReqIf and.. TOGAF contentfwk.  And eclipse has a dropins folder as
well.

I'm thinking that the dropins are supposed to be added to Eclipse in order to
run the examples.

And indeed, it works.
For BPMN2 and ReqIf.  For TOGAF, I get a NullPointerException when trying to
open the Ecore file.  So maybe sticking to what I have generated is better.

Next step in the demo is to create an EMF Viewtype through an Eclipse wizard.  I
don't have that wizard in my recursive Eclipse.  Probably because the "editor"
and "ui" plugin projects are closed in my base Eclipse.

Let's open them.

Uhoh, compile error.

: Viewtype.getHiddenElements()

is undefined.  There is a ~getHiddenAttributes()~ method though.  Let's try
that.

Haha!  Now I have the wizard in my recursive Eclipse.  Clicking next
enthusiastically and:

: java.lang.ArrayIndexOutOfBoundsException: 0
: 	at fr.inria.atlanmod.emfviews.ui.wizard.view.CreateViewtypeScreen.createControl(CreateViewtypeScreen.java:132)

:(

Time to quit for the day.

* [2017-04-07 ven.]
** IndexOfOfBounds exception                                       :emfviews:
Okay so, the incriminating line:

: comboLinksDsl.setText(availableLinksDsls[0]);

We made no provision to check that this array (of strings) had any element, and
we access the first one.

Let's add a check.  I guess I'll be greeted by an empty window, since it means
we haven't found any LinkDSL, but at least I won't crash.

(Though, like JavaScript, Eclipse doesn't crash: it just throws an Exception and
keeps going).

Changing the array value at runtime in the debugger to {"foo"} let me proceed.
I can add the metamodels.  But I can't select any linking DSL (since there is
none, and "foo" isn't a valid one I'd wager).

(To change an array of String in the debugger, right-click -> change value, and:

: return new String[]{"foo"}

primitive values are easier to change, usually just click.)

Still, it let me proceed to the next screen.  Of course, "Finish" triggers a
NullPointerException.

In the demo video, he has "ecl" as DSL language.

Looking at the code that populates availableLinksDsls, it iterates into:

#+BEGIN_SRC java
extensions = Platform
  .getExtensionRegistry()
  .getExtensionPoint("fr.inria.atlanmod.emfviews.virtuallinksdelegator.type")
  .getExtensions();
#+END_SRC

But this is also empty.  I guess because I haven't opened
virtuallinksepsilondelegate.  Let's do that.

I need ECL to compile it.  It comes from org.eclipse.epsilon.  Let's not install
that into Eclipse, but put the JAR in dropins instead.  Wait no, I don't want it
to run as plugin in my Eclipse.  I just need to add it as a dependency to the
project.

Import errors disappear.  Other errors appear.  Deprecated methods
and... methods that are not here anymore.  Presumably because I got the latest
ECL version, and the project used another one.  Is there any trace of the
versions we used previously?

In emfviews/feature.xml:

: <import plugin="org.eclipse.epsilon.ecl.engine"/>
: <import plugin="org.eclipse.epsilon.eol.engine" version="1.0.0" match="greaterOrEqual"/>

Let's try the 1.0.0 version then.  It's from 2012.  I guess the project was
working in 2016, so let's try the 1.3 instead.

1.3 makes one error disappear, still 2 left.  1.2 has only deprecation warnings.

Still errors in the MANIFEST file for unmet dependencies.  But it's for stuff we
don't need, otherwise we wouldn't compile, right?  Let's ditch them.

Ah!  I've got "ecl" in the dropdown menu now.  But clicking "Finish" triggers a
NullPointerException in CreateViewtypeWizard.  It's because we want to open the
newly created .eviewtype using our editor for that file type, and we fail in
Viewtype.loadFilterMetamodel:

#+BEGIN_SRC java
private void loadFilterMetamodel(String filtersMetamodel) {
  ResourceSet filtersResourceSet = new ResourceSetImpl();
  attributesToHideMM = filtersResourceSet.getResource(URI.createPlatformResourceURI(filtersMetamodel, true), true);
}
#+END_SRC

because filtersMetamodel is null at this point.

Culprit: Viewtype.doLoad which pass

: loadFilterMetamodel(properties.getProperty("filtersMetamodel"));

but there is no "filtersMetamodel" property there.  "properties" is created from
parsing an inputStream which seem to correspond to the contents of the eviewtype
file.  And the eviewtype file has no filtersMetamodel value.

When is this written?

Line 545, in Viewtype.serialize.  It puts the value of ~filtersMM~, which is a
String, and populated in the constructor of Viewtype.  Hmm, so that's actually
just the serialization of a Viewtype, but since the constructor already calls
loadFilterMetamodel, I guess this is the wrong place.

In CreateViewtypeWizard.performFinish, we are writing to the eviewtype file.
Then it calls

: serializeViewtype(viewTypeFile, fileContent);

Stepping through CrewteViewtypeWizard.performFinish, there is no code adding the
"filtersMetamodel" line.  And I see no trace of code that /would/ add these
lines to the file.

Also, in the video, there are four files created by the wizard: an ECL, an
Ecore, an EViewtype and an XMI.

I've only got two: EViewtype and XMI.

Strange.

** Reading about Eclipse as a platform                              :eclipse:
http://www.aosabook.org/en/eclipse.html

All classes in a plugin are not considered part of the plugin API.  You need to
define extension points for that.  Visibility of class/method/attributes
is presumably restricted to your plugin.

At Eclipse startup, all plugins manifests are scanned to know the extension
points in advance, but the plugins themselves are not loaded.  It's very much
like Emacs autoloads: they give an example of a plugin adding a menu item, and
only when the user clicks on the menu item will the corresponding plugin be
actually loaded.

Instead of Swing or AWT, Eclipse uses SWT as widget toolkit.  JFace comes on
top, and provides frameworks for preferences and wizards.

Hmm the bit about plugin class visibility is somewhat in contradiction in \sect6.2:

#+BEGIN_QUOTE
If plugin A requires plugin B, plugin A can see all the Java classes and
resources from B, respecting Java class visibility conventions
#+END_QUOTE

Ah I get it know: the above describe the situation before the move to OSGi, and
the paragraph at the start describes the situation after the move.

#+BEGIN_QUOTE
With the switch to OSGi, Eclipse plugins became known as bundles. A plugin and a
bundle are the same thing [...]  Previously, dependencies, exported packages and
the extensions and extension points were described in plugin.xml. With the move
to OSGi bundles, the extensions and extension points continued to be described
in plugin.xml since they are Eclipse concepts. The remaining information was
described in the META-INF/MANIFEST.MF, OSGi's version of the bundle manifest.
#+END_QUOTE

Good news: OSGi supports semantic versioning, very much like SemVer:

major.minor.service.qualifier

Increment major when breaking API
Increment minor when adding API
Increment service for bug fixing
Qualifier is used to indicate a build tag

It's OSGi that takes care of resolving dependencies for a package.

Ah: apart from the extension registry in Eclipse, there is also a service
registry provided by OSGi.  Unlike extensions, services can be discovered
dynamically, after startup.

#+BEGIN_QUOTE
A feature is a PDE (Plugin Development Environment) artifact that defines a set
of bundles that are packaged together in a format that can be built or
installed. Features can also include other features.
#+END_QUOTE

p2 has replaced Update Manager for provisioning Eclipse.  Might be useful for
continuous integration.

** What if I provide filtersMetamodel myself?                      :emfviews:
Since it seems this line is not going to write itself in the eviewtype file,
might as well put it, just to see if the rest of the demo can work.

Ah yes, of course, it points to an ECore file that was also not generated.
Let's bring that in.

Hmm, this time I have a NPE in ViewtypeEditor.createViewtypeTreeEditorPage,
at this line:

: treeViewer.setInput(((Viewtype) viewtypeResource).getResourceSet().getPackageRegistry().values());

because the ~getResourceSet()~ returns null.  The viewtypeResource is populated
from the file at the beginning of the try block:

: viewtypeResource.load(uri.toURL().openStream(), new HashMap<Object, Object>());

and after that, the resourceSet attribute is null.

Since viewtypeResource is an EMF Resource object, maybe a change in the EMF API?

I am tempted to try archeology and rebuild an environment circa 2014.  Here I've
got EMF Ecore 2.12.0 and the feature... has no version requirement.

But, there are version requirements for:

: <import plugin="org.eclipse.emf.ecore.editor" version="2.8.0" match="greaterOrEqual"/>
: <import plugin="org.eclipse.emf.ecore.xmi" version="2.8.1" match="greaterOrEqual"/>

and I've got 2.12 loaded, again.  So since these are part of EMF, a safe guess
would be to find EMF Ecore 2.8.

2.8 is from 2012.

Now, according to the semantic versioning, that shouldn't change anything,
right?

Hmm, trying to put the EMF 2.8 JARs into a copy of my Eclipse Neon 3 resulted in
lots of deep stack straces and an error at launch.  p2 couldn't resolve the
frankenEclipse I created I guess.

Let's get a MDE Eclipse circa 2012 then.  Luna is the first version supporting
Java 8.  That's 2014; maybe it will still make a difference.

Ah of course, I have to set up a new workspace.

Hmm, just importing the projects, and everything builds without errors.
Adding dropins, TOGAF project...

And same exact error!  ViewtypeEditor.createViewtypeTreeEditorPage.

So I guess I would like to know what a ResourceSet is, and what value it should
take at this point.  Maybe brush up my EMF knowledge.

* [2017-04-10 lun.]
** Trying to replicate friday's situation: new error       :eclipse:emfviews:
Can't even get to the EViewtype creation wizard as Eclipse crashes on load with:

: org.eclipse.core.runtime.CoreException: Plug-in "fr.inria.atlanmod.emfviews.virtuallinksepsilondelegate" was unable to instantiate class "fr.inria.atlanmod.emfviews.virtuallinksepsilondelegate.EclDelegate".

How in hell did it work Friday?

Looking at changes I did on the project in Git... wow, there are .class files
checked in.

There are also ~._trace~ files, which I understand are generated by Xtex.  Since
I'm not dealing with Xtext there, and these are generated files, I'd rather get
rid of them.

Some .classpath are checked in, some are not.

Let's just remove the useless files and ignore them to get a usable git diff.

Now, I did modify the MANIFEST which included ECL.  Maybe this wasn't a good
idea?

If I restore these lines, Eclipse complains that it cannot resolve bundles
pertaining to ECL in the MANIFEST of virtuallinksepsilondelegate.

The JARs are in the build path, but maybe they need to be loaded into Eclipse
instead.  Let's remove them from the build path.  Cannot build now because
imports are not resolved.  Let's add them as drop-ins.

Hmm, they don't seem to be recognized when added to the dropins folder.

Opening Window -> Show view -> Error log displays the errors when loading
Eclipse.  To start, I can see that it's trying to load the Git plugin for each
project, even though I've removed it.

Removing all org.eclipse.team plugins fails to start Eclipse.  It's not as
modular when many pieces depend on each other!

Restoring team.core and team.ui did the trick.  At least I got rid of CVS.
I might investigate a minimal Eclipse setup another time.  And one that can be
auto-provisioned trough a config file for reproductibility.

Still have GitProvider errors.  Why is it trying to load the plugin?  Since I
see to Git-related feature under eclipse/features, I'm guessing Git is tightly
integrated into another feature that is still being loaded.  Ugh.

In any case, I can see errors related to my dropins JAR:

#+BEGIN_EXAMPLE
!ENTRY org.eclipse.equinox.p2.publisher.eclipse 4 0 2017-04-10 15:04:47.288
!MESSAGE Unable to acquire PluginConverter service during generation for: /home/fmdkdd/eclipse/dropins/epsilon-1.2-emf-src.jar.

!ENTRY org.eclipse.equinox.p2.core 4 0 2017-04-10 15:04:47.411
!MESSAGE Provisioning exception
!STACK 1
org.eclipse.equinox.p2.core.ProvisionException: No repository found at jar:file:/home/fmdkdd/eclipse/dropins/epsilon-1.2-emf-src.jar!/.
#+END_EXAMPLE

(the Error log view just pretty prints the content of .metadata/.log ... without
giving you the ability to copy lines; and why is there a hidden file in a hidden
folder?  Grmpf).

Maybe, the 1.2 JAR of Epsilon is not OSGi compliant or whatever.  Let's try to
dropins the 1.4.

Hmm nope, same error.

Okay then let's install them from inside Eclipse.  Version 1.2, preferably.

That does get rid of the build errors and MANIFEST errors.  Now, to run.

I have worrying warnings in my .log though:

: org.eclipse.core.runtime.CoreException: Executable extension definition for "class" not found.

But, I can launch the recursive Eclipse.  So, lessons learned:

Required bundles are actually runtime requirements?

** Explaining the discrepancy with the 2014 demo                   :emfviews:
According to H, the previous engineers might have already started some
refactoring in the goal of simplifying Viewtype creation.  In the demo, we see
an Ecore file being created along the Eviewtype.  That's something H did not
want, since we could just register the selected filters in the XMI itself.

In the current version, there is no Ecore file being generated, and the
XMI contains the line:

: <linkedElements elementRef="//Process" modelRef="http://www.obeonetwork.org/dsl/togaf/contentfwk/9.0.0" name="Process" estructuralFeatures="isAutomated"/>

which corresponds to what I've cliked on in the last step of the wizard.

In emfviews.core.Viewtype, there is a serialize method that added the
filtersMetamodel line that was in the demo.  It isn't called anymore by the
wizard.

Looking through the history of this file and in the commit history, all I can
see is that there was a first version of emfviews (0.1 I presume) with a vastly
different Wizard.  Then there was a version 2 (0.2), where the wizard was
changed to basically what I have inherited.

In any case, we don't have a definitive reference of source of truth for "how it
should work" other than examples and videos.  Possibly outdated.

Better brush up my knowledge of Eclipse plugins and EMF.

** A clean Eclipse                                                  :eclipse:
I've found the minimal Eclipse experience:

From [[http://download.eclipse.org/eclipse/downloads/][this page]], go to the latest release, and grab the "Platform runtime binary"
for your arch.

Very snappy Eclipse.  No crap like Mylin and EGit installed by default.

Then, I can pass a configuration folder on the command line:

: eclipse -configuration ~/eclipse-configs/test/configuration

From there, I can install new software from inside eclipse, and they will be
installed at eclipse-configs/test/plugins.  The eclipse-configs/test folder
becomes the new home folder for eclipse.

No pollution between configurations.

So that's good for reproductible environments (except, you know, the manual step
of actually provisioning Eclipse with the new packages).

But now if I need to install the Java dev tools for every configuration... or
even update them... that's going to be a pain.  Ideally, I read somewhere that
p2 was able to pool from a common bundle.  So I should be able to download all
this stuff in just one place, then let Eclipse get the plugins from one pool.

But I really don't want Eclipse to load /all/ the plugins in the pool.  Even if
it's "lazy-loading" them, it's still taking ages AND I have no say in how
what plugins are /actually/ loaded even if I don't use them.

* [2017-04-11 mar.]
** Trying bundle pools in Eclipse                                   :eclipse:
For speeding up provisioning, and making updates more sane.

I tried to use Oomph, which is actually the Eclipse Installer, the default
download provided by Eclipse.  In the advanced mode, you can select the Eclipse
Platform, and additional projects.  Except, these are pulled from master and put
/into/ your workspace; not additional plugins.  The use case this solved is when
you want to contribute to some Eclipse project.

It also works with any Github project, so I guess you could use Oomph to
somewhat easily provision an Eclipse to work on EMFViews.  You just have to say
"pick the MDE product, then add EMFviews, done".

But in my case I don't want all the cruft.

What you should be able to do is run Oomph, install an Eclipse platform, and
that Eclipse will be setup to get its bundles from the bundle pool.  Except
Oomph fails to install Eclipse platform.

Well, so much for saving bandwidth.

** Installing EMF tools in the platform                             :eclipse:
So installing the EMF sdk feature is apparently not enough to run the tutorial.
I lack emf.edit.  Even though they are part of EMF Core, according to [[http://www.eclipse.org/modeling/emf/][the
website]].

Maybe I'm not pulling from the correct update site?

Adding the update site mentioned on [[http://www.eclipse.org/modeling/emf/updates/][this page]] does not work.  After adding it to
the available sites list in the preferences, and loading up the preferences
again, the site has mysteriously disappeared.

But, there is an update site with URL:

: http://www.eclipse.org/modeling/updates/

disabled by default.  Enabling it and going through "Install new software",
selecting it...

waiting a long time...

Now it has added a bunch of other sites (what?)

And I can install EMF... 2.7.  From 2012.

Gosh, why is this so hard?

On [[http://www.eclipse.org/modeling/emf/downloads/][this page]], it seems I can /download/ an update site containing EMF.  So let's
try to add this ZIP as a local update site.

It does not disappear from the list when I leave it there.

And it's lightning fast when I go into "Install new software".  And it's version
2.12

But EMF Edit is grayed out, since it's already installed.

Removing... installing from this local update site...

EMF edit plugin is marked as loaded in Eclipse, but it's marked as not found in
the plugin.xml dependencies.

Let's remove everything EMF related, and try to load it as a dropin.

There's no dropins folder in my test configuration.  AAArgh.

There's one in eclipsen/platform though.

Hmm, maybe I only have the /runtime/, and not the SDK.  That's another download
on the page.  Let's try that.

Well, the SDK feature seems to only add documentation and source.  No
difference.  Other than that, still not finding emf.edit.

Ah, it works!  [[https://www.eclipse.org/forums/index.php/t/134617/][This thread]] was golden.  Apparently, for building plugins, you
need to setup the Target Platform correctly.  And, for some reason, even though
in my two workspaces they target platforms are set up correctly, they do not
find the same plugins.  One finds 190 plugins, the other 316.

Trying to clean up my test configuration now... trying to install things in
dropins, but that's a BadIdea.  Stuff's missing.  "Install new software" works
when I pull from the default update sites.

Installing PDE, JDT and EMF is enough to be able to run the tutorials.

** Eclipse plugin tutorial                                   :eclipse:plugin:
Following "Eclipse 4 plug-in development by example", by Alex Blewitt, Packt
Open Source.

Plugins which add to the UI or require the UI to operate conventionally have
'ui.' in their package name.

MANIFEST.MF file is for dependencies (OSGi-related stuff).  While the plugin.xml
file is for describing extensions and extension points.

Having extensions described as XML speeds up plug-in loading: you don't have to
execute any code of the plugin (though you do need to parse XML).

** Some links on building plugins with Maven+Tycho           :eclipse:maven:
https://zeroturnaround.com/rebellabs/building-eclipse-plug-ins-with-maven-3-and-tycho/
http://www.vogella.com/tutorials/EclipseTycho/article.html

* [2017-04-12 mer.]
** Eclipse plugin tutorial (cont.)                           :eclipse:plugin:
Clock tuto.

Some issue with Display.getDisplay() that crashes when launching Eclipse with
multiple monitors.  Did not happen when hot loading the code.

How are you supposed to get the current display then?

* [2017-04-14 ven.]
** Eclipse plugin tutorial (cont.)                           :eclipse:plugin:
Re: error from last time.  [[http://stackoverflow.com/questions/33157856/getting-swterror-not-implemented-multiple-displays-with-simple-code-sample][Found someone]] who raised the error on SO.  No answer,
no fix.

From what I gather, you /shouldn't/ use Display.getDisplay, since it creates a
new display (that you need to dispose of).

(Also, the error has nothing to do with multiple /monitors/, but multiple
Display objects as understood by Eclipse.)

If I use Display.getCurrent instead, I get null back, since no display has been
created when Activator.start is called.  Another suggestion is to use:

: PlatformUI.getWorkbench().getDisplay()

this also fails on startup with:

: java.lang.IllegalStateException: Workbench has not been created yet.

Again, it seems the plugin is started very early in the process.  One workaround
would be to create the tray item as soon as the workbench started.  This is
[[https://wiki.eclipse.org/FAQ_Can_I_activate_my_plug-in_when_the_workbench_starts%253F][possible]].

Yep, this works nicely.

** Resource management in JFace and SWT
SWT has manual resource management: when create instance of Color or Image, you
are supposed to .dispose() of them when you don't need them anymore.  That way,
SWT, releases the associated native objects.

JFace has resource registries to deal with the allocation and disposal of resources.

* [2017-04-19 mer.]
** Nearly done with the Eclipse Plugin book                  :eclipse:plugin:
Lots of learning were had.

Chapter 9 touches automated testing with JUnit.  Nothing fancy; plugins just
need to run with a special JUnit configuration.

More interesting is the UI testing with SWTBot, to simulate click and go through
the UI programmatically.  It's fun seeing Eclipse launch and crunch through
dialogs at inhumane speeds.

Although, even if the JUnit bar fills with green, I get a bunch of Exceptions in
the host console after the tests are run.  Presumably, SWTBot is too fast for
Eclipse, and does not take care of disposing some resources properly when
exiting then client instance.

: org.eclipse.swt.SWTException: Failed to execute runnable (org.eclipse.swt.SWTException: Widget is disposed)

** Building the plugins with Maven+Tycho                      :maven:plugin:
Following Chapter 10 of the book.  Apart from writing XML files, it's rather
smooth.

: mvn clean package

seems to poll the Eclipse update sites on each build, which takes a loooooooong
time.  You can avoid that check with the ~--offline~ flag:

: mvn --offline clean package

Hmm can't seem to run the SWTBot tests using Maven.  Might be that the book is
slightly outdated, as it was tested with Tycho 0.18, whereas we now have Tycho
1.0.0.

[[http://www.vogella.com/tutorials/EclipseTycho/article.html][This tutorial]] is fresher.

Still have troubles loading requirements for the test... It seems the client
Eclipse launched by Maven is really barebones (good): there are basically no
views!

I should at least get the Clock View that the tutorial plugins adds.

From what I understand, Maven /should/ obey the plugin dependencies in the
MANIFEST file.  But the runtime target configuration is different.

Maybe try to test with a barebones run configuration in Eclipse itself.

* [2017-04-21 ven.]
** Getting Maven+Tycho to run the tests                        :maven:plugin:
Trying with a barebones run configuration in Eclipse.

Only adding "Required plugins".  Cannot validate due to a not very talkative
error:

: org.eclipse.e4.ui.workbench.swt [9]
: Unresolved requirement: Require-Capability: osgi.extender; filter:="(&(osgi.extender=osgi.component)(version>=1.2)(!(version>=2.0)))"

Maybe it's a [[https://bugs.eclipse.org/bugs/show_bug.cgi?id=494913][bug]]?  Anyway, adding org.eclipse.equinox.ds and clicking "Required
plugins" (for the requirements of equinox) fixes it.

Another way to do it is to add equinox.ds to the dependencies in the MANIFEST,
saving it, and and clicking "Required plugins".  But at this point I don't know
if these dependencies should be declared in the MANIFEST itself...!

Now:

: org.eclipse.core.runtime.AssertionFailedException: null argument:Could not find IExtension for application: org.eclipse.ui.ide.workbench

I know that it works if I select all plugins in the Target Platform, so the
question is: which plugin is missing?

: org.eclipse.ui.ide.application

seems to do it.  At least the client Eclipse runs, but the tests all fail, and
the console is full of:

: Event Admin service is not available, unable to publish event org.osgi.service.event.Event

Adding org.eclipse.equinox.event to the dependencies solved it.  Thanks [[https://www.eclipse.org/forums/index.php/t/293382/][thread]].

Now only the third test fails:

: org.eclipse.swtbot.swt.finder.exceptions.WidgetNotFoundException: Timed out waiting for tree item General

Because when SWTBot does File->New->Project, the dialog is empty.  There's no
General folder.  There's only "Project", no categories.  Hmm, then I can just
use "Project" without adding another dependency.

It works.. not.  It creates the project, but the assertion fails.  Somehow it
runs too fast and does not wait for the project to be created.  Adding a
bot.sleep does it, but there's a nicer way with wait conditions.

Okay so now, back to Maven.

: Tests run: 3, Failures: 0, Errors: 0, Skipped: 0

Actually, before that, I had to remove one testUI that's just too brittle, the
one testing against the String value of the SWTBot shells.

But, it works.

** General best-practices for Eclipse plugins               :eclipse:plugins:
In Mastering Eclipse Plugin Development, by the same author as the tutorial book
I finished this week, there's a chapter Designing Modular Applications with some
pointers on best practices for Eclipse plugins.

How semantic versioning works for Java.  Like semver, adding a method is a minor
increase, changing or removing API is a breaking change.  In Java, adding
methods to interfaces is a breaking change, since classes that implement this
interface have to be modified.  Interfaces can be @noimplement, and adding
methods to these is only a minor version increase.

There's also the @since annotation which is rather useful.

There are tools for looking at the API of your releases and suggesting the
correct semantic version increases, like the API baseline in Eclipse.  Maven can
also do it.

** State of EMFViews                                               :emfviews:
We have no tests.  But, there are 15 examples in the examples/ folder.  Do they
still all work?

I've already asserted that the project is not in the state of running the ECNA
demo from 2014.  (Might not be too far, but things have changed since the demo
at least).

The examples are mostly models: XMI, Ecore, UML files.  Cloc is in fact totally
unable to give me a count, as it ignores basically all files except one in this
folder.

More interesting is when I cloc the plugins directory:

#+BEGIN_EXAMPLE
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
Java                           223          12613          19404          36444
Assembly                         3              0              0           6062
XML                             14             44             18            978
-------------------------------------------------------------------------------
SUM:                           240          12657          19422          43484
#+END_EXAMPLE

That seems like a lot of Java.  And assembly, strangely.  But many Java files
are in fact generated.  Let's look at only emfviews.* plugins.

#+BEGIN_EXAMPLE
-------------------------------------------------------------------------------
Language                     files          blank        comment           code
-------------------------------------------------------------------------------
Java                            58           1268           3376           4856
XML                              7             11              2            149
-------------------------------------------------------------------------------
SUM:                            65           1279           3378           5005
-------------------------------------------------------------------------------
#+END_EXAMPLE

Less daunting.  Many files are smallish to tiny:

: cloc --by-file plugins/fr.inria.atlanmod.emfviews.* | cut -c180-190

#+BEGIN_EXAMPLE
588
272
234
204
188
179
173
167
154
153
139
134
129
126
126
113
110
106
98
93
83
82
77
77
74
73
70
65
60
58
56
55
54
51
50
48
43
38
37
31
24
24
21
20
20
19
16
15
15
15
15
15
15
12
11
11
10
10
10
9
7
7
6
5
5
#+END_EXAMPLE

The largest one is emfviews.editor.editors.ViewtypeEditor, which looks all hand
coded and not generated.

[Still have issues with Epsilon missing; have to install 1.2 from the update
site; ecore, ecore development tools, emf; and also UML2 extenders from the Neon
update site... it builds!]

Now let's look at the dependencies.

* [2017-04-24 lun.]
** Dependency graph for EMFViews                                   :emfviews:

[[file:doc/emfviews-plugin-dependencies.svg]]

All the beige boxes are plugins, eggs don't have a plugin.xml, and all are
included in emfviews.feature.

Most depend on the VirtualLinks model, as expected.

Even though it's included in the feature, emfviews.util has no plugin.xml, and
contains only public static methods.

Curiously, virtuallinksocldelegate does not depend on virtuallinksdelegator,
because it does not implement the interface IVirtualLinksDelegate, even though
virtuallinksepsilondelegate does.

In fact, I'm not sure the OCLDelegate is plugged into anything.  The
VirtualLinksDelegator declares an extension point that is used by the
EpsilonDelegate, but not the OCLDelegate.

There are also two plugins (cream) not declared by the feature.  They don't
compile due to change to emfviews.core, so not sure what their role is.

Other plugins: vpdl.dsl.* and monoge.dsl.*.  VPDL is the View Point Description
Language mentionned in the EMFViews paper.  It seems monoge.dsl.* fits the same
role; all of it was added in one commit title "Added DSL for metamodel
extension".  It might do more.  Hard to say.  Most of it is generated by Xtext.
Have to lookup how Xtext works.  On a cursory examination, monoge.dsl.* seems
less fleshed out than Vpdl.

** Point with H                                                    :emfviews:

- Merge emfviews.util in emfviews preferably
- Merge virtuallinksdelegator in vlink-mm
- monoge.dsl is the DSL for the second paper, vpdl.dsl is the DSL for the first
  paper.  Both are useful, but will be tackled later.
- Check for dead code, duplicates.
- Remove unnecessary dependencies that come from transitivity (if you include
  emfviews, you don't need to include vlink-mm explicitly).

* [2017-04-25 mar.]
** Cleaning up warnings                                            :emfviews:
Mostly generics missing, unused vars, and other niceties.

Could not get at everything, since I don't understand the code fully yet.

* [2017-04-26 mer.]
** Still cleaning up and formatting                                :emfviews:
The autoformatter of Eclipse is helpful, but for wrapping especially, multiple
rules can apply, and I'm not sure of the priority between them.  At least, the
process is deterministic (I hope).

Some things are plain weird in the code.  Pretty sure I am looking at dead code
sometimes, but Eclipse cannot tell me that because these are all plugins, and
public methods are part of the API.

Trying to eliminate the dead code...  Eclipse can tell me if a method is used
in the workspace with Ctrl+Alt+H.  That's helpful.  Then, let's say method M is
unused and I remove it.  M called A.  Now A is unused, and Eclipse tells me so.
But if A was the sole caller of B, Eclipse does not immediately tell me by
transitivity that B is also unused.  A bit annoying.

Wait no.  That's not the good approach.  If B subclasses A, and B is given as an
A somewhere, Eclipse can't know that the overridden methods in B will be
called.

Checking for unused constructors I think should be safe.  Static methods as
well.  And methods that are not overridden.

The case of emfviews.elements.MergeElementsImpl is curious: I can't find any
calls for its constructor, but there are references to /casts/ to this class.
Maybe the instances are created through reflection somewhere, but grepping
around does not help.  Also, it seems it's only partially implemented, as
presumably emfviews.rules.MergeRule is strongly related, and most of the methods
there return null.

So, I'm not sure what's truly dead code and what was just forgotten.  Observations:

In emfviews.ui.CreateViewWizard.performFinish, I have a
EMFViewsFactory.createEView call that's seemingly unused.  Instead, we write
directly to a file in the code that follows.  Same thing with the createViewtype
constructor in the factory; it's called nowhere, and in turn one Viewtype
constructor is never called.  In Viewtype.serialize, there's a bunch of stuff
that's eerily similar to EView.serialize.

That was a part that didn't seem to work when I tried to replicate the ECNA
demo... definitely a hot point.

* [2017-04-28 ven.]
** Looking at the examples                                         :emfviews:
Okay so from what I can see, the examples all contain model files (ECore, XMI,
UML).  No code.

Crucially, the only instructions to use the examples are in videos.  Some
examples contain already-created viewtypes and views.

Opening a viewtype file with the viewtype editor throws an exception:

#+BEGIN_EXAMPLE
java.lang.NullPointerException
	at org.eclipse.emf.common.util.URI$URIPool$PlatformAccessUnit.setValue(URI.java:865)
	at org.eclipse.emf.common.util.URI$URIPool.intern(URI.java:1949)
	at org.eclipse.emf.common.util.URI.createPlatformResourceURI(URI.java:2680)
	at fr.inria.atlanmod.emfviews.core.Viewtype.loadFilterMetamodel(Viewtype.java:171)
	at fr.inria.atlanmod.emfviews.core.Viewtype.doLoad(Viewtype.java:160)
	at org.eclipse.emf.ecore.resource.impl.ResourceImpl.load(ResourceImpl.java:1518)
	at fr.inria.atlanmod.emfviews.editor.editors.ViewtypeEditor.createViewtypeTreeEditorPage(ViewtypeEditor.java:118)
	at fr.inria.atlanmod.emfviews.editor.editors.ViewtypeEditor.createPages(ViewtypeEditor.java:445)
#+END_EXAMPLE

The ER2015 video shows that you can also open the viewtype as an ECore model.
Doing that also throws an exception:

#+BEGIN_EXAMPLE
java.lang.NullPointerException
	at org.eclipse.emf.ecore.resource.impl.ResourceImpl$4.getChildren(ResourceImpl.java:522)
	at org.eclipse.emf.common.util.AbstractTreeIterator.hasAnyChildren(AbstractTreeIterator.java:97)
	at org.eclipse.emf.common.util.AbstractTreeIterator.hasNext(AbstractTreeIterator.java:85)
	at org.eclipse.emf.ecore.presentation.EcoreEditor.createModel(EcoreEditor.java:1278)
	at org.eclipse.emf.ecore.presentation.EcoreEditor.createPages(EcoreEditor.java:1339)
	at org.eclipse.ui.part.MultiPageEditorPart.createPartControl(MultiPageEditorPart.java:363)
#+END_EXAMPLE

This one is more concerning, since it's not tied directly to any code in our
plugins.  Might be that we implement some interface incorrectly.

** Trying to open an eviewtype with the editor                     :emfviews:
So the first NPE was due to missing plugins.  This line in Viewtype:

: EPackage contributingEcoreModelPackage = EPackage.Registry.INSTANCE.getEPackage(modelURI);

was returning null.  The modelURI came from the eviewtype file:

#+BEGIN_EXAMPLE
contributingMetamodels=smartEAintegration/metamodels/contentfwk.ecore,http://www.omg.org/spec/BPMN/20100524/MODEL-XMI,http://www.omg.org/spec/ReqIF/20110401/reqif.xsd
#+END_EXAMPLE

In Viewtype.loadContributingMetamodels, we split on this property value, and for
each model, we make a copy of it and in the copy remove attributes and
classifiers.

But this was null, since the plugins were not in the registry.  This should be a
better error.

Anyway, I could have added the model plugins in the dropins folder of my
Eclipse, but I wanted them to run only on the target configuration.  Going into
run configurations, you cannot add arbitrary plugins that are not already loaded
in the current Eclipse: the plugins can only be a subset of the workspace +
target platform.  But you can change the target platform.  And there, you can
add arbitrary plugins.

Adding the model plugins and their requirements did the trick.

Now, another NPE:

#+BEGIN_EXAMPLE
java.lang.NullPointerException
	at fr.inria.atlanmod.emfviews.editor.editors.ViewtypeEditor.createViewtypeTreeEditorPage(ViewtypeEditor.java:143)
	at fr.inria.atlanmod.emfviews.editor.editors.ViewtypeEditor.createPages(ViewtypeEditor.java:445)
#+END_EXAMPLE

This is because of that line:

: treeViewer
          .setInput(((Viewtype) viewtypeResource).getResourceSet().getPackageRegistry().values());

Namely, viewtypeResource.getResourceSet is null.  [[*What if I provide filtersMetamodel myself?][Wait a minute]].  I got to the
same conclusion two weeks ago.  But then, I have no idea why the resourceSet is
null.

All I know is Viewtype extends ResourceImpl, but does not override that method,
so ResourceImpl returns its resourceSet.  Which, from what I see in the code, is
only ever set by basicSetResourceSet.  Which is never called anywhere.

However, we do have a virtualResourceSet in Viewtype.  Could that be it?

Oh wow, adding:

#+BEGIN_SRC java
  @Override
  public ResourceSet getResourceSet() {
    return virtualResourceSet;
  }
#+END_SRC

does seem to work.  I do have a property editor, a tree viewer, and the text
source view.

I wonder how this ever worked... other than changes in the API.

Anyway, success!

* [2017-05-02 mar.]
** Eclipse non-determinism                                 :emfviews:eclipse:
Launching Eclipse, trying to open an eviewtype file with the editor that I've
fixed Friday... only the source tab works.  The other two are blank.  Friday
this was working...

What changed?

After opening other files... the tabs have mysteriously appeared.  Okay, what's
going on here?  Some lazy loading of plugins?  Then our views are silently
failing if we fail to create them?

Okay so starting Eclipse again... Opening up
~EAview_Test/1_viewtype/myEAviewpoint.eviewtype~ haha!  Blank tabs.  At least
it's consistent this time.

(Note: for some reason, when switching tabs, the eviewtype file is marked as
dirty even though we didn't change its contents)

After I open the XMI file in the same folder, the tabs appear.  When opening the
XMI file I noticed a pause, so it most probably did load something.

** Fixing Eclipse tooltip background                                :eclipse:
The background for Javadoc tooltips is black, with white text, and crucially,
dark blue links.  That's a hard contrast, but crucially, the links are difficult
to read.

It seems Eclipse inherits the value from GTK.  It's true that the tooltips in
Firefox are also white on black.

So what do I have to change?

Adding a ~/.config/gtk-3.0/gtk.css file with:

#+BEGIN_SRC css
.tooltip .info {
  background-color: #f5f5bf;
  color: #000;
}
#+END_SRC

This changes the tooltip color in Firefox, and in Eclipse when I hover buttons
in the toolbar, but /not/ the Javadoc tooltips.

[[https://bugs.eclipse.org/bugs/show_bug.cgi?id=501742][This bug]] seems relevant; the issue has been fixed in the Oxygen pre-release.
But what if I don't want to switch?

There's a Javadoc background color preference in Eclipse->Appearance.  It's for
the Javadoc view, not the Javadoc tooltip.  Curiously, there's no setting for
the foreground, which is white by default, with dark blue links again making
selecting a good background color difficult.

Using ~SWT_GTK3=0~ does have an effect: Eclipse seems to switch to the awful
GTK3 theme, where every widget is large.  The Javadoc tooltips are readable then
(black on light grey), but the links are missing since the SWT browser fails to
instantiate.  That's not a solution.

Changing gtk2 preferences has no effect.

I see that the commit fixing the bug just changes one line in the JDT UI
plugin.xml:

#+BEGIN_SRC diff
       <colorDefinition
             label="%JavadocBackgroundColor.label"
             categoryId="org.eclipse.jdt.ui.presentation"
-            value="COLOR_INFO_BACKGROUND"
+            defaultsTo="org.eclipse.ui.workbench.HOVER_BACKGROUND"
             id="org.eclipse.jdt.ui.Javadoc.backgroundColor">
#+END_SRC

My understanding is that ~COLOR_INFO_BACKGROUND~ is picked up from GTK3, but
that's clearly not the case here as the setting is ignored.  Would have to dig
into the source.

: git clone https://git.eclipse.org/r/jdt/eclipse.jdt.ui
: rg --hidden COLOR_INFO_BACKGROUND

Oh hey:

#+BEGIN_SRC java
eclipse.jdt.ui/org.eclipse.ltk.ui.refactoring/src/org/eclipse/ltk/internal/ui/refactoring/RefactoringStatusDialog.java
87:			Color foreground= parent.getDisplay().getSystemColor(SWT.COLOR_INFO_FOREGROUND);
88:			Color background= parent.getDisplay().getSystemColor(SWT.COLOR_INFO_BACKGROUND);

eclipse.jdt.ui/org.eclipse.jdt.ui/ui/org/eclipse/jdt/internal/ui/infoviews/AbstractInfoView.java
390:			fgColor = display.getSystemColor(SWT.COLOR_INFO_FOREGROUND);
401:			bgColor= display.getSystemColor(SWT.COLOR_INFO_BACKGROUND);
#+END_SRC

getSystemColor then.  Trying to get the values returned for that by Eclipse, I
do get black for background, and white for foreground, even with the gtk.css
file.

In the Display class, there are two functions that set ~INFO_BACKGROUND~ from
GTK: ~gtk_css_default_theme_values~ and ~initializeSystemColors~.

The first looks like it's reading the CSS file for the current theme:

#+BEGIN_SRC java
case SWT.COLOR_INFO_FOREGROUND:
if (OS.GTK_VERSION >= OS.VERSION(3, 20, 0)) {
  tSelected = cssOutput.indexOf ("tooltip * {");
} else {
  tSelected = cssOutput.indexOf (".tooltip {");
}
selected = cssOutput.indexOf ("@define-color tooltip_fg_color");
if (tSelected != -1) {
  if (OS.GTK_VERSION >= OS.VERSION(3, 20, 0)) {
    COLOR_INFO_FOREGROUND = gtk_css_parse_foreground(themeProvider, "tooltip * {");
  } else {
    COLOR_INFO_FOREGROUND = gtk_css_parse_foreground(themeProvider, ".tooltip {");
  }
  return "parsed";
} else if (selected != -1) {
  color = simple_color_parser(cssOutput, "@define-color tooltip_fg_color", selected);
  if (!color.isEmpty()) {
    break;
  }
}
#+END_SRC

Looks like it's not really parsing the whole CSS, just looking for specific
strings and getting the colors.

The search for the background color is slightly different:

#+BEGIN_SRC java
case SWT.COLOR_INFO_BACKGROUND:
			tSelected = cssOutput.indexOf ("tooltip.background {");
			selected = cssOutput.indexOf ("@define-color tooltip_bg_color");
			if (tSelected != -1) {
				COLOR_INFO_BACKGROUND = gtk_css_parse_background(themeProvider, "tooltip.background {");
				return "parsed";
			} else if (selected != -1) {
				color = simple_color_parser(cssOutput, "@define-color tooltip_bg_color", selected);
				if (!color.isEmpty()) {
					break;
				}
			}
			break;
#+END_SRC

It's not picking up the background-color property.  initializeSystemColors is
the one who calls the code above, with the logic:

#+BEGIN_SRC java
if (OS.GTK_VERSION >= OS.VERSION(3, 14, 0)) {
			String colorInfoForeground = gtk_css_default_theme_values(SWT.COLOR_INFO_FOREGROUND);
			if (!colorInfoForeground.isEmpty()) {
				if (colorInfoForeground != "parsed") {
					rgba = gtk_css_property_to_rgba (colorInfoForeground);
					COLOR_INFO_FOREGROUND = toGdkColor (rgba);
				}
			} else {
				styleContextGetColor (context, OS.GTK_STATE_FLAG_NORMAL, rgba);
				COLOR_INFO_FOREGROUND = toGdkColor (rgba);
			}
		} else {
			styleContextGetColor (context, OS.GTK_STATE_FLAG_NORMAL, rgba);
			COLOR_INFO_FOREGROUND = toGdkColor (rgba);
		}
#+END_SRC

I don't know how what CSS Eclipse gets back from GTK, but the ones I have in the
theme are separated into multiple files, with a main.css that includes other
files with @import directives.  Since the code above is grepping for
@define-color, I'm guessing it's looking at the raw CSS files sitting on my
drive.  In that case, it will only match the @define-color line, which goes
through ~simple_color-parser~.

#+BEGIN_SRC java
String simple_color_parser (String output, String value, int index) {
	/*
	 * This method takes a color value (rgb(...), #rgb, an X11 color, etc.)
	 * and makes sure it's input we can handle. We can handle rgb/rgba values,
	 * X11 colors, or colors in the format #rgb or #rrggbb.
	 *
	 * We cannot handle shade/gradient functions or references to other colors.
	 * Because of this we strip out values that start with "@" and check
	 * non rgb values against X11 named colors.
	 *
	 * The following would be invalid input:
	 *
	 * shade(@bg_color, 0,7)
	 * or
	 * define-color error_bg_color @bg_color
	 */
	if (output != null && value != null) {
		int position;
		String color;
		position = index + value.length() + 1;
		color = output.substring(position);
		// Check for rgb color case
		if (color.startsWith("#") || color.startsWith("rgb")) {
			return color;
		} else if (!color.startsWith("@")) {
			// Check for an X11 color
			String [] cut = color.split(";");
			if (colorList.contains(cut[0])) {
				return color;
			}
		}
	}
	return "";
}
#+END_SRC

This function, again, seems rather brittle; it will break if there is more than
one space before the actual color value given to a @define-color prop.  Since
this is clearly a flavor of CSS used by GTK, maybe they have a stricter syntax
than CSS.  Or maybe all CSS files on the web are actually non-compliant, but web
browsers are lax in parsing?

Anyway, my ~tooltip_bg_color~ has a hexadecimal value, so it should return it.
And then... since we are not returning "parsed", the color is converted to RGBA,
which calls delegates to native code.

#+BEGIN_SRC java
GdkRGBA gtk_css_property_to_rgba(String property) {
	/* Here we convert rgb(...) or rgba(...) properties
	 * into GdkRGBA objects using gdk_rgba_parse(). Note
	 * that we still need to remove the ";" character from the
	 * input string.
	 */
	GdkRGBA rgba = new GdkRGBA ();
	String [] propertyParsed = new String [1];
	propertyParsed = property.split (";");
	OS.gdk_rgba_parse (rgba, Converter.wcsToMbcs (null, propertyParsed[0], true));
	return rgba;
}
#+END_SRC

The comment suggests that this method only works on rgb and rgba color values,
not hexadecimal.  Changing the values in the gtk-main.css to rgb color values
has no effect.

(I wish I could add a breakpoint into the code I'm seeing Eclipse).

Maybe it's using default colors somehow?  These are the calls to get the default
foreground and background colors:

: styleContextGetColor (context, OS.GTK_STATE_FLAG_NORMAL, rgba);
: getBackgroundColor (context, OS.GTK_STATE_FLAG_NORMAL, rgba);

The first one calls into native code.  The second draws a surface and look at
the color in it.

Oh wait: I /can/ put breakpoints into the code.  That's going to be much
simpler.

Okay so the CSS Eclipse is looking at to determine the colors is... not the one
I was modifying.  It's much larger, and has rule declarations instead of only
@define-color calls and @import statements.

Unfortunately, the debugger is unable to give me the full value.  In the part
I've managed to extract, I don't see any comments.  That might indicate the file
was generated.

So this file apparently contains a ~.tooltip~ declaration, since that's a hit
for the code in Display.  When it gets to it, here is what it finds:

#+BEGIN_SRC css
.tooltip {
  border-bottom-left-radius: 5px;
  border-bottom-right-radius: 5px;
  border-top-left-radius: 5px;
  border-top-right-radius: 5px;
  box-shadow: none;
  color: rgb(255,255,255);
  padding-bottom: 4px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 4px;
  text-shadow: 0 1px rgb(0,0,0);
}
#+END_SRC

That's coherent with the white foreground.  For background:

#+BEGIN_SRC css
tooltip.background {
  background-clip: padding-box;
  background-color: rgba(0,0,0,0.8);
  border-bottom-color: rgba(255,255,255,0.1);
  border-bottom-style: solid;
  border-bottom-width: 1px;
  border-image-repeat: initial;
  border-image-slice: initial;
  border-image-source: initial;
  border-image-width: initial;
  border-left-color: rgba(255,255,255,0.1);
  border-left-style: solid;
  border-left-width: 1px;
  border-right-color: rgba(255,255,255,0.1);
  border-right-style: solid;
  border-right-width: 1px;
  border-top-color: rgba(255,255,255,0.1);
  border-top-style: solid;
  border-top-width: 1px;
}
#+END_SRC

So the question now is: where is this CSS coming from?  It's not from the theme
I've specified, and not from the user CSS file.  Maybe it's a file used by
Eclipse?

It is requesting the "Adwaita" theme by name.

Okay so I've dumped the CSS that Eclipse gets from GTK to disk.  I still have no
clue how it's constructed, and how it sets the color values for tooltips.

Looking at the GTK documentation:

#+BEGIN_EXAMPLE
In addition, certain files will be read when GTK+ is initialized. First, the
file $XDG_CONFIG_HOME/gtk-3.0/gtk.css is loaded if it exists. Then, GTK+ loads the
first existing file among XDG_DATA_HOME/themes/theme-name/gtk-VERSION/gtk.css,
$HOME/.themes/theme-name/gtk-VERSION/gtk.css,
$XDG_DATA_DIRS/themes/theme-name/gtk-VERSION/gtk.css and
DATADIR/share/themes/THEME/gtk-VERSION/gtk.css, where THEME is the name of the
current theme (see the “gtk-theme-name” setting), DATADIR is the prefix
configured when GTK+ was compiled (unless overridden by the GTK_DATA_PREFIX
environment variable), and VERSION is the GTK+ version number. If no file is
found for the current version, GTK+ tries older versions all the way back to
3.0.

In the same way, GTK+ tries to load a gtk-keys.css file for the current key theme, as defined by “gtk-key-theme-name”.
#+END_EXAMPLE

If we actually look in these folders, the gtk.css for Adwaita is empty, since
it's the default theme.  Presumably, all is implemented in the code.

I don't have ~XDG_CONFIG_HOME~ set, but I suspect the user file is still getting
read, since it's modifying the tooltip colors for other parts of Eclipse.

Okay, what about pointing to a custom theme?  Will it follow the CSS then?

Creating a Foo theme and setting as default, with this gtk.css:

#+BEGIN_SRC css
.tooltip {
  color: rgb(91, 91, 91);
}

.tooltip.background {
  background-color: rgb(230, 230, 230);
}
#+END_SRC

In Eclipse, the CSS dump for Foo is as huge as Adwaita's.  Tooltip values are
the same as well:

#+BEGIN_SRC css
.tooltip {
  border-bottom-left-radius: 5px;
  border-bottom-right-radius: 5px;
  border-top-left-radius: 5px;
  border-top-right-radius: 5px;
  box-shadow: none;
  color: rgb(255,255,255);
  padding-bottom: 4px;
  padding-left: 4px;
  padding-right: 4px;
  padding-top: 4px;
  text-shadow: 0 1px rgb(0,0,0);
}
#+END_SRC

Something is fishy with this GTK function, and I can't find a way to influence
the values.

Looks like I have to switch to Oxygen.  Oh well.

Oxygen fixed it... but not the colors in the Content Assist.  I understand
that's a fresh commit that should go in the release.

* [2017-05-03 mer.]
** Fixing the empty tabs in the EMF Views editor                   :emfviews:
So: loading an eviewtype, I get a MultiPageEditor with 3 pages.  Page 2 and 3
are blank.  After I load the XMI file in the same folder, the two pages have
content.

Trying to add a very simple page to the MultiPageEditor with a single button:
switching to this page, the button exists.

Trying to add a more involved page with a ScrolledForm and a TreeViewer:
switching to this page, it is blank.

However, using setActivePage to point to this new page, the page is indeed
constructed and visible, and all the other pages are blank, except the first
one.

Using setActivePage to point to the properties page: it is visible on launching
Eclipse and after switching to and back from other pages.  First page is
visible, other two pages are not.

The common factor in the invisible pages seem to be that they use a
ScrolledForm.

I just need a Control to put into a page.  A ScrolledForm is a Control, but a
Composite is a Control as well.

Trying to put a button and a TreeViewer in a Composite: grey page.

If I add a layout to the composite:

: comp.setLayout(new GridLayout());

Now it's displayed.  Same thing with a ScolledForm actually: adding the
setLayout to its Composite returned by getBody will make the widgets visible
when switching to the page.

But creating the ScrolledForm through the FormToolkit, that doesn't work.

Need to look into forms and editors.

* [2017-05-05 ven.]
** Eclipse forms and editor bug                            :eclipse:emfviews:
Reading: https://www.eclipse.org/articles/Article-Forms/article.html

This is a good (albeit dated) resource on what Eclipse Forms are useful for.  It
seems PDE uses them extensively for editing the plugin.xml file for instance.
So now I understand what the EViewType editor tries to emulate.

Since we are trying to build a multi-page form, it seems the preferred way is to
extend FormEditor rather than MultiPageEditor.

Found [[http://git.eclipse.org/c/platform/eclipse.platform.ui.git/plain/examples/org.eclipse.ui.forms.examples/src/org/eclipse/ui/forms/examples/internal/rcp/FreeFormPage.java][an example]] of using FormEditor and multiple FormPage.

Doing that seems to fix the blank page bug.  I just converted the code that
created the forms to FormPage inner classes.  It's not as clean as I'd like,
since there seem to be weird explicit dependencies between models and views.
Considering that all three pages must stay in sync, it would seem that changing
the model and listening to changes would be cleaner.

But, I also don't know if we want to keep the editor in this current form.  So
that fix should do it for now.

** More dead code                                                  :emfviews:
Looking around, I see there are two classes that are not used: FormComposite, a
small utility class to adapt a composite with a toolkit, and Overview, which
seem to be the properties view as it existed before.

Of importance, the Overview class takes care of using text strings pooled from a
text file for the UI, so they can be translated.  But I can't seem to succeed in
including the page in the editor, so...  Dead code.

* [2017-05-09 mar.]
** Fixing the TreeViewer in the Viewtype editor                    :emfviews:
So, first of all, there was the issue of dirtying the state of the editor when
switching to the TreeViewer.

While trying to understand what the page is for, it hit me that a few things are
not working properly: some boxes should be ticked, and selected elements should
be expanded as well.

In addition, I see that basically all objects can be expanded in the TreeViewer,
even when they don't have any children.

Luckily, I've already done the TreeViewer tutorial from the book.

So, we are reusing the ContentProvider from our Viewtype creation wizard.
Basically, we delegate to the EMFContentProvider to display models, except we
add another layer of EPackage.

Rewriting the ContentProvider getChildren and hasChildren to be in sync fixes
the issue with empty children showing a twistie.

But we expand/tick/reveal calls still have no effect.

Debugging a reveal call indicates that it returns null internally.

So I can get the treeViewer to reveal/tick/expand elements when I build the
elements array myself.  I guess what we get from viewtype.getHiddenAttributes
are not elements related to the input of the tree.

Wait a minute.  The tree is not complete in the view... Looks like the models we
display are the one from the virtual resource set of the viewtype... so the
items are already filtered out.  The code is trying to tick the exact same
items, so of course that shouldn't work.

Not removing the items in the resource set... well, that still doesn't tick
them.  But I think that's because the virtual resource set contains clones of
the models, and the hidden attributes are objects of the base resource set, so
maybe they cannot be equal to one another.

Ah!  Yes, that's it.  After putting the originals in the virtual resource set
and not the copies, the ticks appear.  But the containing classes are not
expanded.

Which is weird, because we construct the array of containing class based on the
hidden attributes.  So if the hidden attributes are ticked because the tree
contains the same objects, it should also contain the right classes.

Hmm maybe we are changing the expanded elements at another point.  We do, in
pageChange.

But commenting that does not help.

Wait, after adding breakpoints to step through the code that sets the expanded
state internally in AbstractTreeViewer, it does work!  Shenanigans.  I guess the
hot code replace can get confused by my changes, without warning me.  This is
unfortunate.

Also, the code was trying to set the expanded state on a tree item below the top
level.  According to the documentation, this should work: expanded states are
saved even if you close the parent, so when you reopen the parent, the expanded
children should still be open.

However, if I manually do it in the tree, that's not the behavior I observe.
Expanding a child, and toggling its parent: the child is closed.  Looks like
closing an item closes its children recursively.  What works is to reveal the
elements.

Okay so now, I still haven't solved the mysterious dirtying of the editor when
opening the Contents page.

There's an editorDirtyStateChanged which I can override.  But it doesn't get
called.

Okay, found it.  The AttributeSelectionAdapter is changing the model every time
we select something.  That's overkill, and wrong.  We should only update the
model when there are actual changes in the tree: that is, when we check/uncheck
elements.

But I'm not fixing it right now.  Better focus on cleaning up the core of
EMFViews: the virtual model.  I'm leaving notes to know that these parts of the
code are busted.

** Notes on EMFViews core                                          :emfviews:
What is referred to as the Weaving model in the paper is our VirtualLinks
package.  It's a collection of links between models and metamodels.  For the
moment, we have two kind of links: associations and filters.

Associations are virtual references.

Filters hide attributes from models.

We don't have virtual attributes or virtual classes, but that might be something
to improve upon (the Extension paper was a step in that direction).

The virtual models are realized by the emfviews.core package.  There we have
Viewtype (a viewpoint in the paper), View and EView.

We have two levels: the viewtype describes the links between contributing
metamodels, and the view describes how to construct a virtual model from
contributing models (which are instances of the contributing metamodels of the
viewtypes).

Practically speaking, when opening an EView file with an ECore editor, EMF will
display the virtual model.  This is achieved by registering EMFViewsFactory as
the parser class in an extension point.

This class then creates an EView and Viewtype, depending on the file extension.
These two files extend ResourceImpl, so they can be transparently used as
resources by EMF.

How being a resource helps in displaying a model, I still have to find out.

H pointed out some redundancies in the EView file: we specify the ECL file, but
we don't need it.  It's already in the EViewtype, which is also specified, and
in any case we only need the XMI describing the weaving model.  The ECL file is
used to generate the weaving model, but once we have one, we don't need the ECL
anymore.  And in any case, we can also provide the weaving model XMI manually.

Same thing in the EViewtype: the examples have an ECore file that contains the
hidden attributes.  These should be part of the XMI, which already register the
association virtual links.

* [2017-05-10 mer.]
** Renaming VirtualLinks package and freshening up the model       :emfviews:
Annoyingly, we had a package named fr.inria.atlanmod.emfviews.virtualLinks.  Note
the camelCase.

Since this package is generated by EMF, better to update the model directly.
I'm not quite sure what to make of the namespace URI we have:

: http://inria.fr/virtualLinks

But I know that changing it will break existing serialized XMI files, so that's
probably a bad idea for the time being if I want to run the examples, without
having to change the namespace there as well.

** Testing out containment references in EMF                            :emf:
In a language where objects are allocated on the heap, I did not understand the
use of containment references.

Turns out, a containment reference and vanilla reference both include a list of
target objects.  The list implementation are different classes, but I don't see
anything vastly different about them.

According to the EMF bible, an object B that has a container can only be in one
container at the time.  Changing its container will remove it from the previous
container.  That's neat.

But doesn't that apply to bidirectional references with a single multiplicity at
one end as well?

Looks like it does.  So, a containment reference is equivalent to a
bidirectional reference with multiplicity 1 at one end.  One difference is that
to make a bidirectional reference in ECore, you have to create two references.

But the major difference since to lie in the serialization.  Contained targets
will be serialized in the same resource as the container, whereas vanilla
references are serialized in different resources.  In other words, it does what
you expect when you consider storage.  A quick test reveals that indeed,
contained objects are saved in the same file as their parent, whereas with
vanilla references you are dealing with multiple files.

So, for simplicity, and simplified management of serialized resources,
we should prefer containment references.

** Trying to use the MoDisCo model browser                         :emfviews:
Because the model browser provided by ECore is very basic, and does not follow
references.  The MoDisCo browser does.

However, trying to open the examples XMI with it, I am greeted with familiar
errors concerning unknown features from the TOGAF metamodel:

: !MESSAGE org.eclipse.emf.ecore.xmi.FeatureNotFoundException: Feature 'belongsTo' not found. (platform:/resource/1_EAdata/models/1_travelAgencyEA.xmi, 26, 270)

I slayed this dragon previously, and I kept my custom TOGAF plugin.  Re-using
it...

Okay, it works.  I can follow recursive links in the model.  It's... not very
impressive, or convincing, but it works.  The Modisco browser is dog slow
however.

** Following the code                                              :emfviews:
After the EmfViewsFactory delegates to either the EView or Viewtype constructor,
what happens?

Well, nothing at that point: the constructor returns a resource.  That resource
is only loaded when its doLoad method is called.  That's where the magic
happens.

In Viewtype.doLoad, we parse the eviewtype file, load the models, and create the
virtual resource set.

The correspondenceModelBase seem unused at this point.  The Viewtype only uses
it in serialize()... except this function is never used.  The actual
serialization happens in the doSave method of the resource.

First we load the filter metamodel: the ECore file that should not be here.

Then we load the contributing metamodels.  We get the corresponding package, and
each package is cloned and kept in contributingEPackages.

Then, we first filter out any attributes as specified by the filter metamodel.
If the package matches one in the filters, we remove every structure from common
classifiers.

There are two issues currently: we loop through all filtered packages, for
each contributing package.  That's unnecessarily quadratic.

Second issue: we only seem to care about EClass classifiers with an unchecked
downcast.  So I'm pretty sure that's an exception if the filter metamodels
contains an Enum or Datatype.

Hmm, it crashes, but not where I expected.  Actually, I see other downcasts in
the same method, and they seem unsafe, but no warnings.  Need to check that as
well.

*** DONE Check copyright/contributions
CLOSED: [2017-09-29 ven. 17:54]
Copyright is attributed to Inria in some places, Atlanmod in others (or should
it be AtlanMod?).

Copyright years are 2013,2014 tops.

Copyright notice should maybe be generated for VirtualLinks.

Copyright notice should be included in all packages.

AtlanMod should be the provider of all packages.

*** Investigate dependencies
Some EMF plugins depends on other EMF plugins, some depend on EMF packages
directly.  The book I read on Eclipse plugin development recommended to depend
on packages.

*** Change the logo background to be transparent
This is unnerving.

* [2017-05-12 ven.]
** Downcasts in Java are "safe"                                        :java:
Because there will be runtime checks...  I was under the impression the compiler
would complain, but it's just in case of unchecked cast with generics.  Because
of type erasure, the compiler cannot insert a runtime check (a List is still a
List), so the warning is to make sure you know what you are doing.

Otherwise, Java assumes you do know what you are doing with straight downcasts
from A to B, even though the compiler only knows that this downcast /could/
work (if B :< A).

That's disappointing.

Is there a linter out there that could at least pick up downcasts so I could
review them?  FindBugs [[http://findbugs.sourceforge.net/bugDescriptions.html#BC_UNCONFIRMED_CAST][appears to]].

The Eclipse plugin is a bit rough, but it does report the stupid downcast from
my test code.  However, it does not report the troubling downcasts in EMFViews.
So, more trouble than it's worth.

** Mysterious crash when loading funky ECore file                  :emfviews:
So adding other classifiers (EEnum, EDataType) to the Ecore file containing our
filters and opening the Eviewtype with the ECore editor results in 3 thrown NPE.

The puzzling part is that, in all of the stack traces, our code is not on the
stack.  Maybe we implement something wrongly.

Adding a breakpoint shows Viewtype.getContents is called and returns the null in
question.  But since the null value is used by ResourceImpl, that's where the
NPE is thrown.

Anyway, we only set virtualContents after doLoad() has completed.

Hmm, I see!  Stepping through again, and in fact ResourceImpl.load wraps our
doLoad with a try/finally, but no catch.  So we do throw a cast exception due to
the presence of other classifiers!  But that was masked by ResourceImpl.
Sneaky.

** EMFViews archeology                                             :emfviews:
So, H found the original demo paper along with the initial prototype
implementation of EMFViews (then called VirtualEMF).  The novel idea at the time
was to have a /virtual/ model, that composed multiple contributing models.  The
virtual model is lazy: attributes are proxies to the concrete models, and the
virtual attributes are synthesized on-demand.

In the code, you can find a VirtualModel class that's absent from the current
version.  That's because at the time, only models were virtualized, not
metamodels.  But, the same virtualization approach can be applied to metamodels,
since they can be viewed as models as well; hence EMFViews.  In EMFViews, we
have Viewtype which should be the equivalent of VirtualModel for metamodels, and
View, which would be closer to the original VirtualModel.

Looking at the rest of the code, everything in emfviews.elements seem very
similar to the first version.

In emfviews.rules, the MergeRule was severely cut.

In emfviews.core, the MetaModelManager was mostly changed.  The
VirtualLinkManager was slightly changed, and that's it (other than added/removed
files).

** Further code investigation                                      :emfviews:
Now I'm in Viewtype.loadCorrespondenceModel.  The correspondenceModel is the XMI
file that describe the VirtualLinks: it gives us the info we need to compose the
contributing models (and in this case, metamodels).

There are two kinds of links actually: Filter and Association.  But filter links
are not currently used in this path of code; the filters are specified in a
separate ECore file which is used in the loadFilterMetamodel phase.

So the code is concerned only with Association links.  For each Association, we
synthesize an EReference with the Association attributes (source, target,
lower and upper bounds) and add it to the EClass in which it resides (in the
virtual packages we created earlier).

Ultimately, in Viewtype.setVirtualContents, we turn the EPackage from our
virtual resource set into a VirtualContents object (which is just an EList).  I
had looked at VirtualContents before: it's a curious implementation of an EList
from a list of lists, which only purpose seem to be to simulate a flat list:

#+BEGIN_SRC java
public E get(int index) {
    if (index >= 0) {
      for (List<E> l : subLists) {
        if (index < l.size()) {
          return l.get(index);
        } else {
          index -= l.size();
        }
      }
    }
    throw new IndexOutOfBoundsException();
  }
#+END_SRC

I'm assuming this is done because getContents requires an EList.  But, then, why
not flatten the lists once and for all?  The VirtualContents list seem to be
read-only, since the set method is implemented by a call to super
which... throws UnsupportedOperation.

** Open questions                                                  :emfviews:
- Is Viewtype creating a truly virtual metamodel?  It doesn't seem to do any
  demand-loading, but maybe that's behind the scenes.  Should compare with what
  View/EView does for models, or what VirtualModel did in the first prototype.

- There are still a bunch of files in the core, are they used by View/EView or
  not?  MergeRule, TranslationRule, etc.

* [2017-05-15 lun.]
** Is Viewtype proxying metamodels?                                :emfviews:
To me it seems that no, it just plain clones them into the virtual resource
set.  This is done in loadContributingMetamodels:

#+BEGIN_SRC java
EPackage contributingEcoreModelPackage = EPackage.Registry.INSTANCE.getEPackage(modelURI);

Copier copier = new Copier();
EObject copy = copier.copy(contributingEcoreModelPackage);
copier.copyReferences();
EPackage copiedPackage = (EPackage) copy;
EcoreUtil.remove(copiedPackage);
contributingEpackages.add(contributingEcoreModelPackage);
#+END_SRC

Regardless of whether there are filters, we clone the packages.  Then, if there
are filters, we remove the attributes from these copies.

: eClassWithItemsToHide.getEStructuralFeatures().remove(theAtt);

Then, if there are associations, we add EReferences to these copies:

#+BEGIN_SRC java
EReference theR = EcoreFactory.eINSTANCE.createEReference();
theR.setName(association.getName());
theR.setLowerBound(association.getLowerBound());
theR.setUpperBound(association.getUpperBound());
theR.setEType(theTargetEClass);
...
theSourceEClass.getEStructuralFeatures().add(theR);
#+END_SRC

So, is this different from how View/EView work?

** Investigating View/Eview                                        :emfviews:
Stepping through the code.  When we load an eview file, we trigger EView.doload.

First thing is to read the file, and create a Viewtype resource from the
compositionMetamodel line.  We are creating a whole new Viewtype (and copying
packages), just for the EView.  If a Viewtype is a virtual metamodel, we should
be able to locate it from the registry, and create it only if it does not exist.

I'm wondering if the EView/View split is the half-finished result of trying to
abstract the common parts of EView and Viewtype into a common abstract class.
But at the moment, EView is the sole subtype of View.

In EView, we then load the View.contributingMetamodels.  This merely register
the metamodels in the virtualResourceSet of View.  But this virtual resource set
is different from the one held by Viewtype.  At this point, the metamodels are
not modified.

Then we create a MetamodelManager.  This one populates a bunch of Maps.  A map
of composition classes keyed by their names; these are taken from the contents
of the constructed Viewtype.  Then a map of all the EClass of the contributing
metamodels, again keyed by their names; these are taken straight from the
classifiers of the contributing metamodels.

Then a map of concrete to virtual classes.  That's interesting:

#+BEGIN_SRC java
for (List<EClass> lcec : contributingClassesByName.values()) {
  for (EClass cec : lcec) {
    List<EClass> lvec = compositionClassesByName.get(cec.getName());
    for (EClass vec : lvec) {
      if (vec.getEPackage().getNsURI().equals(cec.getEPackage().getNsURI())) {
        this.concreteToVirtualClass.put(cec, vec);
        mapFeatures(cec, vec);
      }}}}
#+END_SRC

The "virtual EClasses" (vec) that are put into the map are pulled from
coompositionClassesByName, and used as values keyed by the corresponding class
in contributingClasses.

mapFeatures does the same mapping, but for structural features, recursively:

#+BEGIN_SRC java
private void mapFeatures(EClass concEC, EClass virtuEC) {
  for (EStructuralFeature feature : concEC.getEStructuralFeatures()) {
    EStructuralFeature vf = virtuEC.getEStructuralFeature(feature.getName());
    if (vf != null) {
      this.virtualToConcreteFeature.put(vf, feature);
      this.concreteToVirtualFeature.put(feature, vf);
    }}}
#+END_SRC

Now we have a bidirectional map.

Lastly, there may be additional features in the virtual classes (created by the
associations), so we also record them in a map of virtualAssociations, but only
if they were not present in virtualToConcreteFeatures:

#+BEGIN_SRC java
for (List<EClass> lec : compositionClassesByName.values()) {
  for (EClass ec : lec) {
    for (EStructuralFeature sf : ec.getEStructuralFeatures()) {
      if (virtualToConcreteFeature.get(sf) == null)
        if (virtualAssociations.get(sf.getName()) == null) {
          List<EStructuralFeature> sfs = new ArrayList<>();
          sfs.add(sf);
          virtualAssociations.put(sf.getName(), sfs);
        } else {
          virtualAssociations.get(sf.getName()).add(sf);
        }}}}
#+END_SRC

After that we are back in EView, and that's it for the metamodels.  Now we
loadContributingModels:

#+BEGIN_SRC java
protected void loadContributingModels(List<String> contributingModelsPaths) {

  for (String modelURI : contributingModelsPaths) {
    virtualResourceSet.getResource(URI.createPlatformResourceURI(modelURI, true), true);
  }

}
#+END_SRC

Which just seems to force the loading of each model, without doing anything with
the returned resource (why?).

If there is a correspondenceModelBase we... don't do anything with it (yet)?  We
get the correspondence XMI, create a VirtualLinksDelegator for the
correspondenceModelBase, and let the delegate create the links:

#+BEGIN_SRC java
if (properties.getProperty("correspondenceModelBase") != null) {
  IWorkspace workspace = ResourcesPlugin.getWorkspace();
  java.net.URI linksModelURI = workspace.getRoot()
      .findMember("/" + properties.getProperty("correspondenceModel")).getLocationURI();
  try {
    VirtualLinksDelegator vld =
        new VirtualLinksDelegator(properties.getProperty("correspondenceModelBase"));

    vld.createVirtualModelLinks(org.eclipse.emf.common.util.URI
        .createURI(linksModelURI.toString()), getContributingModels());
#+END_SRC

In this case, it creates an EclDelegate.  In
EclDelegate.createVirtualModelLinks, we open the ECL file and first parse the
aliases in the header.

Here is a sample ECL file from the examples:

#+BEGIN_EXAMPLE
//alias_ea=http://www.obeonetwork.org/dsl/togaf/contentfwk/9.0.0
//alias_bpmn=http://www.omg.org/spec/BPMN/20100524/MODEL-XMI
//alias_reqif=http://www.omg.org/spec/ReqIF/20110401/reqif.xsd

rule detailedProcess
match s : ea!Process
with  t : bpmn!Process
...
#+END_EXAMPLE

I think the intent here is pretty clear: to define ~ea~, ~bpmn~ and ~reqif~ as
aliases for the metamodels in the header.  Still, it would be better to have ECL
support these kinds of declarations rather than hack a parser with indexOf
calls.

[H: usually you'll run ECL with a launch configuration file, specifying the
aliases.  Here it's inlined.  Maybe there is away to provide a launch
configuration at runtime, but it's not really important.]

In any case, we populate two maps keyed by the aliases: one to the resource of
the metamodel, and one to the package URI.  No provisions are made if we don't
find a corresponding resource.

After that, we close the ECL file because we let ECL parse the rest.  Then we
add instances of EmfModel to the model repository of the Ecl module.

Then, we executet the ECL module, and iterate on the resulting MatchTrace in
order to create virtual links for each matching trace:

#+BEGIN_SRC java
for (Match match : matches) {
  if (match.isMatching()) {
    EObject left = (EObject) match.getLeft();
    EObject right = (EObject) match.getRight();

    Association vAsso = vLinksFactory.createAssociation();
    vAsso.setName(match.getRule().getName());
    vAsso.setAssociationTypeName(match.getRule().getName());
    vAsso.setLowerBound(0);
    vAsso.setUpperBound(1);

    LinkedElement lSource = vLinksFactory.createLinkedElement();
    lSource.setModelRef(left.eClass().getEPackage().getNsURI());

    lSource.setElementRef(left.eResource().getURIFragment(left));
    vAsso.setSourceElement(lSource);

    LinkedElement lTarget = vLinksFactory.createLinkedElement();
    lTarget.setModelRef(right.eClass().getEPackage().getNsURI());
    lTarget.setElementRef(right.eResource().getURIFragment(right));
    vAsso.getTargetElements().add(lTarget);

    virtualLinks.getVirtualLinks().add(vAsso);
    virtualLinks.getLinkedElements().add(lSource);
    virtualLinks.getLinkedElements().add(lTarget);
  }
}
#+END_SRC

After that, we save the populated virtualLinks to the XMI file.

So, it seems we always recreate the XMI file from the ECL.

[H: that may not be ideal, but models can be updated, so you usually want your
view to synchronize with these changes by default.  Here we run the ECL query
again.]

Back in EView, we now create a VirtualLinkManager, given the correspondence
model URI (the XMI).  The manager merely holds a reference to both the EView and
the VirtualLinks instance from the XMI.

Then the VirtualLinkManager is initialized, which creates a LinksProjector.
There, for each Assocation in the XMI, we get a virtual element from the
VirtualLinkManager corresponding to the source element of the association, and
we link the target elements to it:

: vElement.setVirtualAssociation(virtualFeature, EStore.NO_INDEX, targetElements);

After that, we set the virtual contents of our EView resource, by translating
each package of the contributing models to virtual elements.  Creating virtual
element happens in VirtualLinkManager.getVirtualElement:

#+BEGIN_SRC java
public EObject getVirtualElement(EObject e) {
  VirtualElement vElem = virtualLinks.get(e);
  if (vElem == null) {
    vElem = new ReproduceElementImpl(virtualModel, e);
    virtualLinks.put(e, vElem);
  }
  return vElem;
}
#+END_SRC

and ReproduceElement uses a ReproduceRule, which implements an EStore... and
that's probably where the secret virtualization sauce lies.  But it already
looks like there is much more happening in EView/View concerning virtualization,
and I didn't see any copying taking place.

So my premature answer is: the Viewtype is not virtualized as the Views are.
Which was kind of the point of EMFViews.  That should be fixed in priority.

* [2017-05-16 mar.]
** Reading the EMF bible                                                :emf:
To get a clearer picture of the concepts at hand.

Questions still open after reading the relevant chapters:

- Can we read a UML model and access it using the
  EPackage/EClass/EAttribute/... interfaces?

- Is demand-loading and demand-creating for resources lazy, eager, or something
  else?  Specifically, the createResource and getResource methods accept a
  boolean argument: does it forces resolution or rather delays it?

- It seems, at least for references, that EMF already does some
  auto-proxification.  What is the mechanism we use in EMFViews
  (ReproduceRule?), and how does it compare?

  If we use "Dynamic EMF" as it's called in the book to create our view
  packages, would we not benefit from proxification?

And an observation:

EMFViews add copies of contributing model packages to a registry local to the
virtual resource set of a Viewtype.  But then, the EView does not tap into this
virtual resource set, so there's duplication here.

* [2017-05-17 mer.]
** EMFViews uses an EStore                                         :emfviews:
A VirtualElement inherits from an EStoreEObjectImpl, which is an EObject
implementation backed by an EStore.  Then, our translation rules are all
different EStore implementation.

From what I gather, this is where the actual magic for models happen (and this
was part of the initial implementation back in 2011).

When we load a model, the very last step of EView.doLoad is to set the virtual
contents:

#+BEGIN_SRC java
for (Resource r : contributingModels) {
  ArrayList<EObject> oneOftheSublists = new ArrayList<>();
  oneOftheSublists.add(translateToVirtualElement(r.getContents().get(0)));
  sublists.add(oneOftheSublists);
}

this.virtualContents = new VirtualContents<>(this, sublists);
#+END_SRC

This populates lists with virtual elements, which are obtained from the virtual
link manager:

#+BEGIN_SRC java
public EObject getVirtualElement(EObject e) {
  VirtualElement vElem = virtualLinks.get(e);
  if (vElem == null) {
    vElem = new ReproduceElementImpl(virtualModel, e);
    virtualLinks.put(e, vElem);
  }
  return vElem;
}
#+END_SRC

That's where reproduce elements are instantiated.  (And, interestingly, only
reproduce elements; MergeElement and FilterElement do not seem to be created
anywhere)

A reproduce element is a virtual element, so an EStoreEObjectImpl, and holds a
concrete EObject called the concrete element.  The idea is to pass through
access to the concrete element using the EStore interface.

At the end of creating a reproduce element, this is what happens in init:

#+BEGIN_SRC java
this.eProperties().setEResource(vModel);
this.concreteElement = concreteElement;
this.eSetClass(eClass);
this.eClass();
setTranslationRule(ReproduceRule.INSTANCE);
eSetStore(this.getTranslationRule());
#+END_SRC

We create a reproduce rule, which implements EStore, and will capture get/set
calls on this virtual object.  That's why, in ReproduceRule.get:

#+BEGIN_SRC java
public Object get(InternalEObject object, EStructuralFeature feature, int index) {
  ReproduceElementImpl vElement = (ReproduceElementImpl) object;

  View vModel = (View) vElement.eResource();
  if (vModel.getMetamodelManager().isVirtualAssociation(feature)) {
    return vElement.getVirtualAssociation(feature, index);
  }
  EStructuralFeature cFeature = vElement.getConcreteFeature(feature);
  Object value = vElement.getConcreteElement().eGet(cFeature);
  ...
  return value;
#+END_SRC

We ultimately return the concrete value.  But not in every case:

#+BEGIN_SRC java
if (feature instanceof EReference) {
  if (feature.isMany()) {
    value = new VirtualModelList<>(object, feature, Arrays.asList((List<EObject>) value));
    if (index != NO_INDEX) {
      value = ((VirtualModelList<EObject>) value).get(index);
    }
  } else {
    value = vModel.translateToVirtualElement((EObject) value);
    if (value instanceof FilterElement) {
      value = null;
    }
  }
}
#+END_SRC

If the requested structural feature is an ERef, and it's many, we return a
virtual list.  Ultimately, inside this virtual list, we will call
getVirtualElement.  If the ref has a single multiplicity, then we can directly
return the virtualElement.

In essence, we perpetuate the virtualization recursively.

It seems to be this part of the code is mixing concerns.  There is a test for
FilterElement here, to mask the value if it should be filtered.  But then we
also have the same test in the VirtualModelList.  Why does the virtual list
repeats this instead of delegating to single virtual elements?

Besides, it seems to me we should have a clear mapping from the Ecore model to
the virtual model, defined for all classifiers and features.

** Using a code coverage tool to find hot/cold code        :emfviews:eclipse:
Following T's recommendation, I used the EclEmma plugin, which is based on
JaCoCo.

Installation was painless.  The plugin supports coverage for running client
Eclipse application, which is my use case.

So now I can answer with certainty that, opening EView and EViewtype files with
a model browser and the viewtype editor, the following classes are never used:

- MergeRule, MergeElementImpl
- FilterElement

In other classes, besides what I already identified as unused, it seems we have
no examples using the sourceAttribute and targetAttribute of a Association.
Maybe there are superseded by sourceElement/targetElements.

* [2017-05-19 ven.]
** How are virtual model attributes filtered out?                  :emfviews:
H raised an interesting question: if an attribute is filtered out in the virtual
metamodel, it is also filtered out in the virtual model.  But how does that
happen?

Does EMF just disregard attributes that are not in the metamodel?  Do we also
need to filter the attributes from the model?

I'm guessing it's the former.  If I comment out the filtering attributes part in
the metamodel, they should appear on the model.

Yes, they do.

Hmm, when the attributes are present in the metamodel, we add them to the maps
of virtual to concrete features in MetamodelManager.  When the attributes are
absent, they are absent from the maps as well.  That's a hint.

* [2017-05-22 lun.]
** Writing tests for EMFViews before refactoring                   :emfviews:
I've got a couple of easy refactorings ahead, related to the EView and EViewtype
files.  But, before that, I want to write some tests to ensure I don't break any
functionality doing so (at least, any functionality we care about).

One problem with writing tests is the way Viewtype and View are written as
resources, you have to provide files through URI, otherwise you cannot construct
them properly.

We could refactor Viewtype and View so that the resource-specific code is
extracted, and calls into a model-specific part that does not have to deal with
files.  But that would be refactoring in order to write the tests for the
/other/ refactoring...

I'll try to write the tests passing files as URI first.

Okay, hit a snag: I'm using URI.createPlatformResourceURI to pass a
workspace-relative filename to the EViewtype file, and it doesn't work.
Presumably, because when I run the code there is no workspace!

So rather I should just use relative paths.  This works:

: URI.createURI("models/foo.eviewtype")

and this will look up the "models" directory in the current project, so it's
relative.  Hopefully that slash is portable as well.

Grmbl, now Viewtype tries to load the filters metamodel.  But it also uses
URI.createPlatformResourceURI, which in turn will use the resource factory
registry to find out how to create an Ecore.  But running in the tests, this
factory is empty:

: System.out.println(Resource.Factory.Registry.INSTANCE.getExtensionToFactoryMap().isEmpty());
: true

I guess I can populate it myself in the tests.

: Resource.Factory.Registry.INSTANCE.getExtensionToFactoryMap()
:     .put("ecore", new EcoreResourceFactoryImpl());

Now I have to find the correct path to set in the Eviewtype file so that it
loads my Ecore model from the right directory.

At the moment, it fails to find it.  I'm in
PlatformResourceURIHandlerImpl.createInputStream.

Amusingly, after prefixing my URI with 'platform:/resource', this method removes
the prefix

: String platformResourcePath = uri.toPlatformString(true);

Ultimately, it calls EcorePlugin.resolvePlatformResourcePath on this suffix,
which merely looks into its getPlatformResourceMap for the root project in order
to produce a platform-specific file URI...

Let's do this:

: EcorePlugin.getPlatformResourceMap().put("foo", URI
: .createURI("file:///home/fmdkdd/proj/emfviews/tests/fr.inria.atlanmod.emfviews.test/models"));

Yeah, it works!  Is there a way to make it relative at least?

In the end, the file URI calls new File(), passing everything to the right of
':'.  Ah, but that's only the URI for the base folder, and EMF uses it to
resolve the resource path below, and this cannot be relative:

#+BEGIN_SRC java
 public URI resolve(URI base, boolean preserveRootParents)
    {
      if (!base.isBase())
      {
        throw new IllegalArgumentException("resolve against non-hierarchical or relative base");
#+END_SRC

Hardcoded it is then.

Then:

: IllegalStateException: Workspace is closed.

Raaaah, we have code in Viewtype.loadCorrespondenceModel which queries the
workspace.  To construct absolute file URIs, again.  This time, to load the XMI
file of the correspondence model.

At this point, I have three options:

1. Fuck it, and not write tests before doing the changes.  That's not totally
   satisfactory; and I will need to write tests anyway for other changes down the
   line.
2. Make the slightest modifications to Viewtype so that it let us provide the
   proper URI
3. Run the tests as a plugin, so that a workspace is loaded


Now that I think of it, 3 would solve the previous problem as well.

Okay, that's better.  Now I can run the JUnit test using a headless Eclipse, but
still loading a workspace.  In fact, I did have to configure a "test" workspace
where I added the ECL, XMI and Ecore files needed by the viewtype to load.

I'd rather have the tests add these files to the test workspace... or even have the
Viewtype code to load them from anywhere.  But anyhow.

Managed to test the presence of features and absence of filtered features.  Now
to be a bit more thorough.

Hmm, hit a snag when trying to check the models.  All I get are
ReproduceElementImpl instances, so I can't cast them to EClass/EPackage to get
their contents or names.

I could cast to ReproduceElementImpl... but then I wouldn't be testing the
virtual access.

So maybe I'm missing something, and we should access these objects through the
EStore interface.  At least, EMF is able to construct a tree viewer from these
contents, so I should be able to inspect these as well.

*** DONE Fix the URI scheme in Eview/Eviewtype
CLOSED: [2017-06-07 mer. 17:26]
We mix platform URLs with http for finding packages of metamodels.  This is
confusing and complicates the code (there are couple instances of duplication
based solely on different URI schemes).

* [2017-05-23 mar.]
** How does the basic Ecore editor goes through our ReproduceElementImpl? :emfviews:
Because that's how I probably need to iterate on them as well.

In EcoreEditor.createModel, our reproduce rule is called by an iterator
resource.getAllContents(), which goes through all the properties.

The actual text is provided by label providers that are given by adapter
factories... EcoreItemProviderAdapterFactory is where the mapping is done from
Ecore objects (ERef, EClass, etc.) to the actual classes that do the work.

For EObjects, it uses the ReflectiveItemProvider.  Setting a breakpoint at
getText there and opening items in the tree confirms that this is the place.

#+BEGIN_SRC java
public String getText(Object object) {
  EObject eObject = (EObject)object;
  EClass eClass = eObject.eClass();
  String label = format(capName(eClass.getName()), ' ');

  EStructuralFeature feature = getLabelFeature(eClass);
  if (feature != null)
  {
    Object value = eObject.eGet(feature);
    if (value != null)
    {
      return label + " " + value.toString();
    }
  }
  return label;
}
#+END_SRC

It's just using eClass().getName().  Okay, let's try that.

It works!  I managed to test the presence of reproduced elements and virtual
associations, and the absence of filtered elements.  But somehow, I've gotten
some values from eClass.getName(), and some others from casting to an EReference
and using getName():

: assertEquals("ReqIF", l.get(1).eClass().getName());
: assertEquals(e.eClass().getName(), "Process");
: if (c instanceof EReference && ((EReference) c).getName().equals("detailedProcess"))

and I don't understand quite why I need to go to the eClass for some, but not
for others.  For the EReferences, eClass() returns EReference... which I guess
is expected.

In the model, I iterate over the /contents/ of the the BusinessArchitecture
object (presumably, a list of structural features).  For each, I can test:

: assertEquals(e.eClass().getName(), "Process");

This is what the Ecore reflective editor gives me: the name of the Eclass.  But
a "Process" in the model also has a name.  The editor gets it from
getLabelFeature and eGet above.

getLabelFeature search for a plausible feature to use as a label: if it's a
"name" attribute or if it's a String.

So I can do:

: e.eGet(e.eClass().getEStructuralFeature("name"))

to get the name of each Process instance.  Similarly, to get the detailedProcess
ref:

: e.eGet(e.eClass().getEStructuralFeature("detailedProcess"))

At first I was surprised with the results:

#+BEGIN_EXAMPLE
fr.inria.atlanmod.emfviews.elements.ReproduceElementImpl@72b10258 (eClass: org.eclipse.emf.ecore.impl.EClassImpl@6113a26f (name: Process) (instanceClassName: null) (abstract: false, interface: false))
null
null
null
null
null
null
null
null
null
null
#+END_EXAMPLE

Only one reference went somewhere, the others null?

But then I opened the model in Modisco, and it turns out that, yes, only the
first process "Booking a trip" has a detailedProcess that leads somewhere.  All
the others are empty references.

Interestingly, even though it's a reference, we don't get an empty list when
there are no elements, or a list when there is only one element.  We get a list
only for two or more elements.

** Removing the correspondenceModelBase in the EView file          :emfviews:
This should be the simplest task on the list.

With the test written and the coverage tool, I can see that: 1) we never
actually do anything with the correspondenceModelBase in Viewtype (the code is
commented out), and 2) removing the correspondenceModelBase in Eview does not
fail the test.

That's because, if we don't specify a model base, we'll just use the existing
correspondence model.

On the other hand, if we do specify the model base, we rewrite the
correspondence model every time.  Except, when the file does not exist (?!).
Putting an empty file there works, which is a bit... meh.  The limitation is due
to getting the URI from finding the file first, rather than just constructing
the URI without looking if there's a file there.

Using createPlatformResourceURI fixes it.

** Getting filters from the virtual links XMI instead of the Ecore :emfviews:
This is slightly more involved.  At the moment, we filter the elements at the
metamodel level, getting the filters from an Ecore file.

For each contributing metamodel, we:

- copy it to our virtual resource set
- remove any feature matching a filter
- load the correspondence model and add associations

What we should do instead is to get the filters from the virtual links model,
and use them to remove features from the metamodels.

So, for each contributing metamodel, we should:

- copy it to our virtual resource set
- load the correspondence model
- remove any feature matching a filter
- add associations

Although, since we specify the elements to be filtered in a different format, we
must change the matching accordingly.  In the Ecore, we did the matching
structurally.  But in the virtual links metamodel, we have only 3 pieces of data
to match the filter element reference:

: <linkedElements elementRef="//Process" modelRef="http://www.obeonetwork.org/dsl/togaf/contentfwk/9.0.0" name="Process"/>

Clearly, we are losing the hierarchical component here.  Though, maybe
"elementRef" is actually intended to be XPath?  If so, I think it means
"any node named Process", which is not more information than "name", but at
least if it's used as XPath you could be more precise than that I guess.

It seems the element ref is used by the links projector in getReferencedObject:

: r.getEObject(elementRef);

So, no XPath then.  Well, regardless, I think I'll go with a pretty basic scheme
to begin with:

: <linkedElements elementRef="contentfwk.BusinessArchitecture.drivers"
:                 modelRef="http://www.obeonetwork.org/dsl/togaf/contentfwk/9.0.0"
:                 name="drivers"/>

This should give enough information to filter the correct element without ambiguity.

Writing the search was a bit more tricky than expected, due to it being a tree.
Using a queue did it.  Not the most readable code, but it works.

Test pass, elements are filtered on the metamodel and the model.

* [2017-05-29 lun.]
** Small morning refactorings                                      :emfviews:
Removing the unnecessary HashMap as arguments to load and save for a resource.
EMF handles null arguments just fine.

On a side note, there are multiple cases of calling load with an explicit input
stream, but the suggested way to load a resource is to first give it an URI,
then call load(null).

In fact, sometimes we call load with an explicit input stream, /and then/ set
the URI.

Now to rename a few things for better coherence with the paper:

- Viewtype to Viewpoint.  That was a pain, as we had also many example files
  with the eviewtype extension, and files that referred to them...  These
  examples might not be even working anymore for all I know, but hey, coherence.

- "Correspondence model base" to "Matching model".  I guess "correspondence" was
  alright, although different than the paper, but "matching" is shorter.  And we
  had many thing beginning with 'c' already, it was getting Confusing.

At this point I discover projectile-replace: much faster!  Though it doesn't
seem to save files automatically, there is projectile-save-project-buffers;
launching magit-status also prompts to save them individually.  Still leaving
the Java refactorings to Eclipse, since I have more faith in its correctness.

- "Correspondence model" to "Weaving model".  The correspondence model is
  actually just an instance of the virtual links metamodel.  But I suppose
  there's no reason to couple the two, so "weaving" will do (besides, it's
  shorter).

- "Composition metamodel" to "viewpoint", since that's what it is.

- "Contributing (meta)models" to "contributing".  Hmm, this one will wait for
  the unification of EView and Viewpoint, where we will have a single
  "contributingModel" line.  For coherence with the other properties in an
  EView/EViewpoint file, the suffix should stay, at least for the time being.


These morning refactorings went well in the afternoon I guess.

* [2017-05-30 mar.]
** Making Viewpoint a virtual metamodel                            :emfviews:
At the moment, to construct the Viewpoint, we merely clone the contributing
packages into a virtual resource set, then remove filtered attributes and add
references corresponding to virtual associations.

The idea is to use what we have in View already to construct a Viewpoint, by
feeding it Ecore as a metamodel.  First difficulty is that EView refers to a
Viewpoint.  If we use EView to represent a Viewpoint at the metamodel level,
then it would still need a Viewpoint.

What is the Viewpoint used for anyway?

In EView.doLoad, we load a full Viewpoint from the "viewpoint" property.  We
then use this viewpoint to get a reference to the matching model and the list of
contributing metamodels.

This list of contributing metamodels is used by View to populate the virtual
resource set of the View/EView (the attribute is declared by View, instantiated
by EView, and populated by View.loadContributingMetamodels).

It seems similar to what Viewpoint is doing, except we are /not/ copying the
packages we put in the virtual resource set of View.

So at this point, we have a virtual resource set with the original packages,
/and/ a Viewpoint with its own virtual resource set containing clones of the
exact same packages, albeit modified by virtual links.

How are both used?  First, the content of the virtual resource set of View is
only used by View.getContributingModels, that is: to produce a list of the
resources contained by the resource set (but we filter out Ecore resources for
some reason).

This list is used by other getters of View, and also in doSave.  Most
interestingly, its contents are passed unfiltered to the constructor of a
MetamodelManager in EView.doLoad, along with the viewpoint.  Another point of
use of interest is View.setVirtualContents: the (filtered) list is used to
populate the virtual contents with translated virtual elements.

The viewpoint of EView is /only/ used to pass to the MetamodelManager.  So it
seems this is where the link between concrete metamodels, virtual metamodels,
models and virtual models happen.

In fact, the third argument to the constructor of MetamodelManager takes a
reference to the EView instance that created it... the EView holds a reference
to the MetamodelManager, and that's the only class where the manager is
instantiated.  Seems to me they are rather coupled.

MetamodelManager holds maps of concrete to virtual features as I have [[*Investigating View/Eview][previously
covered]].

It uses the EView reference in only one place, this test:

#+BEGIN_SRC java
 if (virtualModel != null && virtualModel.getResourceSet() != null
        && virtualModel.getResourceSet().getPackageRegistry() != null
        && virtualModel.getResourceSet().getPackageRegistry().values() != null
        && virtualModel.getResourceSet().getPackageRegistry().values().size() > 0) {
      Collection<Object> listOfVirtualMMPackages =
          virtualModel.getResourceSet().getPackageRegistry().values();
#+END_SRC

Written in a rather defensive style, this list of virtual metamodel packages is
the same thing that we pass in the first argument to the constructor... which
the constructor collects into a list of EPackage: contributingMetamodels.

Now, coverage for the test I've written tells me that the test returns false
anyway, because getResourceSet() returns null.  So at the moment, we don't do
anything at all with the EView reference.

We do use the other two arguments: the list of (unaltered) contributing
metamodel packages is put into contributingMetamodels, and the viewpoint is used
to populate the compositionClassesByName, where we put the altered metamodel
classes.

Then, in buildMaps, we iterate over each EClass from the contributing
metamodels, and if we find an EClass of the same name, belonging to the same
package, in the map of composition classes (from the Viewpoint), then we add it
to the concreteToVirtualClass map, and iterate on their features.

It's the same thing for mapFeatures: for each concrete feature in the
contributing EClass, if it also exists (by name) in the virtual EClass (the
EClass from the viewpoint), then we add the feature to two maps:
virtualToConcreteFeature and concreteToVirtualFeature.

I'm puzzled by two things: why we use names for comparisons, and why we don't
just iterate on the virtual metamodel.

Using names is brittle, and leads to the redundant checks for classes that
belong to the same package.  Also, is there any guarantee of name uniqueness in
EMF?  It looks like there is: adding a feature or class with the name of an
existing one will fail the validation.  So that's a safe assumption.  We can use
names, but it might be best to have them qualified.

Iterating on the virtual metamodel: since we will only add classes and features
present in the virtual metamodel, and we will add all of them (save for
associations), it might make more sense to iterate on them to start with, and
just get the corresponding class/feature from the qualified name in the
contributing metamodels.

Or, do a parallel descent in the trees.

* [2017-05-31 mer.]
** Re: How are virtual model attributes filtered out?              :emfviews:
Coming back to [[*How are virtual model attributes filtered out?][How are virtual model attributes filtered out?]].  I've established
that when attributes are absent in the viewpoint, they will be filtered in the
models.  But where is the connection taking place?

Stepping into an eGet call to find out where it plugs into our code.

Interesting: an eGet(EStructuralFeature) call is delegated to another eGet,
which looks up the feature ID and delegate to an eGet(int).  But in that one,
the feature ID is turned into ... an EStructuralFeature!  This the exact same
object given to the first eGet call in my debug trace.

After a while, we end up in ReproduceRule.get, and since I'm testing a virtual
association feature, in ReproduceElementImpl.getVirtualAssociation.  In this
case, it's a single reference, so this just virtualizes the target element.

Note: we cache virtualized elements in a map, but EStore also has his own cache
(see isCaching).

In eGet, if the feature is absent from the metamodel (filtered out), then EMF
raises an exception.

When we iterate on the contents of BusinessArchitecture using eContents, we
iterate on the structural features of the eClass.  So this just looks up in the
Viewpoint.

Since EMF uses the structure of the metamodel to iterate on the actual values of
the model, when they are filtered at the metamodel level, they do not appear in
the virtual model.

However, does this mean that there is a way to access these values in the model
if you know the feature name?

* [2017-06-02 ven.]
** Writing a test for modifying models                             :emfviews:
Since we have a virtual model, it should reflect changes in the models, right?
I'm not sure we support that yet, but I figure that there's nothing in the code
that should prevent it.  Caching, maybe.

I've written a small test, and it looks like changing the model does /not/
propagate the changes to the virtual model.

What blocks it?

: vea_labels.get(0).eGet(label_name)

I would expect the first ~get~ to return a proxy to the concrete object, and the
eGet would be delegated to the concrete object.

What's happening: we end up in ReproduceRule.get, where we get the concrete
feature, and the concrete element:

#+BEGIN_SRC java
EStructuralFeature cFeature = vElement.getConcreteFeature(feature);
Object value = vElement.getConcreteElement().eGet(cFeature);
if (feature instanceof EReference) {
  if (feature.isMany()) {
    value = new VirtualModelList<>(object, feature, Arrays.asList((List<EObject>) value));
    if (index != NO_INDEX) {
      value = ((VirtualModelList<EObject>) value).get(index);
#+END_SRC

Since the concrete element is a reference with >1 multiplicity, we create a
virtual list.  Hmm, that means we create a /new/ virtual list every time the a
reference is requested.  Maybe that's how EMF does it as well, providing an
immutable list.  But that does not seem necessary, since it's just a proxy in
this case, we could instantiate it once and save it for further calls, since it
will only delegate to a concrete EList.  Anyway.

We create the virtual list, and if the index is ~> -1~, we return the correct
value, otherwise we return the whole list.

Inside the virtual list, we walk the sublists to get the concrete element:

#+BEGIN_SRC java
EObject concreteEO = (EObject) l.get(k);
EObject virtualEO = virtualModel.getVirtualLinkManager().getVirtualElement(concreteEO);
#+END_SRC

Here, my concrete element is the Label EClass, and the virtual element is a
ReproduceElement containing the concrete element.

Finally, we translate it (again?) to a virtual element before returning
it... hmm.

#+BEGIN_SRC java
return (E) virtualModel.translateToVirtualElement((EObject) l.get(index));
#+END_SRC

Oh, I see.  The first part of the code only wants to find out the true index of
the concrete element, since we can have filter element that should be hidden.
Then, once we have the index, we translate the concrete element to a virtual one
and return it.

So: the ~get(0)~ call returns the Label EClass wrapped in a ReproduceElement.
So far so good.  Except, the concrete element is not the same Label instance as
the one in the concrete model.

Which kind of make sense: to construct the virtual model, we loaded the
resource.  To construct the model, we also loaded the resource from XMI, a
second time.  There's no reason for the instances to be the same.

So what's happening is we are modifying a label instance in memory, but it's
completely disconnected from the instance kept in the virtual model.

The virtual model makes no guarantee to hook into every instance of the model to
watch for changes.  I guess we could use the notifying architecture of EMF..

Our view would be updated if we saved the changes to the model, and recreated
the view... but that's not really an update anymore.

But let's follow the ~eGet~ call.  We end up in EStoreEObjectImpl.dynamicGet,
where:

#+BEGIN_SRC java
Object result = eSettings[dynamicFeatureID];
if (result == null) {
  // actually get the result and cache it
}
return result;
#+END_SRC

Oh oh.  So it /is/ caching values for us.  Here it finds the "Software kind" in
its cache and returns it.  If we remove the cached value, it goes to
ReproduceRule.get, where... the concrete value is "Software Kind".

So, yeah.  The above.  We are dealing with separate instances: the virtual model
is completely disconnected from the model, since they are loaded as separate
resources.

I'm not even sure that the virtual model /should/ reflect changes in this way.
At the very least, I would expect that changes to the underlying concrete models
held by the virtual model are reflected in the virtual model.

But we need to access the contributing models.  Let's do that.

: java.lang.ClassCastException: org.eclipse.emf.ecore.impl.DynamicEObjectImpl cannot be cast to contentfwk.EnterpriseArchitecture

Hmm that's interesting, I could cast to concrete instances when loading the
model myself, but when they are loaded by EView, they are dynamic objects...

Okay, okay, let's make it all dynamic access.

: org.junit.ComparisonFailure: expected:<[foo]> but was:<[Software Kind]>

Of course.  I'm guessing the EStore caching is the culprit here.  If I bypass
it...  Yep!  The test passes.  The change is reflected to the virtual model.

And adding:

#+BEGIN_SRC java
protected boolean eIsCaching() {
  return false;
}
#+END_SRC

to our ReproduceElementImpl is sufficient to turn caching off definitely.

But I'm not sure in what scenario one would peek at the contributing models this
way, rather than taking them straight from the resource.

The notifying approach is more promising, /if/ you can subscribe to changes from
/every instances/ of the same model, which I don't think you easily can.

Maybe hooking into the resource factory or something.  But that's out of scope.

** Accessing a filtered feature in the models                      :emfviews:
Filtered features are removed from the metamodel, but they don't seem to affect
the models in any way.  There /is/ code for skipping instances of FilterElement
in the virtual model list, but we don't construct any instances of these at the
model level.

So, in theory, we should be able to access the content of a filtered feature.
But maybe EMF does not have any mechanism to let us do so.

Wrote a test.  If you give eGet a feature object, it will convert it to a
feature ID using the eAllStructuralFeatures array.  That array is built from the
metamodel, so again, it will fail to find the feature.

I'm not sure there's a way around it.  But I'm unclear on where the eClass for a
model is coming from.  When you load a model, how do we instruct EMF to use the
filtered metamodels?  More questions...

* [2017-06-06 mar.]
** Following the trail while it's hot                              :emfviews:
Picking up where I left things last time.

The eClass for our view is inside the eProperties of the ReproduceElementImpl.
That field comes from EStoreEObjectImpl, and the class is set in
ReproduceElementImpl.init:

:    this.eSetClass(eClass);

The init method is called by the two constructors, but one constructor is
seemingly unused.  So we are left with:

#+BEGIN_SRC java
public ReproduceElementImpl(View vModel, EObject concreteElement) {
  super();
  EClass tempEClass =
      vModel.getMetamodelManager().translateToVirtualEClass(concreteElement.eClass());
  this.init(vModel, concreteElement, tempEClass);
}
#+END_SRC

The eClass used by the reproduce element is looked up in the maps built by the
metamodel manager.  That's where we assign the virtual metamodel with the
filtered features to the virtual model.

Trying to add the feature to the eClass using:

: vba.eClass().getEStructuralFeatures().add(f);

I get a nice array index out of bounds exception, since the eSettings array used
to lookup the feature is not extended when we add the feature as above.

I'm not sure it's something you'd want to do anyway.  But it doesn't work.

So I'll assume that we cannot access features filtered by the virtual
metamodel.  Good thing.

On the other hand, following the code I was reminded of something interesting:
when we filter a reference, what happens to its opposite (it if exists?).  I'm
guessing: nothing, so EMF will probably complain if we try to reach the
opposite.

Let's try it.

Hmm, inconveniently, none of the features we already filter have opposites.
Let's make a new, minimal, test.

Created a minimal ECore metamodel.  Now I need a model.  Here is the code to
generate it:

#+BEGIN_SRC java
String mmURI = "/viewpoint-test/metamodels/minimalref.ecore";
EPackage p = (EPackage) (new ResourceSetImpl()
    .getResource(URI.createPlatformResourceURI(mmURI, true), true).getContents().get(0));

EFactory f = p.getEFactoryInstance();
EObject a = f.create((EClass) p.eContents().get(0));
EClass bClass = (EClass) p.eContents().get(1);
EObject b1 = f.create(bClass);
b1.eSet((EStructuralFeature) bClass.eContents().get(0), a);
EObject b2 = f.create(bClass);
b2.eSet((EStructuralFeature) bClass.eContents().get(0), a);

Resource r = (new ResourceSetImpl()).createResource(URI
    .createPlatformResourceURI("/viewpoint-test/models/minimal.xmi", true));
r.getContents().add(a);
r.getContents().add(b1);
r.getContents().add(b2);
r.save(null);
#+END_SRC

Now the eviewpoint and eview files.  Do I need multiple metamodels in the
eviewpoint?  I think I will trigger the extension part of the code in Viewpoint
if I don't.  But it doesn't matter for filters, since these are applied
regardless.

Ugghh, spent 10 minutes debugging a typo in the modelRef of a linkedElement from
the weaving XMI...  Some validation of these files could be helpful.

Now I need an ECL file.. even though I'm only using filters, so technically I
don't need it.  Wait.  I don't need the ECL file for the view... if it doesn't
exist we'll just skip it.  But I do need an XMI for the view as well... an empty
one will do.

Okay, I can load the view without errors.  Now, I just need to check that the
reference is filtered, that it's opposite is not, and then get the opposite of
the opposite to see what happens.

Oh, interesting.  If I filter out a containment reference: A contains a number
of B, then the view will only contain A.  Since there is no way to access the B
anymore, I cannot access the opposite ref.

Let's try a non-containment then.

Strangely, ~view.getContents()~ returns a list with only one reproduce element,
for the A class from the metamodel.  Even though I haven't filtered anything
yet!

Okay, in View.setVirtualContents, we do:

: oneOftheSublists.add(translateToVirtualElement(r.getContents().get(0)));

Except, with the files I have created for this example, r.getContents() returns
the list [A, B, B], and get(0) returns just the A instance.  So that's why only
A appears in the virtual model contents.

Now, in the working example, getContents returns [EnterpriseArchiteture].  This
is coherent with the model XMI, where everything is wrapped in an
EntrepriseArchitecture tag.  Same for the other ReqIf and BPMN model: they are
wrapped in ReqIf and BPMN tags.

Hmm.  Actually, they are wrapped because that's how the model are made: they
each contain a class with containment references where everything should go.

But that means the View code will only work with such models.  Why not take
everything contained by the resource rather than just the first object?

Seeing as we already have a VirtualContents class that takes sublists...

Okay, made the change.  It shouldn't affect the existing examples since the
behavior is the same for resources containing only one element.  Now we just
don't ignore the other ones.

Wrote a test.  The filtered reference is not available on the metamodel.  Its
opposite is still present.  I think that's acceptable, if we say that views are
"lightweight", that they do not enforce EMF invariants.

However, we can get a hold of the filtered reference by the getEOpposite method
on its opposite.  That's weird.  I would expect it to return null, given that
it's filtered at the metamodel level.  Where is this getEOpposite call looking
for it?  Maybe it's cached?

But more worrying, I would expect to be able to follow the opposite reference.
That currently does an NPE in EStructuralFeatureImpl.getSettingsDelegate.

#+BEGIN_SRC java
EReference eOpposite = getEOpposite();
if (eOpposite != null)
{
  eOpposite.getEContainingClass().getFeatureCount();
}
#+END_SRC

First of all, why is this code even there?  It accesses the feature count... and
does nothing with it.

Regardless, getEContainingClass returns null... but that's not even a containing
reference.  Weird.

* [2017-06-07 mer.]
** EOpposite is set when the model is loaded                   :emfviews:emf:
That's the first answer.  When loading the XMI, bidirectional references set
their opposites to each other.

So when we filter out one part of the reference, the other still has its
eOpposite field set.

Second problem was the null EContainingClass.  It should return the class
containing the feature.  But since the feature is filtered out, it has no
containing class anymore.

Actually, bypassing the code:

#+BEGIN_SRC java
if (eOpposite != null)
{
  eOpposite.getEContainingClass().getFeatureCount();
}
#+END_SRC

we do get access to the feature value, and the test succeeds.  So this bit is
problematic.

I really don't know why it's there.  I understand that this getter is memoized,
so calling it is a way to force the computation of whatever underlying data it
returns.  But if you need to force the computation, it's because you are peeking
under the sheets of the interface; ergo, doing something you shouldn't.  Or, you
don't really need to force the computation, and this code is useless.

Doing a bit of git spellhunking...

: http://git.eclipse.org/c/emf/org.eclipse.emf.git/
: git://git.eclipse.org/gitroot/emf/org.eclipse.emf.git

Found [[orgit-rev:~/proj/org.eclipse.emf/::22137e7][one commit]] from 2005 (!) where the code was updated, but the strange ~if~
was already there:

#+BEGIN_SRC diff
       if (eOpposite != null)
       {
-        eOpposite.getEContainingClass().getEAllStructuralFeatures();
+        eOpposite.getEContainingClass().getFeatureCount();
       }
#+END_SRC

Before that, I get the initial git commit from 2004 when the repo was created by
splitting from a previous CVS probably (there are .cvsignore files lying
around).  From what I can find, the CVS repository is now unavailable, so I
won't get any history beyond that.

** Cleaning up URI loading                                         :emfviews:
This was getting annoying to deal with a dummy workspace just to have the test
resources.

Asked G about it, he suggested I use createURI instead of
createPlatformResourceURI, to avoid being tied to the workspace.  We tried it
together, and using relative paths with createURI will load the files from the
current plugin.

Had to make changes in multiple places where we previously used platform URI, or
worse, findMember on the workspace.  Now it's more homogeneous.

He also suggested we pass URI to the EMFViews core instead of strings, so that
we leave the problem of creating and resolving them to the client.  I agree, and
would even go as far as passing resources directly.

*** Investigate duplication of model loading from XMI
Since in Viewpoint we pass around URI strings when we really want to deal with
EPackages, there may be some duplication where we load a package from the XMI
instead of getting an EPackage directly, or getting it from a registry.

That seems like unnecessary work, and a potential source of bugs (since we have
clones of models lying around, so strict equality wouldn't work).

* [2017-06-09 ven.]
** Trying out Eclim for controlling Eclipse from Emacs        :emacs:eclipse:
Out of the box I set the bar a little too high for Eclim, since I'm running
Eclipse 4.7 M6, and only 4.6 is supported at the moment.

There is a development branch for 4.7 on the Git.  I try it out, follow the
build from source guide.  But it fails to build on my Eclipse config, since I
have a separate configuration folder and platform folder.

Ok ok.  Maybe I should try a plain Eclipse to see if it's even worth the
trouble.  I'm afraid it doesn't have useful stuff like Javadoc on hover (maybe
using Eldoc?).  Let's see.

It installs with 4.6.  Now I run the eclimd daemon from inside Eclipse
(View->Eclimd).

Not that slow.  You can get Javadoc for a type, not on hover, but with a
binding.  Auto-completion seems to work, although you don't have the Javadoc for
completion items.

I think the way windows are created and (not) disposed for each function is more
annoying than helpful.  I'll keep my current setup.

** Using Eview for metamodels                                      :emfviews:
To use EView for the metamodel level, we would need to provide a viewpoint.  The
viewpoint is used to populate the maps in the metamodel manager; essentially,
its role is to assert what features are present on the viewpoint.

If the Eview is used for the metamodel, its viewpoint should be the Ecore
metamodel itself.  Maybe I can try building a test around that.

* [2017-06-12 lun.]
** Made a class diagram of emfviews.core                          :emfviews:
A bit hairy.  PlantUML uses GraphViz behind the scenes, so the layout engine
quickly shows its limits when you get a dozen of boxes.

Regardless, it helps to see the whole picture.

I think I want a sequence diagram of Viewpoint.doLoad, and most importantly,
EView.doLoad.  Probably an object diagram of everything created by
EView/Viewpoint for the minimal example.

* [2017-06-16 ven.]
** More diagramming                                                :emfviews:
I fleshed out the class diagram a bit, fixing the layout so it's more readable.
Dependency arrows are in a light color so they don't drown the rest of the
information.

I wrote a small JS bookmarklet to add interactive highlighting of the outgoing
edges from a node, so we can make sense of all the information that's in it.

** Review of tools for creating UML class diagrams                      :uml:
I tried other tools for UML diagrams; small tools like Dia or yEd give fine
control over the layout, but you have to do everything graphically.  yEd has
advanced layout and grouping features, with collapsing.

Can't easily export PlantUML output to something these tools eat, unfortunately.
PlantUML has XMI export, but it's only for the classes, not the arrows.

Large tools like Modelio/Papyrus/UML Designer are slow and cumbersome for
drawing just a class diagram if you don't need all their facilities for
generating code from it.  One of them froze when I tried to zoom out on the
nearly empty canvas.

I think yEd is decent enough for quickly whipping something up, and the
auto-layout features does a better job than GraphViz.  Too bad it's proprietary.
Though you can export to GraphML which is an open format.

Trying out the ObjectAid plugin for generating class diagrams directly from the
code.  It does a decent job at that, it can even show dependencies between
classes.  I think it's picking up constructor calls; but not casts.  The
auto-layout feature avoid nodes overlapping, but seems to largely ignore edges,
so it's overall useless.  Also, you cannot control what hide/show individual
attributes or links.  The diagram is serialized to XML by default, so it's
reusable, but there is no export to a common graph format, only images (and no
SVG).

** Exciting use case for EMFViews                              :emfviews:uml:
That gets me thinking.  The output of automated tools for generating class
diagrams will always be too rich, will contain too much information.  Usually,
you want to filter out this information to focus on a specific functionality, to
understand the project piece-wise.  That's why I started out by mapping the
project manually, because I knew that I wanted to do a partial representation of
the project, even with some simplifications.

So, usually, you want /views/, not the whole picture.  Using EMFViews to create
partial views of programs would be a terrific use case (and using it on itself
would be nice).  We might also want to aggregate info from multiple models: like
a model of which Java statetement was executed by some test (impact analysis),
combined with a MoDisCo model to create a view of a class diagram where you only
see the class/methods/attributes used by a specific test.

Interestingly, MoDisCo has entries for each cast and constructor invocation in a
project, but if you want to know that a specific type is used as argument to a
method or indirectly through the return value of a method (i.e.,
a.getB().foo()), it seems you have to extract it yourself.

* [2017-06-20 mar.]
** Converting PlantUML to GraphML                              :plantuml:uml:
I've finished this big object diagram, giving me a concrete view of what objects
are created when we call EView.load.

It's a bit cumbersome to navigate, since GraphViz made a bit of a mess of it.
It's about twice as large as the class diagrams, and it's only a /very/ simple
example (there are no virtual associations involved, and only one contributing
metamodel).

I was thinking of looking if yEd was able to get a better layout out of it.
Since this is the second diagram I made where I wish there was an easy way to
import PlantUML files into yEd, let's shop for solutions.

There is a [[https://github.com/Kesin11/plantuml_class_digram_parse][PlantUML parser there]].  It's in Perl, and doesn't seem to parse
much.  Class diagrams, classes and relationships.  But doesn't look like it
parses labels on relationships.  Also, I don't know Perl.

There is [[https://github.com/leungwensen/plantuml-parser][one in JS]].  This one looks rather complete.  There's a PEG grammar file
that looks exhaustive.  But it doesn't seem to work as-is.  Calling plantUMLParser.parse:

#+BEGIN_EXAMPLE
	      peg$startRuleFunctions = { start: peg$parsestart },
	                                        ^

ReferenceError: peg$parsestart is not defined
#+END_EXAMPLE

Apparently, the parser is lifted from [[https://github.com/bafolts/plantuml-code-generator][plantuml-code-generator]].  That project
looks more fleshed out (there's a README, at least).  And in fact, that's where
the PEG grammar is coming from.

Trying it out.. it looks like loading the parser generated by pegjs does the
trick.  On a simple string, it looks for the "@startuml" line, so that's a good
start.

On my object diagram though:

#+BEGIN_EXAMPLE
/home/fmdkdd/proj/plantuml-to-graphml/plantuml.js:2247
      if (peg$c91.test(input.charAt(peg$currPos))) {
                             ^

TypeError: input.charAt is not a function
    at peg$parsenoise (/home/fmdkdd/proj/plantuml-to-graphml/plantuml.js:2247:30)

#+END_EXAMPLE

:(

Hmm, wait a minute.  Maybe I'm not passing a String?  I was using fs.readFile
from node, and I forget to pass the encoding.

Haha, yes!

#+BEGIN_EXAMPLE
/home/fmdkdd/proj/plantuml-to-graphml/plantuml.js:3299
      throw peg$buildException(
      ^
SyntaxError: Expected "@startuml", [ \t], [\n], [\r\n] or end of input but "'" found.
    at peg$buildException (/home/fmdkdd/proj/plantuml-to-graphml/plantuml.js:361:14)
#+END_EXAMPLE

Doesn't support comments?  Looks like it does, but not before the ~@startuml~
tag.  PlantUML doesn't care... fixing.

#+BEGIN_EXAMPLE
/home/fmdkdd/proj/plantuml-to-graphml/plantuml.js:3317
      throw peg$buildException(
      ^
SyntaxError: Expected [^\r\n] but "\n" found.
    at peg$buildException (/home/fmdkdd/proj/plantuml-to-graphml/plantuml.js:361:14)
#+END_EXAMPLE

What?  Grmbl grmbl.  If you don't tell me /where/ in the input you failed to
match. that's going to be tedious.

There's a ~--trace~ option I can pass to pegjs.. but it's still not telling me
where it failed.

plantuml-code-generator uses pegjs 0.9, and there's a 0.10 with "improved error
messages".  Let's see.

Still doesn't tell me the location in the input where it failed to match...
Ahah!  Catching the syntax error in a try/catch, it does include location
information.  The default toString does not report it.

Okay, so it's not taking '/' or '.' in attribute names.  Pretty strict.  I guess
it's because the grammar is used to /generate/ code from the PlantUML file, so
you don't want anything funky in your identifiers.  But it ultimately depends on
the language, so...

After making it more lax with object names / members names, it appears it's not
parsing names of relationships either.

At this point, I know that I can use PegJS to make a more generic parser... or I
could write my own... or I could just use the PlantUML parser and add a GraphML
exporter in there.

I think I'd rather have it as a standalone tool than an addition to PlantUML.

[[http://graphml.graphdrawing.org/primer/graphml-primer.html#Graph][GraphML has a spec]], under CC-by.  Doesn't look too fancy, just XML.  A good POC
would just output nodes and edges, ideally with attributes (as a label inside
the node?  Just have to check out what yEd exports...)

#+BEGIN_EXAMPLE
    <node id="n0">
      <data key="d4"/>
      <data key="d5"/>
      <data key="d6">
        <y:UMLClassNode>
          <y:Geometry height="120.48000000000002" width="262.56000000000006" x="318.71999999999997" y="-10.240000000000009"/>
          <y:Fill color="#FFFFFF" transparent="false"/>
          <y:BorderStyle color="#C0C0C0" type="line" width="1.0"/>
          <y:NodeLabel alignment="center" autoSizePolicy="node_width" configuration="CroppingLabel" fontFamily="Dialog" fontSize="13" fontStyle="plain" hasBackgroundColor="false" hasLineColor="false" height="21.1328125" horizontalTextPosition="center" iconTextGap="4" modelName="internal" modelPosition="c" textColor="#000000" verticalTextPosition="bottom" visible="true" width="262.56000000000006" x="0.0" y="3.0">Viewpoint</y:NodeLabel>
          <y:UML clipContent="false" constraint="" omitDetails="false" stereotype="" use3DEffect="false">
            <y:AttributeLabel>-contributingEPackages: List&lt;EPackage&gt;</y:AttributeLabel>
            <y:MethodLabel>#doLoad()
#doSave()
+getContents()
+getResourceSet()</y:MethodLabel>
          </y:UML>
        </y:UMLClassNode>
      </data>
    </node>
#+END_EXAMPLE

Looks like yEd has its own additions to the GraphML format... That was more or
less expected, since the GraphML doesn't have much in it, and yEd adds a bunch
of information.

** Observations from the diagram                                   :emfviews:
The EPackage has three clones lying around.  One is the original, one is the
filtered copy, but then one is used by the VirtualLinkManager to map virtual
classes.  There's probably a better way to do it.

I'm not sure that have not duplicated other stuff as well.  Should redo the
experiment by noting the objects id to be sure.

* [2017-06-21 mer.]
** Is it possible to use neato instead of dot with PlantUML?       :plantuml:
Not out of the box.  There is an argument ~-graphvizdot~, but passing
~/usr/bin/neato~ has no effect.  There is also some "layout strategy" option in
the parser, but it's not connected to anything.

So, the PlantUML -> GraphML route still stands.

* [2017-06-22 jeu.]
** Getting a heap dump from Eclipse                            :eclipse:java:
If I wanted to generate an object diagram mechanically, I would first need to
get a heap dump.

Looks like there are at least [[https://stackoverflow.com/questions/25168490/java-eclipse-create-heap-dump-on-breakpoint][two ways]] to do that.  jvisualvm comes with Java,
and you can plug into an existing process.

So I put a breakpoint in Eclipse, and obtain a heap dump in the HPROF format.
JVisualVM allows me to browse this dump more or less like the Eclipse debugger
can (there's fewer "nice" short string format).

There's also ~jhat~, a bundled command, that creates a local server to browse an
HPROF dump.  It's not very useful for browsing through, since it doesn't show
attribute values.

Most tools dealing with heap dumps seem targeted to people who want to find
memory leaks, understandably.  Like [[http://www.eclipse.org/mat/][MAT]].

JVisualVM accepts plugins though, so we could imagine an export to DOT.

Another way to obtain an HPROF dumb, without jvisualvm:

: jmap -dump:format=b,file=/tmp/foo.hprof PROCESS-PID

This can then be opened in MAT for browsing.  Though I'm not seeing values for
inherited attribute in MAT... but unless JVisualVM was doing some peeking in the
runtime process when displaying the HPROF, the info is probably there.

[[https://web.archive.org/web/20121221115642/http://java.net/downloads/heap-snapshot/hprof-binary-format.html][Here's some documentation]] on the HPROF binary format.  But it might not be the
freshest info.  Looking at the source of jmap or jvisualvm is also an option.

** Some concerns about creating object diagrams mechanically       :emfviews:
First, heap dumps are huge, and you are concerned with only a fragment of it at
one time.  MAT reports nearly 500k objects in the heap for the small test I
built the object diagram from.  In the object diagram I made, there are ~50
objects.  Whitelisting interesting objects should be the default.

Second, heap dumps are instantaneous and cannot give any temporal information
individually.  That's relevant for keeping track of objects collected by the GC.
In the object diagram I made, an instance of LinksProjector is created by
VirtualLinkManager to setup virtual associations, but no reference to it is held
so it is collected when the method ends.  Depending on when the heap dump is
captured, you might or might not see this object.  Taking multiple dumps can
help, but is not guaranteed to be correct, unless you are able to register all
allocations.

In fact, the JVM can also perform escape analysis and decide to allocate objects
directly on the stack.  In that situation, I'm not sure the object would appear
in a heap dump at all.  So, again, heap dumps cannot give you a full list of
created objects.

Third, you might want to enrich the visualization of some types or values.  In a
heap dump, objects are just nodes that points to other objects.  But depending
on the type, you might want to make them distinguishable.  Lists, arrays and
hashmaps are easier to recognize as tables.  In the object diagram, I've made
simplifications for URI objects to just their string value.

* [2017-06-26 lun.]
** Wrote a test for virtual associations                           :emfviews:
Using minimal metamodels and models.  The main difference with the minimal
filters test is that this time we need a view ~weaving.xmi~ that's not empty.

Providing the viewpoint weaving.xmi is not enough.  It's used to construct the
metamodel of the virtual model, but not to populate it.  The weaving.xmi of the
view is used by the LinksProjector to create the virtual associations.

One difficulty to create this view weaving model was to find the correct
~elementRef~ values.  A first clue is in LinksProjector.getReferencedObject,
where this field is used:

: referencedElement = r.getEObject(elementRef);

It calls Resource.getEObject, which accepts an URIFragment from EMF.  Here are
examples of URI fragments from the three-model-composition test (where they are
generated by ECL):

#+BEGIN_EXAMPLE
  //@architectures.1/@processes.0
  _48wAUN6xEeCbzp_EHZybUg
  //@architectures.0/@strategicElements.0
  rmf-19428170-0b70-4b81-9fd2-0cdec5778a49
  //@architectures.0/@strategicElements.1
#+END_EXAMPLE

Some are "structured fragments", that reflect the path to follow in the
resource.  It's a mix of structural features names and indices for contents
lists.  Others are "IDs", which look like hashes.

To find the correct fragment, one can load the models in a resource, and call
~Resource.getURIFragment(eObject)~ on the EObject in question.

Turns out, with the simple models that I have, the fragment ~/0~ works.

Now, it's brittle to use resource-dependent indices; I'd rather use a qualified
name, but since this field is supposed to be created by ECL... maybe there's no
easy way to construct a qualified name there.

Hmm, using ~/~ as fragment also works in this case...  I cannot find
documentation on the URI scheme used by EMF.  All I found was a comment by Ed
Merks saying that it was "XPath-like", but not XPath.

Regardless, after discussing with H., I think we are aiming towards qualified
names for this field, as they are more readable.  Just have to check that ECL
can generate them.

* [2017-06-28 mer.]
** Rethinking the virtual links metamodel                          :emfviews:
We have some attributes that are never used, some attributes that are used in
bad ways, and things we cannot express.

The requirements are:
- We can have any number of contributing models
- We can transpose concepts and properties from contributing models in the view
  so they appear in the view
- We can filter concepts and properties from contributing models so they do not
  appear in the view
- We can create new concepts, properties and associations in the view that do
  not exist in the contributing models

Additionally, we can choose between "blacklist" and "whitelist" modes: blacklist
mode, the default (and current behavior), is when the view contains all the
concepts of the contributing models, and you can filter some concepts /out/
explicitly.  In whitelist mode, no concepts are included by default, and you
must filter them /in/ explicitly.

We could also want the same feature at the concept level, to filter out/in
properties for each concept separately.

After sketching a few alternatives and iterating with H., we converged [[file:doc/virtuallinks-metamodel3.svg][on one
design]] that ticks all the boxes, and should be a definite improvement over what
we had so far.

Some notes:
- It supports blacklisting/whitelisting at the view level, but not for
  individual concepts.  You don't lose expressive power, but only convenience
  and maybe performance by having to exclude or include a bunch of properties.

- The fact that synthetic elements (New*) are also instances of LinkedElement
  (through VirtualElement) lets use them as target for other synthetic
  elements.  So a NewProperty can be added to a NewConcept, rather than being
  locked to concepts belonging to the contributing metamodels.

  This is also means there is a risk of circular dependencies (if A and B are
  new elements, and A refers to B and B refers to A).  This can happen for,
  e.g., associations: Assoc A1 from A to B has opposite A2, A2 from B to A has
  opposite A1.  Cannot set the opposite fields before the two associations are
  created.

  We'll see how best to resolve that once we get to translate the metamodel to
  EMF code.  Using the order of elements from the resource looks like a decent
  first approach.

- The model is quite orthogonal, but lacks constraints for invalid situations.
  For instance, NewAssociation ~opposite~ field should target either
  NewAssociation (if VirtualElement) or have an FQN that points to an
  EReference.

  Likewise, a NewConcept can have sub- and super-concepts, but you can create
  non-sensical hierarchies like A < B and B < A.

* [2017-06-30 ven.]
** Switching to the new virtual links metamodel                    :emfviews:
Wrote the metamodel in Ecore, generated the code.  Errors everywhere.

Disregarding the UI and Editor plugins for now, to focus on the core and on the
tests.

Apart from renaming things to follow the new metamodel, the code is already
simpler in a few places, when fetching the new element from the metamodel or
model notably.

For now I'm just trying to get back the previous functionality, and disregarding
the additional functionality like creating associations between virtual
elements, and synthesizing new elements (for which I didn't have tests anyway).

One difficulty is that we cannot use fully-qualified names to target model
elements.  The code in EMFViewsUtil.findElement is not working for models
because it tries to get the "name" feature on the EClass, but model objects have
their metamodel class as EClass, and these don't necessarily have a name.  We
/could/ write a findModelElement function that first gets the structural feature
corresponding to the FQN, and then try to eGet this feature in the model.  That
wouldn't be sufficient if we wanted to target EClass instead... so the approach
has to be different.  Besides, one named feature on the metamodel may correspond
to multiple model objects, in the case of lists.

For the moment, I'm sticking to EMF URI fragments for the model level.  That's
one discrepancy between how we use the virtual links metamodel for viewpoints
and for views.

Currently passing 2 tests out of 5... the three-model-composition is larger, so
harder to fix.  It generates the weaving model for the view through ECL, so I
have to fix that as well.

Fixing ECL...  looks like it's still missing something.  Will investigate next
week.

* [2017-07-03 lun.]
** Installing Eclipse Oxygen                                        :eclipse:
Release version.  I was on milestone 6 previously.

As usual, since it's easier to add plugins than to remove them from Eclipse, and
a lower number of plugins makes for a healthier Eclipse, I went with the Oxygen
platform download:

http://download.eclipse.org/eclipse/downloads/drops4/R-4.7-201706120950/

Then, I added:

- JDT
- PDE
- EMF SDK (EMF is already included)

EMFViews requires also OCL, Epsilon and Epsilon EMF integration.

I used the update site for Epsilon 1.2, since migration to version 1.4 is still
a TODO.

After that, and after making sure the target platform and run configuration are
coherent, I was able to run the tests.

** Fixing tests for the new metamodel                              :emfviews:
We actually have 4/5!  Three-model-composition runs into an NPE.  Investigating.

We fail in EStructuralFeatureImpl:

#+BEGIN_SRC java
EClassifier eType = getEType();
Class<?> instanceClass = eType.getInstanceClass();
#+END_SRC

eType is null.  Since a feature is a typed element, it /shouldn't/ be null.

Found it: the setEType in Viewpoint is null... because targetElem is null.
That's because of a wrong path in the viewpoint weaving model (BPMN instead of
bpmn2).

Will have to make sure we find the element before creating associations...

Still, too bad EMF doesn't inform us that the EReference is invalid before...
That probably stems from the fact that we create the EReference piece-wise, so
we can violate invariants before we are done initializing it, and there's no way
to tell EMF we are done.  The builder pattern solves this.

Added some checks.  Found a bug in another concreteElement for reqif (reqif10 is
the name of the root), which was not tested.  Early errors pay off!

* [2017-07-04 mar.]
** Writing tests for adding concepts                               :emfviews:
Some unresolved questions.

Where do new concepts go?  If they subtype an existing concept from a
contributing package, they could go in that package.  But if they subtype none?
Or if they subtype two concepts from separate packages?

After discussing with H., we settled on putting all new concepts in a specific
package that has the name of the viewpoint.  Now the viewpoint must have a name.

In testing for this feature, there's a bit of non-determinism: the order of
packages in the virtual contents of the viewpoint is undefined.

Actually, the order stems from the order of the underlying hashtable used by the
package registry were we put these packages.

It would make sense to specify the order.  One order that seems obvious is to
take the order of the contributing models, plus the extra virtual package after
these.

I added support for new properties as well.  We now have basic functionality as
provided by the new metamodel.  I should go over what was possible in the
previous version to make sure we have not lost any expressive power.

* [2017-07-05 mer.]
** Handling errors in EMF models                                   :emfviews:
Wrote a small test.  EMF does not seem to do any validation of metamodels
created dynamically.  However, calling Diagnostician.INSTANCE.validate can give
you a list of diagnostics: if two attributes are the same name, then it's not
OK.

The code that does the actual validation is in ecore.util.EcoreValidator.

** Handling errors in Viewpoint                                    :emfviews:
There are many ways in which loading a weaving model and executing it can fail:

#+BEGIN_SRC java
EObject parent = tryGetEObject(p.getParent());
if (!(parent instanceof EClass)) throw new InvalidLinkedElementException(String
    .format("Parent of new property '%s' should be an EClass", p.getName()));
EClass parentClass = (EClass) parent;

String n = p.getName();
if (n == null) throw new ViewpointException("New property name is null");
if (!n.matches("[a-zA-Z][a-zA-Z0-9]*")) throw new ViewpointException(String
    .format("New property name '%s' should be non-empty, start with a letter, and contain only letters or digits",
            n));

for (EStructuralFeature f : parentClass.getEAllStructuralFeatures()) {
  if (n.equalsIgnoreCase(f.getName())) throw new ViewpointException(String
      .format("New property name '%s' is already taken in class '%s'", n,
              parentClass.getName()));
}
#+END_SRC

In many respect, the weaving model is a language, and Viewpoint.loadWeavingModel
is an interpreter.  These ViewpointException are thus semantic errors, stemming
from an invalid usage of the language.

Now, the problem is that mixing exceptional code with the happy path is hard to
read.

Solutions:

1. Do the validation in a separate class, before going down the happy path.

   That amounts to writing an interpreter that visits the whole model,
   simulating operations taken by the viewpoint, but not actually creating any
   side effects, just throwing exceptions if the invariants are not obeyed.

   Pros: happy path is totally separated from exceptional code.  All possible
   errors are treated on the side.

   Cons: duplication of visiting code, duplication of logic (to check if a
   NewConcept has a unique name, you have to collect all the new concepts),
   duplication of work (validation is interpreting once, and the happy path is
   interpreting a second time).

   Not sure if we can actually catch /all/ errors without creating the
   side-effects on Viewpoint.  So there might still be exceptional code in the
   happy path.

2. Keep the validation in the happy path, but as one-liners.

   E.g., call ~validateName()~ on the name before using it.  The code in
   validateName takes care of all exceptional cases, and is hidden away.

   Pros: exceptional code is kept to a minimum in the happy path.  No
   duplication of visiting code/logic/work.

   Cons: If we do any work optionally, then we may never raise an error for an
   invalid usage.  That's like every dynamic programming language, where code
   that is never executed is never checked.


Will think more about this on the way home.

* [2017-07-07 ven.]
** Dealing with exceptional cases in Viewpoint                     :emfviews:
I've adopted solution 2.  It's the simplest, and with an utility function like
this:

#+BEGIN_SRC java
  private ViewpointException EX(String msg, Object... args) {
    return new ViewpointException(msg, args);
  }
#+END_SRC

You can write:

#+BEGIN_SRC java
if (model == null)
        throw EX("Model '%s' of concrete element cannot be found in package registry", modelURI);
#+END_SRC

Which is minimal noise.

In concert with the Optional type, we can write this:

#+BEGIN_SRC java
EObject obj = EMFViewsUtil.findElement(model, path)
          .orElseThrow(() -> EX("ConcreteElement '%s' cannot be found in model '%s'", path,
                                modelURI));
#+END_SRC

I've thought about moving the different types of errors in ViewpointException,
like so:

#+BEGIN_SRC java
static ViewpointException INVALID_NAME(String msg, Object... args) {
  return new ViewpointException("Invalid name '%s'", String.format(msg, args));
}
#+END_SRC

But that's creating an interface that I have no use for at the moment.  Maybe
when I write tests for failures, that would be easier to check against an error
type (from an enum?) than the exact string message.

* [2017-07-10 lun.]
** Reviewing features cut in the new weaving metamodel             :emfviews:
From what the previous code did for the "Extension" case in Viewpoint.

| Previous          | Now    |
|-------------------+--------|
| refine            | yes    |
| generalize        | yes    |
| add property      | yes(1) |
| filter property   | yes(2) |
| filter class      | yes(2) |
| add constraint    | no     |
| filter constraint | no     |
| modify property   | yes(3) |
| add reference     | yes(4) |
| filter reference  | yes(2) |

1) we don't support all primitive types yet, but that's a matter of minutes.
2) superseded by ElementFilter
3) indirectly through filter+add, but are they really equivalent?
4) the previous code supported specifying containment reference

So we are not missing anything major.

Now, the most interesting aspect of the new weaving metamodel is the ability to
target virtual model for properties, concepts and associations.  Let's focus on
that.

The "VirtualLink"/"VirtualElement" objects that are in the weaving model can be
thought of as "instructions" or "construction orders": that's how they are used
by the Viewpoint.  From a NewConcept, we will create an EClass.  The two objects
are different, but the NewConcept stands in for the EClass, and there's a clear
mapping from one to the other.

So, we can collect the EObject created from a VirtualLink in a map, and look up
this map whenever we want to target a VirtualElement.

** Solving circularity in the new weaving metamodel                :emfviews:
As already noted, creating a NewAssociation R1 with an opposite R2, R2 has to
exist at the time R1 has its opposite set, otherwise it cannot work.

Currently, since new associations are created in the order given by the XMI,
this cannot work.

Solutions:

1. Delay setting the opposite value of new references until after all references
   are created.

   That fixes the circularity for opposites, but not for other cases.  Are there
   other cases?  I'm not sure.

2. Do two passes: one to create virtual elements and populate the mapping, and a
   second one to set their EMF fields.

   Solves all potential circular dependencies.  Also removes the order
   constraint of the XMI elements.  Slightly more code complexity.

3. A fully virtual metamodel would not have this issue, as it would be lazy
   anyway.  The value for the opposite would be resolved only when code queries
   it (e.g., the Diagnostician).

   Might be the solution we end up adopting anyway.


Circular references could happen for concepts if, e.g., C is new and has D as
subconcept, and D is new.  D effectively has C has superconcept, but we don't
need to repeat it.  If we did, then we would end up with a circular dependency.

I'll go with #2, in the interest of correctness.

Done.  Code complexity was not increased much.  A bit of additional implicit
control flow dependency between methods (have to populate the synthetic element
map first).  Might be mitigated by making the map an explicit argument of the
build* methods.

* [2017-07-11 mar.]
** Implementing whitelisting                                       :emfviews:
In blacklisting, when you filter an element, you are implicitly filtering every
element under it, since we are dealing with trees.

So it makes sense that in whitelisting, when you filter an element /in/, it
should implicitly include all the elements that are /above/ it in the tree, but
not the elements below.  Otherwise, you cannot access the element, and you would
have to explicitly filter /in/ everything above it.

In practice, we can delete everything that is not filtered.

Idea: go through all contributing packages and their contents, recursively, and
create a list of (path, objects) tuples, where path is the qualified name to
access the objects.  Then, remove all tuples from the list where path is a
prefix to to an element filter.  Then, delete all objects that remain.

But Java has no tuples.  Okay, I guess I can generate the list on the fly: for
each EObject, determine its path, if it's not a prefix of any element filter,
add the object to the list.  Delete all objects in the list.

If an EObject is not a prefix of an element filter, we can add it to the
elements to delete /and not descend to the children/.  But if it is a prefix, we
have to descend until it's a match, and add the children to the elements to
delete.

Well, no, actually we can't short-circuit, because one of the children may be
explicitly whitelisted as well.  So we can delete only objects whose path is not
a prefix of any element filter.  That simplifies the code, but increases time
complexity.

* [2017-07-12 mer.]
** Whitelisting complexity is slightly worse than blacklisting     :emfviews:
Whitelisting is O(len(model)*len(filters)), where len(model) is the total number
of elements in the model graph, across all contributing packages.

Blacklisting loops on the filters, but it calls findEObject, which calls
findElement, and that is O(len(models)) as well.  So the two complexities are
equal.

Except than in practice, findElement can shortcut, while whitelisting in
applyFilters never shortcuts.

But, whitelisting works now.  The better approach would certainly be to be fully
lazy.  Will think about that.

** Weaving metamodel variation                                     :emfviews:
In the last meeting, it has been brought up that the new weaving metamodel was a
bit confusing: new associations sources and targets point to linked elements,
but these can be new association, which does not make sense.

At the moment it's a runtime error, but it could be enforced by the metamodel.
We could use OCL constraints, but just having a separate type for valid targets
would work.

However, while we could do that for virtual elements, we cannot do that for
concrete elements, since we only have a path to link to them.

Unless it's possible to link directly to the concrete elements... by loading the
weaving model alongside the contributing models in the same resource set?

Wrote a test... it's definitely possible to add EObjects directly into the
weaving model, save and load that resource.  No need for findElement, and it
should work with any EObject, not just named objects.

Made [[file:doc/virtuallinks-metamodel6.graphml][another class diagram]].  But I'm not sure it's worth it, since it might tie
too much to Ecore.

Here is a [[file:doc/virtuallinks-metamodel7.graphml][slight variation]], where we only add "Concept" and "Association" and
remove "VirtualElement".  That way, we are sure that synthetic elements cannot
be bogus.  But we still have no clue for concrete elements, since we only have
the FQN string.

* [2017-07-18 mar.]
** Virtualizing the viewpoint                                      :emfviews:
Currently, Viewpoint is not truly virtual: we use the weaving model to build the
viewpoint at construction time, and it can never change.

The upside is that it's simple: you deal with stuff that's not changing.  And
it's probably more efficient that way: the virtual contents are built ahead of
time.

The downsides are that, for large metamodels, it might not be that efficient to
iterate over all of their features ahead of time, if only a few of them are ever
used.  And we lose the ability to mutate the viewpoint when the metamodels
change...

But how often to metamodels change?  And we can even subscribe to their changes?

Also, if an EAttribute changes name, or is deleted, but it was the target of a
NewAssociation in the weaving model, now the weaving model is invalid.  Should
the viewpoint subscribe to changes to the weaving model as well?

H. reminded me that the point of EMFViews is to have a /lightweight/ solution to
combine multiple models.  Not only to create new metamodels and models from
existing ones—model transformation tools can do that already—but to combine
huge models quickly.

Another concrete use case that surfaced is for NeoEMF: using EMFViews to control
the visibility of features for users with different access privileges.

*** Use case 1: lightweight combination of models
Now, for the first use case.  Let's say you have a few metamodels and want to
create a viewpoint.  You:

1. Create an .eviewpoint file
2. Specify the weaving model
3. Create the viewpoint

Now, let's assume the viewpoint contents are built at creation, and cannot be
updated.  That's the current state.

**** Updating a metamodel
You update the metamodels.  You remove a concept, or rename a concept.  The
viewpoint doesn't change.  You have to recreate it.

- Rebuild viewpoint

This discards the viewpoint and rebuilds it, using the .eviewpoint file and
weaving model once again.

If that concept was referenced by the weaving model and you did not propagate
the changes to the weaving model, then the viewpoint will emit an error.  Then
you need to:

- Update weaving model
- Rebuild viewpoint

Otherwise, the viewpoint will now reflect the new versions of the metamodels and
weaving model.

**** Updating the weaving model
Now you update the weaving model, by adding another contributing model.  The
viewpoint still does not pick up this change.  You have to:

- Rebuild viewpoint

Every time you change the weaving model.

*** Use case 2: access control
You define a viewpoint for access control on an existing metamodel:

1. Create an .eviewpoint file
2. Specify the weaving model (whitelist)
3. Create the viewpoint

It looks like the same thing.  Actually, in use case 1, the actions are probably
interactive, and ultimately map to actions the core API.  In use case 2, the
actions are probably direct calls to the API.

*** When the viewpoint is virtual
One approach is to make the viewpoint fully lazy.  Currently, the only entry
point to a Viewpoint is the ~getContents~ method of the resource.  And
currently, we return a virtual contents object that has been built at loading
time.

If, instead, we build the virtual contents object each time ~getContents~ is
called, then we are guaranteed to always reflect the latest states.

*** Let's focus on virtualizing the viewpoint without considering updates for now
After debating with H., we just want to have the Viewpoint emit virtual
elements, as is the case currently for EView.

How to handle updates in the viewpoint or weaving model is then a separate
concern that we might tackle later.

** Trying to wrap the objects returned by Viewpoint                :emfviews:
In the VirtualContents.  Questions abound.

First of all, most of the tests that deal with Viewpoint should be changed to
use the reflective EMF API.  Now, we know that we have EPackages, but we should
have VirtualElements that may or may not be packages.

But, do we need to wrap recursively the contents of EPackage?

* [2017-07-19 mer.]
** Virtualizing viewpoint: first approach                          :emfviews:
The point is to avoid copying the contributing models, and just proxy.

For each contributing package:

- create a new VirtualEPackage that delegates to the EPackage

That should be transparent right?

Looks like the Diagnostician is not happy, as it assumes it can convert to
InternalEObject... but these are not.

Too bad.  Let's toggle it off.

All my tests pass... though they shouldn't, since here I am modifying the
original metamodels.  But the tests never check for that.

Hmm, but this approach won't work.  Or at least, it works at the metamodel
level, but we should use the same architecture for the model level.  There, we
will have no other choice but to be a VirtualEObject.  If VirtualEPackage is a
subclass of VirtualEObject, then it works.

One of the disavdantage of this approach is that implementing EObject/EPackage
has a lot of methods.  To filter classifiers according to the blacklist for
instance, there are at least two relevant methods:

- getEClassifiers()
- getEClassifier(String)

But, one could also obtain the list of classifiers through the reflective API of
eInvoke, eGet, etc.  That's a lot of holes to patch.

The EStore approach has only two methods: get and set.  Easier to make sure it's
correct.

Otherwise, there's the DynamicEObject approach.

** Virtualizing viewpoint: using EStore                            :emfviews:
Running into weird exception of impossible casts from EPackageImpl$2 to
EObject...

That's an anonymous inner class to EPackageImpl, but where is it coming from?

#+BEGIN_SRC java
 public EList<EClassifier> getEClassifiers()
  {
    if (eClassifiers == null)
    {
      eClassifiers =
        new EObjectContainmentWithInverseEList.Resolving<EClassifier>
          (EClassifier.class, this, EcorePackage.EPACKAGE__ECLASSIFIERS, EcorePackage.ECLASSIFIER__EPACKAGE)
        {
          private static final long serialVersionUID = 1L;

          @Override
          protected void didChange()
          {
            eNameToEClassifierMap = null;
          }
        };
    }
    return eClassifiers;
  }
#+END_SRC

It's this newEObjectContainment thingy.

Oh wait, it's returning a /list/ and the EStore.get should return a lone
element.  Yes, that works.

Now I think I really need to update the tests to use the reflective API.... but
it's really verbose.  Or there is another way: I could just write an utility
method that would test if a Viewpoint matches an expected format.  We would just
descend on the TreeIterator given by Viewpoint.getAllContents, without having to
lookup features/classifiers by name.

** Adapting the tests                                              :emfviews:
Oh hey, Viewpoint.getAllContents() returns an empty array.

* [2017-07-20 jeu.]
** Adapting the tests                                              :emfviews:
Update on yesterday: Viewpoint.getAllContents() returns what I need.  But I
don't know what's the best way to test it.

It's a tree iterator.  I could check it against a simple Iterator, but that
would be checking against an infix walk of the contents, so you lose some
information.

Also, there's the matter of /how/ to describe the expected structure.  Using
only names is too restrictive.  It seems I need to be able to check, for each
level, the value of some feature.  So I need a tree of "feature: value".

I wish I was in Lisp... making and iterating over a tree would be trivial.

Using the reflective API is verbose, even with shortcuts.  Before I wrote this:

#+BEGIN_SRC java
EClass A = (EClass) ((EPackage) l.get(0)).getEClassifier("A");
assertEquals(0, A.getEStructuralFeatures().size());

EClass B = (EClass) ((EPackage) l.get(1)).getEClassifier("B");
assertNotNull(B.getEStructuralFeature("b"));
#+END_SRC

Now I have to write this:

#+BEGIN_SRC java
EObject A = (EObject) ecall(l.get(0), EcorePackage.EPACKAGE___GET_ECLASSIFIER__STRING, "A");
assertEquals(0, ((List) eget(A, "eStructuralFeatures")).size());

EObject B = (EObject) ecall(l.get(1), EcorePackage.EPACKAGE___GET_ECLASSIFIER__STRING, "B");
assertNotNull(ecall(B, EcorePackage.ECLASS___GET_ESTRUCTURAL_FEATURE__STRING, "b"));
#+END_SRC

When all I really want to do is to test the resource against an expected result
of:

#+BEGIN_EXAMPLE
- minimalA
  - A
- minimalB
  - B
    - b
#+END_EXAMPLE

* [2017-07-21 ven.]
** Adapting the tests for the reflective API on Viewpoint          :emfviews:
I went with a few helper methods to reduce the boilerplate, and now the tests
use the reflective API /and/ are more readable.

The "check an expected structure" seemed to cumbersome.

Here is yesterday's example:

#+BEGIN_SRC java
EObject A = getClassifier(l.get(0), "A");
assertEquals(0, getFeatures(A).size());

EObject B = getClassifier(l.get(1), "B");
assertNotNull(getFeature(B, "b"));
#+END_SRC

It made other places much neater:

#+BEGIN_SRC diff
-    @SuppressWarnings("unchecked")
-    EList<EObject> ea_labels =
-        (EList<EObject>) ea.eGet(ea.eClass().getEStructuralFeature("labels"));
-    @SuppressWarnings("unchecked")
-    EList<EObject> vea_labels =
-        (EList<EObject>) vea.eGet(vea.eClass().getEStructuralFeature("labels"));
+    EList<EObject> ea_labels = eList(ea, "labels");
+    EList<EObject> vea_labels = eList(vea, "labels");
#+END_SRC

Looking at the whole diff, all changes are as long or shorter than the previous
code.  Crucially, we don't have way fewer casts.  So I guess it's a win.

Now, does it help with the EStore backing for Viewpoint?

** Using EStore implementations for Viewpoint contents             :emfviews:
So the tests do not magically all pass using this reflective API, unfortunately.

Most of them fail trying to cast Boolean to EObject... That's weird.

: getClassifier(l.get(0), "A")

Okay, so that's because getClassifier calls o.eInvoke on an operation that we
got from o.eClass.  On regular EMF classes, that works, but on our virtual
element, the eInvoke call never goes back to our EStore implementation and
returns ~eIsProxy~ in EObjectImpl instead.

Does that mean that eInvoke does not work on EViews?  If so, that would probably
be a nail in the coffin for this approach.

Hmm, there are no operations on EViews.. for objects of the minimalref model.
Maybe it's because there is no generated code for this metamodel?

What about TOGAF?  Still empty.  eClass() always seems to be an EClassImpl, so
eOperations is a feature, but it's empty.

Hmm, I was under the impression that some operations were generated for
getter/setters of attribute, but it seems it's not the case.  Operations are a
separate type of element in the metamodel.  So the Ecore metamodel has
operations, but TOGAF doesn't seem to have any.

Okay, so the question remains: does eInvoke works for ReproduceElements created
by EView?  I could load Ecore as a model and try it out...

The nsURI is: http://www.eclipse.org/emf/2002/Ecore

Hmm, that's interesting.  No errors when creating the EView, but the contents
are empty.

As far as I can tell, in View.loadContributingModels, we do
virtualResourceSet.getResource() and as a side-effect the resources attributes
of virtualResourceSet contains the contributing models resources.

However, when given a model as HTTP, it /finds/ something and returns an
EResourceFactory (instead of, say, an XMIResourceImpl), but the resources
attributes remains empty.

Since this resources attribute is used to build the contents of the View, the
contents will be empty.

Getting the returned value and explicitly adding the resource to the
virtualResourceSet seems to fix it.

Finally: I do get a ReproduceElement around an EPackage.  Calling
eClass().getEOperations on this reproduceElem gives me...

: [org.eclipse.emf.ecore.impl.EOperationImpl@56c0a61e (name: getEClassifier) (ordered: true, unique: true, lowerBound: 0, upperBound: 1)]

But, using my getClassifier helper gives me the same Boolean to EObject class
cast exception.

So: it doesn't work on the EView ReproduceElement either.

I should write a self-contained test with an EStoreEObject backed by an EStore
and try an eInvoke on it.  But my guess is that it doesn't work out of the box,
and I don't know yet if there is a way to make it work.

* [2017-07-24 lun.]
** Finding out if EStore supports eInvoke or not               :emfviews:emf:
If it doesn't, I'll have to check DynamicEObject, or write our own EObject
implementation.

Writing simply:

#+BEGIN_EXAMPLE java
    EStoreEObjectImpl o = new EStoreEObjectImpl();
    o.eInvoke(EcorePackage.Literals.EPACKAGE___GET_ECLASSIFIER__STRING,
              ECollections.asEList("Foo"));
#+END_EXAMPLE

(I found a new way to get the EOperation)

Violates an assertion in BasicEObjectImpl: "The operation 'getEClassifier' is
not a valid operation".

Setting the class:

#+BEGIN_EXAMPLE java
    EStoreEObjectImpl o = new EStoreEObjectImpl();
    o.eSetClass(EcoreFactory.eINSTANCE.createEPackage().eClass());
    o.eInvoke(EcorePackage.Literals.EPACKAGE___GET_ECLASSIFIER__STRING,
              ECollections.asEList("Foo"));
#+END_EXAMPLE

returns ~false~.  A boolean.  Probably eIsProxy again.

After debugging, yes, we end up in EStoreEObjectImpl.eInvoke with the
operationID of 1, and it happens to be the ID of ~EOBJECT___EIS_PROXY~, so we
call that method and return false.

Am I using this wrong?  It looks like using eInvoke on EStoreEObject is used to
call /EObject operations/.  And I want it to /delegate/ eInvoke on a backing
object.  But EStoreEObject has no notion of a backing object.

It /does/, however, delegate attributes get and set to the backing store.

Using eGet on the EStoreEObject ends up calling dynamicGet(featureID), which
delegates to eStore().get().

Hmm.  But we could extend EStoreEObjectImpl to delegate eInvoke to the backing
store... but we would have to extend the store interface as well.  And,
ultimately, it seems that extending EStoreEObject is not the proper solution: an
EStoreEObject /delegates/ to a an EStore, but a VirtualElement aims to be
/transparent/.

Is DynamicEObject a better fit?

** Exploring DynamicEObject                                    :emfviews:emf:
Doing:

#+BEGIN_SRC java
DynamicEObjectImpl o = new DynamicEObjectImpl();
o.eSetClass(EcoreFactory.eINSTANCE.createEPackage().eClass());
o.eInvoke(EcorePackage.Literals.EPACKAGE___GET_ECLASSIFIER__STRING,
          ECollections.asEList("Foo"));
#+END_SRC

throws UnsupportedOperationEx "eInvoke not implemented for getEClassifier"

Promising!  We end up in BasicInvocationDelegate.dynamicInvoke, which will only
invoke the given operations if it is an EObject operation:

#+BEGIN_SRC java
 if (eOperation.getEContainingClass() == EcorePackage.Literals.EOBJECT)
 {
   switch (eOperation.getEContainingClass().getEAllOperations().indexOf(eOperation))
   {
     case EcorePackage.EOBJECT___ECLASS:
       return target.eClass();
     case EcorePackage.EOBJECT___EIS_PROXY:
       return target.eIsProxy();
 ...
#+END_SRC

Otherwise, it throws.

So it looks like we need to define an InvocationDelegate that does something
else.

An invocation delegate is a property /of an operation/.  It's a class that tells
the operation how it should execute itself given the arguments.  So we go from:

: obj.eInvoke(operation, args)

to:

#+BEGIN_EXAMPLE
eInvocationDelegate(operation).dynamicInvoke(obj, args)
BasicInvocationDelegate.dynamicInvoke(obj, args)
switch (operation id)
  ECLASS => obj.eClass()
  ECONTAINER => obj.eContainer()
  ...
#+END_EXAMPLE

It's a bit convoluted.

Invocation delegates are created through annotations... or we can set them
directly on the operation.

But I don't want to set them per operation.  I just want to delegate the eInvoke
call to a concrete object, which will know how to handle it.

So it looks like we'll have to build our own EObject implementation.  However,
DynamicEObject can serve as a reference of what is actually useful in there.
There is very little actual logic in that class, so it should serve as a guide.

I think we could have a DelegatingEObject class that just delegates to a
concrete element.  Then we could extend this Delegating class as VirtualEObject
which adds additional control related to views (potentially filtered, etc.).

* [2017-07-25 mar.]
** Using a DelegateEObject for virtualizing Viewpoint              :emfviews:
Have to make some adjustments.

We used a ResourceSet to hold the cloned and modified packages, and got them
back through registry.getEPackage.  However, getEPackage returns null when the
objects we put there are not instances of EPackage.

When using a DelegateEObject instead of cloning the EPackages, this cannot
work.  Instead, a simple Map from the package URI to the package object
(actually, a delegate) works.

Now, most of the tests pass; there is the issue that applyFilters just deletes
stuff from the original packages through the delegate.  We can't do that
anymore.

Instead, we should /hide/ elements.  Having a VirtualEObject class that extends
DelegateEObject and has a ~filtered~ field would work.  However, at the moment
we use DelegateEObject only at the package level, but we should make sure that
every object inside this delegated package is also delegated.

One way would be to iterate on the contents of each EPackage at first, and wrap
them in DelegateEObject.  Another would be to do it lazily at runtime, using
membranes.

Let's try to do it at construction time.

Hmm, I need the original structure of the package to guide me.  What if cloned
the packages, but then replaced every object inside by a delegate to itself?

ArrayStoreException

Argh, seems I can't just store anything I want here.  Okay, fine, let's go with
the membrane approach then.

First hole to plug is eContents: this gives access to non-Delegate object, and
we should change that.

Defining a MembraneEObject (for lack of a better name), that constructs a
MembraneEList and wraps objects in the get(index) method.

Objects are saved in a Map to avoid creating duplicate wrappers (and to help
preserve reference equalify).

Problem: calling EcoreUtil.delete on a MembraneEObject doesn't work, because
they are casted as InternalEObject.

Well, good thing we don't need actually want to delete these objects then.  But
wait, filters don't work, that I understand, but creating new elements should
work.

Hmm, that's because we are casting to EClass in order to be able to access and
modify getESuperTypes.  Can't we use the reflective API?

: ((EList<EClass>) sub.eGet(EcorePackage.Literals.ECLASS__ESUPER_TYPES)).add(klass);

It's not pretty, but we can.

Hmm, wait.  The addSubConcept is more interesting.

#+BEGIN_SRC java
EObject sup = findEObject(e, registry);
if (!(sup instanceof EClass))
  throw EX("Superconcept '%s' of new concept '%s' should be an EClass", e, c.getName());
klass.getESuperTypes().add((EClass) sup);
#+END_SRC

If sup is a MembraneObject, then we can't cast it to EClass obviously.  We could
use his delegate object... but should we?

It seems we have to, because we can't add anything other than an EClass a super
type.  MembraneEObject does not implements EClass, and if we start duplicating
Membranes to include MembraneEClass, MembreEReference, etc., then we are not
sharing code with the View level anymore.

What happens if we use the delegate?  It means we mix virtual classes and
original classes.  And it adds a way to get to original classes from synthetic
elements in the virtual package.

Hmm I think I see a solution:

     delegates to
 D_1 ------------> C_1
                   ^
                   |
 D_2 ------------> C_2

C_1 is the original concept, C_2 is a new subconcept.  Since we deal with
delegates, we can't subclass D_1 with C_2 directly.  However, we can subclass C_1
with C_2, and then expose a delegate D_2 to C_2.

That also solves the question of whether to expose synthetic elements directly
or through delegates.

* [2017-07-26 mer.]
** On yesterday's solution to the subconcept problem               :emfviews:
There's one downside: if the inheritance link is added to the concrete elements,
then we are modifying the existing model, rather than modifying the view.

The alternative is then to create the inheritance link between the delegates:

     delegates to
 D_1 ------------> C_1
 ^
 |
 D_2 ------------> C_2

But then, the delegates need to hold extra information.  Basically, we need to
have a VirtualClass object that implements EClass.

Trying the VirtualClass/VirtualProperty approach.  First VirtualProperty:

#+BEGIN_SRC java
public class VirtualEAttribute implements EAttribute {

  protected VirtualProperty virtualProperty;
  protected EAttribute backing;
#+END_SRC

It implements EAttribute to be transparent for EMF.  But it holds the
VirtualProperty instance it is supposed to represent.  The backing EAttribute is
used to hold all the relevant EAttribute fields, rather than duplicating them.

But now, this object cannot be cast to InternalEObject when doing:

: parentClass.getEStructuralFeatures().add(attr);

Well, we don't want to do that anymore anyway.  The VirtualEAttribute already
has a parent that is held by its VirtualProperty.  The more interesting question
is what happens when we interrogate our VirtualEAttribute?

We fail at:

#+BEGIN_SRC java
   EObject f = getFeature(A, "newProperty");
   assertNotNull(f);
#+END_SRC

~f~ is null, because we can't find the feature on A.  So in order to make this
work, we have to implement VirtualEClass as well, and create these.

Similar problem with VirtualEClass:

: virtualPackage.getEClassifiers().add(klass);

Cannot cast to InternalEObject.  Seems EMF doesn't want us to mix our homebrewed
virtual EObjects with regular EPackage.

Have to implement VirtualEPackage then.

Or maybe we can work around this restriction, since the only thing that we do
with our new concepts is to expose them in our VirtualContents.  If instead of
providing a package, we provide a list EClass, it could work.

* [2017-08-01 mar.]
** Picking up on VirtualEClass                                     :emfviews:
VirtualEClass.getName() does return the name of the backing virtual concept or
EClass, but toString doesn't.

Oh, that's why.  Here is ENamedElementImpl:

#+BEGIN_SRC java
public String toString() {
  if (eIsProxy()) return super.toString();

  StringBuffer result = new StringBuffer(super.toString());
  result.append(" (name: ");
  result.append(name);
  result.append(')');
  return result.toString();
}
#+END_SRC

It uses the name attribute directly, instead of using the getter.  If we want
toString to be transparent, we have to change the attribute as well then.

But that's again an issue: are there other such attributes that are used
internally without going through the accessors?

I'm dreading what happens on equals...

Hmm, okay, looks like we end up in Object.equals, which uses reference
equality.  I was wondering whether Object.equals was an equivalence, and the
documentation says it is.  Good.

Now, do we need to implement equals on VirtualEClass?

Hmm, even if we did, it wouldn't be symmetric:

: x.equals(y)

would work when X is a VirtualEClass, but not when X is an EClass and Y is
virtual, because X would juste reference equality.

So I guess this is a nail in the coffin of the "completely transparent"
objective.  Hmm.  We could always tell a virtual EClass from its backing class
using object reference directly though.  So I guess what's still missing is a
definition of what "transparent" means here.

In any case, maybe the tests are the one that need to change here.

Hmm, actually, the issue here is that we test:

: assertEquals(getClassifier(l.get(0), "A"), sups.get(0));

In words: the super type "A" of our new virtual concept "C" should be equal to
the class "A" we obtain from the first contributing package in the viewpoint.

Given the equals implementation on EClass, equality here means that they should
be /the same object/.

I think the test is correct in expecting both to be the same objects.  It's
related to issue #1: What happens to classes of a contributing package that are
not referenced.

But here the case is pretty clear I think: the virtual C references the concrete
A, so we must lift A into the virtual world.  That the test fail is a leak in our
virtual world abstraction.

So, that means that the virtual contents should lift everything it contains.
How do we do that?  That's issue #5: how to make sure that the virtual contents
are always virtual objects?

To start with, I'll wrap all the EClass of the contributing packages into a
VirtualEClass.

Now all the addConcept tests pass.  But surely, it does not stop here as we only
delegate the /name/ of the EClass in our VirtualEClass, and nothing else.

* [2017-08-02 mer.]
** New plan for the virtual metamodel                              :emfviews:
First, try to see if we can trick EMF into taking VirtualEClass by implementing
InternalEObject.  Maybe it's feasible, maybe it's a hassle.  Maybe we have to
add so many attributes that are too similar to the reference EClassImpl
implementation that it makes little sense.

The main benefit of implementing InternalEObject would be to be lightweight by
having less attributes than an EClass.  If we end up with too many attributes,
then it's probably not worth it.

** Different ways of handling filters                              :emfviews:
There are two ways of using the filters in the weaving model.

First, we can use them statically, at the creation of the virtual metamodel: we
create only the VirtualEClass that are not filtered.  This is equivalent with
what we have now.

Or, we use the filters dynamically.  VirtualEClass (or rather, VirtualEObject)
has a list of filters to apply, and whenever we ask for the contents of
VirtualEClass, we only return the ones that are non-filtered.

Again, there are two ways of doing that.

We could populate the VirtualEClass, at creation time, with proxies for all its
potential contents.  When asked for its contents, it will just return those that
are not filtered out.

Or, we could be fully lazy, and just keep a list of created proxies so far.
Whenever we get asked for the contents, we populate a list with the proxies of
unfiltered classifiers, and if a proxy does not exist, we create it on the fly.

* [2017-08-03 jeu.]
** Trying out implementing InternalEObject                         :emfviews:
Doesn't seem to make much sense.  Right now it's asking me for:

: public NotificationChain eInverseAdd(InternalEObject otherEnd, int featureID, Class<?> baseClass,
                                       NotificationChain notifications) {

To be implemented.  But, first it really looks like it deals with internal
matters of EMF, so it might be a very bad idea.  Second, I actually have no clue
on how to implement that.

However, maybe we can inherit from BasicEObjectImpl instead of EClass, and
implement EClass on top of that.

** Extending BasicEObjectImpl                                      :emfviews:
Haha.  I get an UnsupportedOpEx, not in my VirtualEClass, but in
BasicEObjectImpl itself:

#+BEGIN_SRC java
protected EPropertiesHolder eBasicProperties()
{
  throw new UnsupportedOperationException();
  // return eProperties;
}
#+END_SRC

Even though BasicEObjectImpl is not abstract, it's not a class you can use
without overriding methods.  Trying this:

#+BEGIN_SRC java
EPropertiesHolder eProperties;

@Override
protected EPropertiesHolder eProperties() {
  if (eProperties == null) {
    eProperties = new EPropertiesHolderImpl();
  }
  return eProperties;
}

@Override
protected EPropertiesHolder eBasicProperties() {
  return eProperties;
}
#+END_SRC

Doesn't work, because the EPropertiesHolderImpl constructor is not visible, even
though the class is protected.

Hmm.  Protected should be available to subclasses, but for some reason it
doesn't work here.  Maybe the /constructor/, being implicit, has the default
visibility.

Hmm, doing:

#+BEGIN_SRC java
public class TestVisibility {
  protected static class A {
    protected A() {}
  }
}
#+END_SRC

and:

#+BEGIN_SRC java
public class Foo extends TestVisibility {
  public void foo() {
    A a = new A();
  }
}
#+END_SRC

in another package, still doesn't work.  Changing the constructor A to public is
the only way to make it accessible.

Is it that way for non-inner classes as well?

#+BEGIN_SRC java
public class TestVisibility {
  protected TestVisibility() {}
}
#+END_SRC

#+BEGIN_SRC java
public class Foo extends TestVisibility {
  public Foo() {
    super();
  }
  public static void main() {
    TestVisibility t = new TestVisibility();
  }
}

#+END_SRC

Here, the explicit call to ~new TestVisibility()~ is failing because the
constructor is not visible.  But the call to ~super~ is okay, even though we use
the same constructor: because the constructor is available to subclasses.

If however we remove the ~protected~ on the TestVisibility constructor, then the
~super~ call is illegal.

Okay, coherent.

Now, for the inner class, ~protected~ makes the constructor only available to
subclasses of /that inner class/, and not to subclasses of its parent class.

And since I don't see any constructor, I'm guessing a ~protected~ inner class
has a default constructor with ~protected~ visibility.

One trick would then be to extend the inner class, just to be able to
instantiate it:

#+BEGIN_SRC java
public class Foo extends TestVisibility {
  protected static class MyA extends A {}
  public void foo() {
    A a = new MyA();
  }
#+END_SRC

And this works.  It's a tad stupid looking, because it's just saying: "ok,
please let me construct this class", and usually the visibility is controlled by
the class that defines it, not the class that uses it.  Meh.

Found two relevant SO threads:
- https://stackoverflow.com/q/11605388
- https://stackoverflow.com/a/17610709

In both, the accepted answer is to make the inner class constructor public.  But
what if it's not code you can modify?  In the second link, someone suggests to
extend the class like I did here.

Let's try that on BasicEObject then.  Yes, I can construct MyEPropertiesHolder.

Now it doesn't work yet because... I need to implement getStaticClass().

Okay, done.

Now, another method that is unsupported but not abstract:

#+BEGIN_SRC java
public InternalEObject eInternalContainer()
{
  throw new UnsupportedOperationException();
  //return eContainer;
}

public int eContainerFeatureID()
{
  throw new UnsupportedOperationException();
  // return eContainerFeatureID;
}
#+END_SRC

I do notice that at some point it was implemented, maybe?

At this point, it might make more sense to directly extend EObjectImpl, since
I'm lifting code from it anyway.

** Extending EObjectImpl                                           :emfviews:
Oh, wow.  The addConcept test passes, all the other stack overflow.

For the first test addProperty, I've isolated the stack overflow to
buildNewProperties.findEObject.  Then in EMFViews.findElement.

Then in this:

: return dynamicFeatureID < 0 ?
      eGet(eFeature, resolve) /* backward compatibility with old generated overrides */

And that's where we loop, because that ~eGet~ will call the ~eGet~ on the same
BasicEObjectImpl.

So it looks like we have to override eGet to get out of this loop.

Or, that comment seems to indicate that maybe we shouldn't go through this
branch.

Hmm, implementing eGet is not enough, as it looks like we need eIsSet as well.
And probably eSet and eUnset.  Crapshoot.

Or maybe we shouldn't pass as /the/ EClass class in eStaticClass.

Trying to override eStaticFeature count to 0 so always go through the dynamicGet
method... we then go through eSettings[featureID] instead, but this array is
full of nulls.

When is it populated?  Looks like neither BasicEObject or EObject populates it,
so... no wonder it's empty.

Out of curiosity, extending MinimalEObjectImpl instead of EObjectImpl makes no
difference.  From the docstring, the Minimal variant is "space-compact", but it
doesn't say if its inner workings are different.

Let's try the eGet route one more time.

Copying the eGet/eIsSet methods of EClassImpl.  Get a bunch of errors because
these sometimes refer to attributes that exist in EClassImpl, not merely
accessors.

Replacing with UnspportedOpEx, we end up in:

#+BEGIN_SRC java
  public boolean eIsSet(int featureID) {
    switch (featureID) {
    case EcorePackage.ECLASS__EANNOTATIONS:
#+END_SRC

Replacing with ~return false~ then (let's say VirtualEClass does not support
Annotations currently).

Same for ~ETYPE_PARAMETERS~.  EOPERATIONS.  ~ESTRUCTURAL_FEATURES~.
~EGENERIC_SUPER_TYPES~.

Then, overriding getESuperTypes like this:

#+BEGIN_SRC java
@Override
public EList<EClass> getESuperTypes() {
  if (virtualConcept != null) {
    List<EClass> l = new ArrayList<>();
    for (Concept c : virtualConcept.getSuperConcepts()) {
      l.add((EClass) viewpoint.findEObject(c));
    }
    return ECollections.asEList(l);
  } else {
    return eClass.getESuperTypes();
  }
}
#+END_SRC

makes the addSubConcept test pass.

However, the addSuperConcept has a worrying ClassCastException.  We'll see
tomorrow.

* [2017-08-04 ven.]
** ESuperAdapter class cast exception                              :emfviews:
Okay, so EClassImpl actually implements ESuperAdapter.Holder in addition to
EClass.

That's getting tiring.  There's only two methods in there, but it's still yet
other code that has no bearing on VirtualEClass.

I feel I've exhausted this path at this point.

I'll go back to extending EClassImpl.

But before that, I want to check whether it's possible to use the reflective API
of EMF in Viewpoint.

** Using the reflective API in Viewpoint                           :emfviews:
Since I rewrote the tests using the reflective API to avoid casting, there might
also be a way to rewrite the code in Viewpoint the same way.

And then, maybe we don't even have to extend EClass, but just use EStore, maybe?

I think I just need to test whether you can add an EObject as super type through
the reflective API to start with.

When trying this:

#+BEGIN_SRC java
EObject C = EcoreFactory.eINSTANCE.createEObject();
EList<EObject> sups = (EList<EObject>) A.eGet(EcorePackage.Literals.ECLASS__ESUPER_TYPES);
sups.add(C);
#+END_SRC

: ArrayStoreException

That's because the feature ESuperTypes is typed to contain only EClass, and even
though the reflective API allows you to send any EObject, there is still a
runtime check for the type.  In DelegatingEcoreList.validate:

#+BEGIN_SRC java
if (object != null && !isInstance(object))
{
  throw new ArrayStoreException();
}
#+END_SRC

And isInstance ends up doing:

: getEStructuralFeature().getEType().isInstance(object);

So, no dice.

Back to extending EClass then.

** Extending EClassImpl                                            :emfviews:
VirtualEPackage, VirtualEClass, VirtualEAttribute, ...

If I turn of the Ecore validation, all tests pass except the
filterBidirectionalReference.

We can still access the parentA feature through eOpposite.  I'm guessing now
that we partly copy stuff and partly proxy them, we have some dangling links.

For instance here, I have parentA that's a feature in the ProxyEClass for B, but
its eOpposite field is the manyB feature in the concrete class A.

So we have mixed concrete and virtual elements.

** Summary of the different solutions to make the metamodel virtual :emfviews:
Note that we are really talking about the /metamodel/ level here, and not the
model level.  The model level is already virtual with ReproduceElement and other
elements, so for instance updates on the contributing models are propagated to
the view without any difficulty.

So, we wanted to make the metamodel (Viewpoint) virtual in the same way.

*** Previous behavior: cloning everything
Previously, for the metamodel level was simply cloned each of the contributing
metamodels into a "virtual" resource set.

After that, we could simply delete the cloned elements (classes/features) that
were filtered out.  Then add any virtual elements
(concepts/properties/associations) to these cloned and filtered packages.

What are the downsides with the cloning approach?  Mainly, that it's not
virtual, so we lose the ability to react to changes to the contributing
metamodels.  Also, it's static: the contributing metamodels are cloned at the
construction of the Viewpoint, so we cannot adapt to change in the weaving model
either.  There is a slight memory hit as well, as ideally the metamodel level
would contain only lightweight proxies to the contributing elements of the
metamodel, and not full EPackage/EClass/EAttribute instances.  Finally, it would
have been neater to have some symmetry between the metamodel level and the model
level, as it was announced in the EMFViews paper.

So, now what are our options for making the metamodel level virtual?

*** Using EStore
The first one is using EStore, as this is what is used at the model level.  At
the model level, our proxies are EStoreEObjectImpl objects, that use singleton
EStore instances (e.g., ReproduceRule) to capture eGet/eSet calls.

Why can't we use EStore at the metamodel level?  In the viewpoint, we can add
new properties or new concepts.  To add a property, we create an EAttribute and
we add it to a parent concept using:

: concept.getEStructuralFeatures().add(attr)

But in order for that to work, ~concept~ must be an ~EClass~, and ~attr~ must be
an EStructuralFeature.  An EStoreEObject is neither of those.

Can't we subclass EStoreEObject and implement EClass?  Well, first of all,
implementing EClass is far from trivial.  There are many methods, and it's not
all clear how we would define them, apart from lifting code from EClassImpl.
But more importantly, EClassImpl implements two additional, /internal/
interfaces InternalEObject and ESuperAdapter.Holder.  If we don't implement
these, our EStoreEObject won't be able to pass off as a regular EClass and we'll
get a ClassCastException.  Again, implementing InternalEObject and
ESuperAdapter.Holder could be done in theory, but in practice I have no idea
what to put in these methods that is not already defined by EClassImpl itself.

So, it seems the only practical solution is to reuse an implementation like
EClassImpl and to override the right methods.

However, we could also reuse a more generic implementation, like
BasicEObjectImpl and work from there.

*** Extending BasicEObjectImpl
BasicEObjectImpl is nearly at the top of the hierarchy of Ecore.  We could
create:

: VirtualEObject extends BasicEObjectImpl

Then:

: VirtualEClass extends VirtualEObject implements EClass

This way we would be able to pass off as a proper EObject with internal
interfaces, satisying EMF, but we would still neatly implement only interfaces,
in order to have less coupling with EMF in the subclasses of VirtualEObject.

Why doesn't this work?  Because BasicEObjectImlp, as the name might have
implied, is not complete: it has methods that throw UnsupporteOperationEx.  Even
though the class is not abstract, you cannot use instances of it without
overriding some methods like eProperties, eBasicProperties, eContainer,
eInternalContainer...

We can go down in the hierarchy and try EObjectImpl.

*** Extending EObjectImpl
EObjectImpl basically overrides the methods that BasicEObjectImpl left throwing
UnsupportedOperationEx, and not much else.

Why doesn't this work?  Because EClassImpl has an additional internal interface,
ESuperAdapter.Holder, so we face the same issue as with InternalEObject.  A
VirtualEClass, being an EObjectImpl, cannot be cast as yet another internal
interface.  Though, ESuperAdapter.Holder contains only two methods, but one of
them is to return a new ESuperAdapter, and again I have no idea how to create
one other than lifting the code from EClassImpl.

So, again, we end up with extending EClassImpl as being the only practical
solution for a VirtualEClass.

*** Foreseen challenges in extending EClassImpl
So, let's see where are at: we wanted lightweight proxies by avoiding the
creation of a full EObject/EClass/EAttribute as that was the case in the cloning
approach.  But now, we are proposing to have VirtualEClass, VirtualEAttribute,
etc., and each one would /be/ an instance of EClass or EAttribute.  So we
wouldn't be any more lightweight in this case.

Also, the step after the virtualization of the metamodel is to use the same
architecture for the model level, unifying the two.  For that, a VirtualEObject
seems indicated.  If we had VirtualEClass as a subclass of VirtualEObject, then
the whole thing would have make much sense.  But we can't have that if
VirtualEClass is already a subclass of EClassImpl.  That's a bummer.

However, we would be able to be more dynamic, so it's still a gain in this
respect.

The main challenges of this approach is that, while cloning was rather
straightforward, now we have proxies to the elements of the contributing
metamodels, and we have to make sure the assembled virtual metamodel is
coherent, with respect to the many pointers between bidirectional references,
containers, and the like.

It's also somewhat harder to be sure that we don't modify the contributing
metamodels.  In the cloning case, well we had full clones, so we trust EMF to do
the right thing.  But here, our proxies have to use the data from the underlying
metamodel elements, without giving direct access to them or accidentally
modifying them.

*** An unexplored solution with EStore
Writing this, I realize there's a trail I have left unexplored.

I've said that EStore was insufficient, because we couldn't cast an
EStoreEObject to an EClass.  But that restriction stems from the fact that we
are /populating/ the virtual metamodel with actual instances of
EClass/EAttribute etc.

It seems to me that, at least in theory, we could make the virtual layer lazy,
and "just" intercept interesting features through eGet/eSet in EStore, in order
to make the relationships described by the weaving model "appear" real to the
callers, even though the instances would be short-lived.

Or, we can memoize the instances.  Shouldn't make much of a difference.

* [2017-08-21 lun.]
** Exploring the alternative EStore solution                       :emfviews:
As a way to refresh my mind on how this all works.

I think using EStore for proxifying contributing classes should work fine.  At
least at the top level, for exposing packages.  But then, I guess it's not
sufficient if we want to make sure that the contents of this virtual package are
also virtual objects.

Let's see.

Trying to use ReproduceElementImpl directly as my EStore backing to see how if
it can be used directly on Viewpoint as-is.  Looks like no, because
ReproduceRule will cast the resource of the element to a View in order to get
the MetamodelManager and access to translateToVirtualElement.

Let's try to implement EStore ourselves.

Implementing EStore.get like this:

#+BEGIN_SRC java
public Object get(InternalEObject object, EStructuralFeature feature, int index) {
  Object value = concreteElement.eGet(feature);
  if (index != NO_INDEX) {
    List<Object> list = (List<Object>) value;
    return list.get(index);
  } else {
    return value;
  }
}
#+END_SRC

is enough to fool eGet(o, "name").  But then I run into [[*Using%20EStore%20implementations%20for%20Viewpoint%20contents][the same issue I had
with getClassifier]], namely that calling eInvoke on EStoreEObject does not
delegate to the EStore, like eGet does.

Okay, getClassifiers() works because it returns the eClassifiers /feature/, so
that is delegated to the EStore.  But getClassifier(String) actually looks up
the classifier by name using the getEClassifier(String) operation on the
EPackage class, through eInvoke.  The latter is not delegated to the EStore and
thus calls the wrong method.

Well, if we reimplement getClassifier(String) to not use eInvoke, we should be
good.

Yep, that's enough.

So it looks like this could work.  The major hurdle I encountered previously
wasn't really one.  I was confused, and did not think of using the reflective
API to its full extent.

The main difference with extending EClassImpl is that we are not an EClass, but
merely an EObject.  Consumers would be restricted to use the reflective API to
list classifiers etc., whereas they could just cast to
EPackage/EClass/EAttribute in the other solution.

Both solutions have the same challenges: making sure that elements returned by
our proxies are also proxified.  Although, in the EStore case, we have less
methods to worry about (many of the EStore methods are defined in
TranslationRule in terms of a core set including ~get~, ~isSet~...).

Ultimately, the EStore solution has the advantage of being much simpler /and/
would allow us to share most of the code between the model and metamodel levels.

Tomorrow: see how to add a virtual concept.

* [2017-08-22 mar.]
** Virtualizing all objects                                        :emfviews:
I'm not /sure/ it virtualizes everything, but I based the logic off
ReproduceRule, and it's a start.

The tricky bit is that EStore.get can return EObject as well as basic Java
objects (depending on the feature).  And it also can return a whole list, or an
individual element, depending on the index argument.  All these cases are munged
together in this method.

And when we return a list, we have to make sure that accessing an element from
it doesn't leak a non-virtual object.

Adding a virtual concept works the same, for the time being, since virtual
concepts are put into a new virtual package (that is actually not virtual at
all, it's just an EPackage instance).

Adding a property, as well as filtering, modifies the original metamodel.  The
reason for that is that's we actually do in Viewpoint right now: we create a new
EAttribute and add it to the /original/ classes.  Then in getContents we return
a proxy to this class, but it's already modified.

What we should do instead is, when we list the features of A, we answer with a
list made of its concrete features /plus/ its virtual features.  We can
intercept that in EStore.get.

Yep, that works.  And this may hint at a specifity of the metamodel
virtualization: we have to add specific logic for features of EcorePackage if we
want to disguise our virtual objects as EPackage, EClass, and so on.  Whereas,
at the model layer, we don't need this special handling code.

In fact, it might make more sense to have this logic be in separate class.
Singletons, since we only need one instance of each type
(EPackage/EClass/EAttribute) to redefine the EStore methods.

Tomorrow: assocations and filtering.

* [2017-08-23 mer.]
** That Eclipse bug about the disappearing cursor                   :eclipse:
is really irritating.

I can reproduce reliably: hover on a warning, click on quickfix: invisible
caret.  I can still type and text will appear where the caret was, but not all
bindings work (like undo).

Moving the mouse to the window title bar restores the caret.

Tried in Unity: caret doesn't disappear.

Tried with compiling latest i3 (4.13): caret disappears.

Tried to get a log of i3, but didn't see anything.  Here is a timeline of events
that seem relevant:

#+BEGIN_EXAMPLE
23/08/2017 13:15:16 - focused now = 0xea2110 / foo.java - Eclipse Platform
...
23/08/2017 13:15:16 - x.c:x_push_changes:1116 - Updating focus (focused: 0xea2110 / foo.java - Eclipse Platform ) to X11 window 0x01c000ce
23/08/2017 13:15:16 - ipc.c:ipc_send_window_event:1279 - Issue IPC window focus event (con = 0xea2110, window = 0x01c000ce)
...
23/08/2017 13:15:18 - ClientMessage for window 0x01c0497f
23/08/2017 13:15:18 - handlers.c:handle_client_message:656 - Unknown atom in clientmessage of type 367
23/08/2017 13:15:19 - handlers.c:handle_event:1369 - event type 33, xkb_base 85
23/08/2017 13:15:19 - ClientMessage for window 0x01c0565d
23/08/2017 13:15:19 - handlers.c:handle_client_message:656 - Unknown atom in clientmessage of type 367
23/08/2017 13:15:19 - handlers.c:handle_event:1369 - event type 18, xkb_base 85
23/08/2017 13:15:19 - handlers.c:handle_unmap_notify_event:446 - UnmapNotify for 0x01c0497f (received from 0x000000fb), serial 3997
23/08/2017 13:15:19 - Not a managed window, ignoring UnmapNotify event
23/08/2017 13:15:19 - handlers.c:handle_event:1369 - event type 85, xkb_base 85
23/08/2017 13:15:19 - handlers.c:handle_event:1378 - xkb event, need to handle it.
23/08/2017 13:15:19 - handlers.c:handle_event:1404 - xkb state group = 0
23/08/2017 13:15:19 - handlers.c:handle_event:1369 - event type 10, xkb_base 85
23/08/2017 13:15:19 - handlers.c:handle_event:1369 - event type 28, xkb_base 85
23/08/2017 13:15:19 - handlers.c:handle_event:1369 - event type 85, xkb_base 85
23/08/2017 13:15:19 - handlers.c:handle_event:1378 - xkb event, need to handle it.
23/08/2017 13:15:19 - handlers.c:handle_event:1404 - xkb state group = 0
23/08/2017 13:15:19 - handlers.c:handle_event:1369 - event type 33, xkb_base 85
23/08/2017 13:15:19 - ClientMessage for window 0x01c000ce
23/08/2017 13:15:19 - handlers.c:handle_client_message:705 - _NET_ACTIVE_WINDOW: Window 0x01c000ce should be activated
23/08/2017 13:15:19 - workspace visible? fs = 0xea1f20, ws = 0xea1f20
23/08/2017 13:15:19 - handlers.c:handle_client_message:744 - Focusing con = 0xea2110
23/08/2017 13:15:19 - workspace.c:_workspace_show:384 - Not switching, already there.
23/08/2017 13:15:19 - con.c:con_focus:199 - con_focus = 0xea2110
23/08/2017 13:15:19 - con.c:con_focus:199 - con_focus = 0xea1f20
23/08/2017 13:15:19 - con.c:con_focus:199 - con_focus = 0xea1d50
23/08/2017 13:15:19 - con.c:con_focus:199 - con_focus = 0xea0ee0
23/08/2017 13:15:19 - con.c:con_focus:199 - con_focus = 0xe96270
#+END_EXAMPLE

Might try to ask on IRC when I have more time.

** Adding associations with EStore                                 :emfviews:
Memoizing the virtual objects created to ensure reference equality works is
enough to pass this test.

But now I need to do the same trick as for properties, and not modify the
original metamodels.

Interesting thing: when we create a new association, it's an EReference. It
needs an EType, and that must be an EClassifier, so we cannot wrap the argument
in a virtual object at this point.  However, we need to make sure that whenever
someone asks for eType on this EReference, it returns a VirtualEObject wrapping
the eType.

We can wrap the EReference itself for that, and VirtualEObject will take care of
mapping to the corresponding virtual object when asked for the eType.  So, in
the end we have:

1. EClass "B" containing feature 2
2. EReference "assoc" with EType 1
3. VirtualEObject wrapping 1
4. VirtualEObject wrapping 2

Asking for eType on 3 will return 2.

** Adding virtual concepts with EStore                             :emfviews:
Another interesting situation: new concepts are added to the virtual package.
New concepts are plain, unwrapped EClasses.  When we add C as a super type to A,
a contributing class, we don't touch the original A, but C will appear in the
eSuperTypes feature of the virtual object standing in for A.  However, C will be
wrapped automatically, and we will get the virtual object standing in for C, not
the C EClass.

That's a slight inconsistency.  It means we should probably wrap the synthetic
objects that are exposed in the virtual package.  We can do that when building
the virtual contents.

Other thing: virtual objects have to know about their potential (virtual) super
types.  The easiest way to do that is hold a list of them in VirtualEObject, and
populate it when building the viewpoint.  If we wanted to add a super type
dynamically, we would just add it to this list, and it would be reflected on the
next call to eSuperTypes.

Same thing with virtual features.

That leads to an interesting specialization: all virtual objects do not need
these lists of super types and features, only the virtual objects that stand in
for a virtual class do.  We can thus have a VirtualEClass class that handles
these extra concerns.

** Fixing filtering for EStore                                     :emfviews:
Tackling filters now... instead of deleting the metamodel objects from the
copied package, we should just do as is the filtered elements do not exist.  We
can add a ~filtered~ flag on VirtualEObject to keep track of filtered objects.
In VirtualEList, we just ignore filtered elements.

That makes the blacklist and whitelist tests pass, without having to modify the
Viewpoint code extensively.

Next time: the remaining failing tests are NullPointerExceptions due to the View
layer not liking our changes. Investigate.

* [2017-08-24 jeu.]
** Fixing the View layer                                           :emfviews:
NullPointerException happen in MetamodelManager.buildMaps, because
compositionClasses is empty.  The culprit is this cast:

#+BEGIN_SRC java
for (Iterator<EObject> i = viewpoint.getAllContents(); i.hasNext();) {
  EObject obj = i.next();
  if (obj instanceof EClass) {
#+END_SRC

since we now have VirtualEObject and not EClass.  I ought to rewrite this code
at some point anyway.  But right now, using the reflective API to test the
instance will do.

Argh, actually, most of the code here in there assumes we have EClass or
EStructuralFeature instances.  But now we have VirtualEObjects.  Should we only
deal with these objects using the reflective API, or should we bypass the
virtual layer and deal with the underlying concrete objects?

At some point, we /have/ to produce concrete objects.  For instance, in
ReproduceRule.getContainingFeature:

#+BEGIN_SRC java
public EStructuralFeature getContainingFeature(InternalEObject object) {
  View vModel = (View) object.eResource();
  EObject cElement = ((ReproduceElementImpl) object).getConcreteElement();
  EStructuralFeature vFeature = vModel.getMetamodelManager()
      .translateToVirtualFeature(object, cElement.eContainingFeature());

  return vFeature;
}
#+END_SRC

We have to return an EStructuralFeature, so we cannot return a VirtualEObject.
We have no choice but returning the concrete feature behind the virtual object.
But where does it go?  If it's used internally by EMF, it may cause confusion
since the virtual object will also be flying around.

Okay, maybe this is not really used.  Let's throw in this method for now and
we'll see about doing the correct thing later.

Other place where it matters, ReproduceElement.init:

#+BEGIN_SRC java
private void init(View vModel, EObject concreteElement, EClass eClass) {
  this.eProperties().setEResource(vModel);
  this.concreteElement = concreteElement;
  this.eSetClass(eClass);
#+END_SRC

Here we need to pass an EClass instance, but the metamodel has only virtual
objects.  Here we need to use the underlying concrete EClass.

It might not cause confusion if the concrete elements are used behind the
scenes, as long as the interface exposes virtual objects.

Grmbl, all this code is too tied up to the previous way of doing things.

I'll try a clean approach of rewriting EView with my own VirtualViewEObject to
see what's really needed.

Arg, still not working.  The crux of the issue seems to be that the virtual
objects at the view level need an EClass.  That EClass is used to know which
features the object possess.  When we request a virtual feature on a virtual
view object, EMF uses the EClass to compute the feature ID, then find out the
feature object, then ultimately will call EStore.get with the feature object.

So ideally, what should happen is: I request the virtual feature "assoc", it
exists on the metaclass of the virtual view object; in fact, it's a virtual
object wrapping an EReference.  Then, EMF should call EStore.get on the virtual
view object with that virtual feature object, and we could do the correct
mapping.

Instead, what happens is that EMF does not find the feature "assoc" on the
metaclass of the virtual view object, since its EClass is the /concrete/ "A"
class from the contributing metamodels, and /not/ the virtual class containing
that virtual feature.

I tried to call eGet with the virtual feature directly:

#+BEGIN_SRC java
EObject virtualFeature = getFeature(v.viewpoint.getVirtual(A.eClass()), "assoc").get();
assertEquals(B, A.eGet((EStructuralFeature) ((VirtualEObject) virtualFeature).concreteElement));
#+END_SRC

but this fails because the feature is not passed to EStore.get directly, as EMF
first finds its ID by looking up the metaclass, again.

And as far as I can tell, there is no way to fool EMF, since eClass must return
an EClass.

If we have to return an EClass, I guess we could return one that delegates to
a VirtualEObject, so we can masquerade that object.  Although it would, in the
end, be very similar to extending EClassImpl directly to implement
VirtualEObjects that represent EClasses.  Except that, maybe that way we could
preserve a hierarchy of Virtual* objects, and have an EClass be a simple bridge
to fool EMF.

* [2017-08-28 lun.]
** Using an EClass bridge between the virtual model and metamodel  :emfviews:
So, the problem is: what is the eClass of a virtual view object?  If we put the
/concrete/ eClass there, the virtual view object won't have any virtual
features, features won't be filtered, etc.  Basically, it's useless, it's the
wrong answer.

We cannot put the virtual object that corresponds to the concrete class because
setEClass requires an EClass.  And eClass is not a feature, but an operation, so
we cannot use the reflective API to fool it.

So, I tried to pass a VirtualEClass object that contains a reference to the
virtual object representing the concrete eclass.  That works, of course.  And we
can now override the getEStructuralFeature() to something like this:

#+BEGIN_SRC java
public EStructuralFeature getEStructuralFeature(String name) {
  for (EObject c : virtualClass.getFeatures()) {
    if (name.equals(eGet(c, "name"))) {
      return (EStructuralFeature)c; // <- problem: the cast fails
    }
  }
  return null;
}
#+END_SRC

But now we have another problem: we have to return an EStructralFeature here,
but the features of the virtual object are VirtualEObjects, again.  That's
basically the same issue as with eClass(EClass).

We could again, create an EStructralFeature bridge.  But now it's trickier,
because I don't know what methods in that bridge would be called.  And, more
importantly, we would then return a bridge, and that bridge would /not be the
same/ object as the virtual object representing the feature.  It was already
hard to have to deal with virtual objects and concrete objects, but if we add
bridges to the mix... it becomes chaotic.

What would be great would be to have a hierarchy under EStoreEObject:
EStoreEClass, EStoreEReference, etc.  That way we could use these objects as
EClass/EStructuralFeature where EMF expects them, but they would also delegate
all their lookups to the EStore, so we could capture all relevant methods /and/
keep a clean hierarchy.

Ultimately, that's a lot like a DelegateEObject/DelegateEClass situation... it
seems at the metamodel level we have no choice but implement EClass/EReference
in order to mesh with the existing EMF infrastructure.

* [2017-08-29 mar.]
** Summarizing the roadblocks with EStore                          :emfviews:
I've used an EStore to represent the virtual metamodel objects.  That worked
until I had to bind these virtual metaobjects to the virtual model objects.  The
virtual model objects use an EStore, so it needs an eClass, and that eClass must
be an instance of EClass.  But the metaclass of that virtual model object is
represented by an EStoreEObjectImpl; we can't pass the EClass from the
contributing metamodel that EStore object contains, since it won't contain
virtual features, and won't have any filters, etc.

To solve this eClass problem, I've tried to use a dummy EClass to use as that
eClass value, which delegates to the virtual metamodel object.  But this is not
enough, because now I would also need to add bridges for structural features,
and probably all the objects from the virtual metamodel.

Now, maybe the problem stems from the fact that we are using EStoreEObject at
the view level, which refuses to work with a virtual metaclass.  If the view
layer used the reflexive API to access its metaclass, that would work fine
(albeit slowly).  We could implement an EObject, like a DelegateEObject or
whatever, to behave as such.  But ultimately, eClass: EClass is a method on the
EObject interface, so at some point we have to give a proper EClass back to EMF.
And once that EClass is out, we must take care of preserving referential
equality.

And, since it seems we /have/ to have an EClass, we might as well use it to
represent directly the virtual metaobject, rather than putting the logic
separately in an EStore.  It's better for referential equality, and just simpler
to deal with fewer classes and objects.

** Getting the goods into master                                   :emfviews:
Commits to cherry-pick into master from the EStore branch:
- 610db2b72869f86ea78573b87f819453bd214daf
- 25bfd11eefe73e9725b1448eb78f3b9f5629d033
- ad84a7604fd94c9810e492e8b9c5d7e38110d641
- 95fdff7067f55479b5a3596b33839ffa66984664
- 5523c1834c44fc2befd3c4e834e72d8647e90c4d
- bf9a7c56b489582f74e078077ff853eecc61aea0

- part of 59e3a237fb6632a72f38733f6306f950d6c01eb4

Discovered that I could jut log a specific branch (l o) in Magit, then do (A A) or
(A a) to either cherry pick the commit directly, or apply its changes to the
working tree, thus letting me select what I want from it.  Very convenient.

** Using DynamicEObject in lieu of EStore                          :emfviews:
I've looked previously at DynamicEObject when searching for a solution to the
invoke() problem.

DynamicEObject has a simpler implementation than EStoreEObject.  I wonder if I
can use it as a base.  Does it funnel all feature access to a single method like
EStoreEObject?  There's a ~dynamicGet(int feature)~.  Doing:

#+BEGIN_SRC java
EObject o = new DynamicEObjectImpl(EcorePackage.Literals.ECLASS) {
  @Override
  public Object dynamicGet(int featureID) {
    System.out.println("dynamicGet: " + featureID);
    return "foo";
  }
};
#+END_SRC

It is indeed called for features that all the features that exist on the
metaclass.  DynamicEObjectImpl has zero static features, that's how everything
is routed through the dynamic* methods.

So it looks like DynamicEObject could serve just as well as EStoreEObject.  In
fact, EStoreEObject does work that we end up duplicating in EStore.get:

#+BEGIN_SRC java
public Object dynamicGet(int dynamicFeatureID)
{
  Object result = eSettings[dynamicFeatureID];
  if (result == null)
  {
    EStructuralFeature eStructuralFeature = eDynamicFeature(dynamicFeatureID);
    if (!eStructuralFeature.isTransient())
    {
      if (FeatureMapUtil.isFeatureMap(eStructuralFeature))
      {
        eSettings[dynamicFeatureID] = result = createFeatureMap(eStructuralFeature);
      }
      else if (eStructuralFeature.isMany())
      {
        eSettings[dynamicFeatureID] = result = createList(eStructuralFeature);
      }
      else
      {
        result = eStore().get(this, eStructuralFeature, InternalEObject.EStore.NO_INDEX);
        if (eIsCaching())
        {
          eSettings[dynamicFeatureID] = result;
        }
      }
    }
  }
  return result;
}
#+END_SRC

Here it already dispatches to different code depending on the structural feature
type.  But ~createList~ creates a list that will end up calling back into
eStore.get.

EStoreEObject also caches values by default, which we do not want.

I believe DynamicEObject would be a better base.

However it does not solve the main issue: we need a DynamicEClass, and assorted
family of classes.  Say we use DynamicEObject.  It still needs an EClass as
metaclass, and we want to use a Dynamic object for that.  We can't have a
DynamicEClass that extends DynamicEObject and EClassImpl, because we don't have
multiple inheritance.

We have two solutions:
1. DynamicEClass must extend EClassImpl first, and then implement
   DynamicValueHolder to funnel all calls to dynamicGet.  If we can capture
   calls to eStructuralFeatures that way, it would greatly simplify the
   implementation.  Although, we pay the price of inheriting from the large
   EClassImpl, with many methods we will never use.

2. DnyamicEClass extends DynamicEObject and implements EClass and associated
   interfaces.  It's lighter, and it's the conceptually cleaner way.  But one
   issue is that we may have a bunch of interface methods which we have no clue
   how to fill.  That's especially true for associated interfaces, because "one
   does not simply implements EClass".

I'll try #2 first because it's the proper way; the additional control from
implementing all methods ourselves will also help in ensuring we do not leak any
non-virtual object.

** Virtual values in BasicEObjectImpl                          :emf:emfviews:
In BasicEObjectImpl, there is code relating to "virtual values".  It looks like
dead code, since the core methods to set and retrieve virtual values throw:

#+BEGIN_SRC java
protected Object[] eVirtualValues()
{
  // return eVirtualValues;
  throw new UnsupportedOperationException();
}

protected void eSetVirtualValues(Object[] newValues)
{
  // eVirtualValues = newValues;
  throw new UnsupportedOperationException();
}
#+END_SRC

Though the previous code is still commented out... I have no idea if this is
used anywhere, but basically, it looks like an array of Object that you could
set and get.

Nothing related to what EMFViews is trying to do; and nothing usable.  Red
herring.

* [2017-08-30 mer.]
** Trying to implement EClass for DynamicEClass                    :emfviews:
Wrote a simple standalone implementation for now, to avoid interactions with the
existing code.  I want to make sure we can link the model and metamodel layers
without any hiccups.

And here's the first one: "b" is a virtual feature on the virtual class VA, and I
create a virtual object VO with VA as its metaclass.  When I access "b" on VO, I
get a null pointer exception because the feature "b" (EAttribute in VA) has no
eContainingClass.

The thing is, eStructuralFeatures is a bidirectional reference in Ecore, with
eContainingClass as its opposite.  When you add stuff to eStructuralFeatures,
EMF makes sure to update the opposite as well.  But here, I'm holding the
virtual features in a separate (plain) list, and the eContainingClass opposite
is never updated.

I could update the opposite myself.  Or I could use a VirtualEAttribute and
override eContainingClass.

* [2017-09-01 ven.]
** Setting the bidirectional reference                             :emfviews:
I can't seem to set eContainingClass using eSet on the EAttribute.  Because
eContainingClass is not a proper feature of EAttribute, strangely.

What happens when we add a feature to getEStructuralFeatures on a basic EClass?

1. getEStructuralFeatures() returns an EObjectContainmentWithInverseEList
2. Then we call add() on that list, which proceeds into addUnique()
3. It does doAddUnique, then inverseAdd()
4. Which calls eInverseAdd() on the EAttribute(a) (the /added/ attribute), and
   passes the EClass A (where it is added) and the feature ID of the opposite
   end of the EReference.

   Getting the feature ID, it casts the structural feature into an EReference.
5. This ends up in inverseAdd on EStructuralFeatureImpl, which does:

   #+BEGIN_SRC java
   case EcorePackage.ESTRUCTURAL_FEATURE__ECONTAINING_CLASS:
     if (eInternalContainer() != null)
       msgs = eBasicRemoveFromContainer(msgs);
     return eBasicSetContainer(otherEnd, EcorePackage.ESTRUCTURAL_FEATURE__ECONTAINING_CLASS, msgs);
   #+END_SRC

   Which ultimately sets the container of the EAttribute to the EClass, and also
   sets the feature ID corresponding to the containment.

And when we add the virtual feature on the virtual class, none of that happens.

Why can't I add the virtual attribute to the actual structural features of the
virtual EClass?  Oh right, our VirtualEClass does not have an actual list of
structural features, it just implements the EClass interface, by answering to
the getEStructuralFeatures() method.  The only list of structural features that
exists is the one in the concrete EClass, and we certainly don't want to modify
that one.

But maybe we could lie about our containing class?

Ok, so I created VirtualEAttribute that implements EAttribute, but it also has
to implement EStructuralFeature.Internal, since BasicEObject needs the feature
to cast into that.

Then I need to return a SettingDelegate in getSettingDelegate... which is a
615-lines method in EStructuralFeature!  Argh.

* [2017-09-04 lun.]
** Stubbing ESetttingsDelegate                                     :emfviews:
Doing this:

#+BEGIN_SRC java
@Override
public SettingDelegate getSettingDelegate() {
  SettingDelegate.Factory f = EcoreUtil.getSettingDelegateFactory(this);
  return f.createSettingDelegate(this);
}
#+END_SRC

and EcoreUtil walks up the hierarchy and asks for the eContainingClass of the
VirtualEAttribute, and the EPackage of the VirtualEClass.

... and ultimately returns null.  Grmbl.  Okay so it was looking for existing
setting delegates, but it seems I have none.

And SettingDelegate is an interface.  When I instantiate a ~new
SettingsDelegate()~, its dynamicGet() method is called... this is a bit
confusing, since I already have a dynamicGet on VirtualEAttribute.

Debugging with G, we managed to make the example working.  The SettingDelegate
just calls back into the DynamicValueHolder... a bit dumb, but it works.

So now I can actually create a VirtualEObject with a VirtualEClass as metaclass,
and I can set/get virtual features on this VirtualEObject without affecting the
original object.

I'm not sure what happens if that feature is an EReference though.

But I guess I could start using these virtual objects for the Viewpoint and
EView now.

* [2017-09-05 mar.]
** Expanding on VirtualClass                                       :emfviews:
Okay so my plan would be to write a new suite of tests that create virtual
objects directly, rather than using Viewpoint.  This way I can make sure I have
basic functionality of adding properties, reference, etc.  Even testing that it
works at the model level.

Then, I can go back to Viewpoint and EView and integrate these changes.

Okay so I'm trying to test adding a virtual reference.  What should eGet return?
An EList, preferably one we can add and remove from.

I think I've got the basics, except filtering.

Just adding a filtered field on virtual objects should do it.  But then the list
of virtual features should take these filtered objects into account, like I did
in the EStore implementation.

* [2017-09-06 mer.]
** Filtering elements                                              :emfviews:
It stands to reason that all elements of the metamodels can be filtered.  I
don't know if it makes sense to filter a VirtualEObject directly, but it
wouldn't be difficult to do so.

If the "filtered" attribute is held by virtual elements, then to be able to say
that a feature "a" on class "A" is filtered, we need to set the attribute on the
virtual element that wraps this feature.  So when asking for a feature on a
virtual class, it should wrap its contents.

Moreover, when asking for the list of features, we want the virtual class to
report only the non-filtered features.  For that to work, the virtual class
needs to be able to interrogate the filtered status; it can do that if it holds
the virtual elements that wrap the concrete features directly.

There are at least two ways to implement that, and I'm not sure which is better
for our purpose.  The first one would be to have VirtualEClass simply hold a
list of structural features, and when constructing the VirtualEClass, we would
wrap features before adding them to the class.  That way, VirtualEClass does not
need to know which features map to concrete objects and which are purely
virtual.  And it can report the non-filtered features, since every feature has a
"filtered" attribute.

The second way to implement it would be to keep a reference to the concrete
EClass and a list of virtual features, but when we report the list of features,
we wrap it in a virtual one.  And we keep track of the mapping from concrete
features to virtual ones.  That way, we can still report non-filtered features,
but we only wrap objects on demand.  It can also allow us to respond to changes
in the contributing metamodels, though we don't know if that would often be
useful in practice.

So, if we go the first route, then I would have trouble discerning that from the
previous state of affairs: we would construct the Viewpoint in the same way, but
wrapping everything instead of copying.

The second route is more lazy, so closer to the "lightweight proxies" idea.
Besides, the point is to have a symmetry between model and metamodel, and
wrapping objects on the fly is what already happens at the model layer, so...
Second route it is.

Okay so it works now.  I created a VirtualFeature object to hold the filtered
attribute to start with.  I'm sure I can lift that higher, as classifiers should
be able to be filtered from a package as well.

Haha!  Interesting complication:

#+BEGIN_SRC java
EObject o = EcoreUtil.create(A);
o.eSet(a, 1);
o.eSet(a2, 2);
VirtualEObject Vo = new VirtualEObject(o, VA);

assertEquals(2, eGet(Vo, "a2"));
#+END_SRC

On the class VA, the feature "a" is filtered out.  But we should still be able
to access "a2", right?  We do!  But, we return 1, instead of 2.

Why?  Because we end up calling VirtualEObject.dynamicGet(0), 0 being the
dynamicFeatureID since "a2" is now the first feature on the meta class.  But
VirtualEObject is oblivious to the filtered feature and gets the value of the
feature 0 in the concrete object.

So oviously we have to instruct VirtualEObject of the filtered features.

For the same reason, the following let us access the value of feature "a":

: Vo.eGet(0, false, false);

because we also end up in dynamicGet(0).

* [2017-09-08 ven.]
** Fixing feature ID with filtered features                        :emfviews:
Given the class A with features "a" and "a2", with respective feature IDs of 0
and 1.  If we filter "a", what should be the ID of "a2"?

Currently, EMF returns 0, and that explains the bugs from last time.  If a2
kepts its ID of 1, it would fix the first bug, but not the second: calling
eGet(0) would still give us access to the filtered feature "a".

What we need is to be able to map the virtual ID back to a concrete ID.  In this
case, ID 0 is feature "a2", so we to look for that feature and get its ID.

When I do eGet(o, "a2"), here is what happens:

- a2 is looked up in the features of the VirtualEClass, so we get the
  VirtualEAttribute "a2"
- eGet then calls eDerivedFeatureID which calls
  VirtualEClass.getFeatureID(feature), which returns the ID among filtered
  features: 0
- eGet then gets the feature object back from the VirtualEClass, so we get again
  the VirtualEAttribute "a2"
- eGet gets the setting delegate for this feature, and calls dynamicGet on it,
  with its feature ID (0)
- which calls VirtualEObject.dynamicGet(0)

So if we want to know which feature was called from the original eGet, we can
get it back from the VirtualEClass (again!).

In the end, I went with adding a getFeatureAbsoluteID(feature) on VirtualEClass,
which goes through all (filtered and non-filtered) features to give the ID.
This ID can then be used to query the concrete EClass and compute the offset in
the array of values in VirtualEObject.

I think I have all I need to implement back into Viewpoint.  At last!

* [2017-09-11 lun.]
** Implementing Viewpoint with VirtualEClass and co.               :emfviews:
Forgot VirtualEPackage.  It's basically the same thing as VirtualEClass: it
holds classifiers instead of features.

And at this point, I should also make sure that referential equality holds
between created virtual objects, using a common object to manage the map of
concrete to virtual objects.

Can't seem to use EcoreUtil.create() on a virtual class that does not belong to
a package... and that package also needs a factory.  Well, I'm not sure we will
create many objects at runtime through the virtual API anyway, so I'll just use
a dummy EPackage to create this instance and wrap it in a VirtualEObject.

Or, I guess the other way is to create a dynamic object ourselves and set its
metaclass.

I was a bit surprised that this bit worked:

: DynamicEObjectImpl Vo = new DynamicEObjectImpl(Vc);

where Vc is a VirtualEClass instance.

I don't have to wrap the dynamic object in a VirtualEObject in order to eGet or
eSet its features...  The reason it works is because here the Virtual class is a
simple proxy, and I only interact with a concrete feature existing on the
concrete class.  A DynamicEObject is oblivious to virtual features and filtered
features.

Hmm, actually, a DynamicEObject is perfectly capable of handling virtual
features and filtered features, as long as the metaclass /does not change/
dynamically.  Does that mean that a VirtualEObject can handle changes?

Yep, it does.  I made it even more flexible by storing the values in a map from
features to object, as the indexed array was too messy with filtered features
changing IDs and so on.  It's probably eating more memory though, but we'll
worry about that later.

Hmm, interesting!  In VirtualEObject.dynamicGet:

: return concreteEObject.eGet(concreteFeature);

fails because at some point, EAttribute looks up for the settingDelegate of its
containing class's /package/.  If the containing class is not part of the
package, null pointer exception.  So it looks like I need to have a concrete
EPackage on top of synthetic EClass even.

* [2017-09-12 mar.]
** Recapitulating tradeoffs for the new metamodel                  :emfviews:
Here is a class diagram of the new elements I've created so far:

[[file:doc/emfviews-new-class-diagram.svg]]

Rather straightforward: we have Virtual* classes that wrap concrete EMF objects
and delegate to them, but also support filtering and virtual features.

*** Why DynamicEObjectImpl and not, say, EStore, or implement EObject?
Implementing EObject seemed like a waste of time, since we could reuse existing
implementations.  Also, the docstring of EObject advises against it:

#+BEGIN_QUOTE
Implementations of EObject should extend BasicEObjectImpl or one of its derived
classes because methods can and will be added to this API.
#+END_QUOTE

(Also, while EObject has only a few methods, you really need to implement
InternalEObject to do anything with EMF, which has many more methods that are
not strictly relevant to our virtual layer)

For EStore, I've looked into the implementation, and I didn't see anything it
did more than DynamicEObjectImpl that could be useful.  The eStore.get API is
cumbersome: you get feature and an index into it, which leads to an overly
complex implementation to deal the different cases.  It's simpler to just
intercept dynamicGet, so we might as well extend DynamicEObjectImpl directly
rather than taking cruft.

The point is to have lightweight proxies.  We could probably even shed
DynamicEObjectImpl and extend BasicEObject and implement DynamicValueHolder
directly.  But this extending DynamicEObjectImpl was easy enough to start with.

*** Why do you need as VirtualEClass separate from VirtualEObject?
We need to control the metaclass (eClass) of VirtualEObject instances.  Since
a VirtualEObject is a DynamicEObjectImpl, we use eSetClass(EClass).  That
requires an EClass instance!

In fact, the ~EClass eClass()~ method is part of the EObject interface, so
there's no getting around it: we need an actual EClass implementation for the
metaclass.

VirtualEClass also holds a list of extra (virtual) features that do not exist on
the  concrete backing EClass.

*** Why do you need VirtualFeature?
A VirtualFeature is more than an EStructuralFeature: it can be filtered out, so
we need to keep track of that flag somewhere.  We could keep it in
VirtualEClass, along side the list of features, or we could keep it in a
"MetamodelManager" class (essentially, the Viewpoint).

But there's another reason to implement EStructuralFeature: EMF
(BasicEObjectImpl) uses the settingsDelegate of a feature in the reflexive
eGet/eSet calls to an EObject.  So, in order to capture the eGet calls to a
virtual object, we need to control the settingsDelegate of its features.  Hence,
we need to implement EStructuralFeature.Internal.

*** Why do you need VirtualEPackage?
We need a place to store pure virtual classes: those that are not backed up by
any concrete EClass from the contributing metamodels.

Also, since, virtual classes can be filtered out, having VirtualEPackage lets us
implement getEClassifiers() to return the list of non-filtered classifiers.

*** Why VirtualEAttribute and VirtualEReference?
I'm not sure we actually need those.  At the moment, it looks like we could move
their functionality to VirtualFeature and everything would work.

But while I haven't finished plugging this model back into Viewpoint and EView,
I don't want to prematurely factorize.

*** Why isn't VirtualEObject a parent for the others?
It could, possibly; I try to avoid prematurely factoring.  After I plug this
into Viewpoint/Eview, I will look into refactoring.

Currently, I think there's no benefit, apart from making a pretty class diagram,
to making VirtualEClass inherit from VirtualEObject:

- We don't want to inherit the dynamicGet/Set implementation of VirtualEObject,
  since they are specific to the model layer.
- We don't need to treat a collection of VirtualEClass as VirtualEObject.  If we
  put the filtered attribute on VirtualEObject, then it could be interesting.

*** Why does VirtualEClass need to wrap all its contents in virtual objects?
That is, when you ask for the features of VirtualEClass, you will get all the
concrete features of the backing EClass /wrapped in VirtualEFeature/ (same thing
for VirtualEPackage/VirtualEClass).  Is that necessary?  Why not give the
original feature back?

I think currently wrapping features in VirtualFeature is just a way to be able
to filter them.  Since the filtered attribute is held by a VirtualFeature, I
chose to create a VirtualFeature for each concrete feature in an EClass.
Another solution would be to have another object to ask whether a feature is
filtered, and where we would set the filters.

We still need VirtualFeature for purely virtual features, to catch the
settingsDelegate though.

*** Things to investigate later
- Referential equality does not hold; need a repository of virtual objects.
  This might get obsoleted if we simply do not wrap existing objects.  If the
  filtered bit for features is held by VirtualEClass for instance, then we not
  need to map concrete features to virtual features anymore, so referential
  equality holds.

  Otherwise, we need to keep track of the mapping between concrete and virtual
  objects.  Maybe on a virtual class scope, maybe on a viewpoint scope.

- Lot of stub methods.  Some might not make sense on a "virtual" object (so we
  can let them throw unsupported), but some do, and we have to make a decision
  about what they should answer.

  Also, the dynamicGet methods are supposed to work with all methods of the
  metaclass.

- VirtualEClass, VirtualEObject, etc. should be interfaces.  To mimic EMF class
  diagram, and so we could see what they add to EClass, EObject.

  Currently, there are really three differences: VirtualEClass has virtual
  features, VirtualEPackage has virtual classes, and all (including non-virtual)
  features/classes can be filtered.

* [2017-09-13 mer.]
** Factorizing into VirtualEFeature                                :emfviews:
Changed my mind about waiting for factorizing that, because it was an easy thing
to do, and I don't think I will need to revert it:  VirtualEFeature implements
EStructuralFeature.Internal, and now the EAttribute/EReference-specific methods
are more visible in VirtualEAttribute and VirtualEReference.

** Plugging back into Viewpoint                                    :emfviews:
Wrapping the contributing packages in VirtualEPackage.  All tests fail.  Yeah!

The validator is asking for an eFactoryInstance.  We don't have that on the
Viewpoint.  Turning off validation I'm afraid.

Easiest thing to do would be to plug back the filters.  Interestingly, we now
need a hold on the parent element to be able to filter something in the
metamodel.  Specifically, the Virtual parent.  It means that, in the Viewpoint,
we need to turn at least the EClass into VirtualEClass if we want to be able to
filter features.

Seems VirtualEPackage.getEClassifiers() cannot return an unmodified EList, as
EStructuralFeature.dynamicIsSet casts the return value to
EStructuralFeature.Setting!

#+BEGIN_SRC java
Object setting = settings.dynamicGet(index);
if (setting == null)
{
  return false;
}
else
{
  return ((EStructuralFeature.Setting)setting).isSet();
}
#+END_SRC

Hmmmmmm.  So EcoreEList$UnmodifiableEList implements EStructuralFeature.Setting,
but ECollections$UnmodifiableEList does not.  Sigh.

Have to use the former, which is a bit awkward.

Looks like I can't use the debugger to "Run to line" in a lambda.  Sad.  Back to
loops instead of forEach then.

AAArgh.  So blacklisting works, but whitelisting doesn't.  Everything is
filtered out when whitelisting.  Why?  Because EMFViewsUtil.getEObjectPath does
not return a full path, but only the name of object ("Element" instead of
"contentfwk.Element").  That's because we build the path by going up the
container hierarchy:

#+BEGIN_SRC java
    while (o != null) {
      comps.add(((ENamedElement) o).getName());
      o = o.eContainer();
    }
#+END_SRC

But for virtual objects, eContainer is null.  Probably have to fix that, and/or
find another way to build the path.

* [2017-09-15 ven.]
** Fixing empty eContainer                                         :emfviews:
eContainer is a method on the EObject interface.  According to the docs:

#+BEGIN_QUOTE
An object is contained by another object if it appears in the {@link #eContents
contents} of that object.  The object will be contained by a {@link
#eContainmentFeature containment feature} of the containing object.
#+END_QUOTE

Since a VirtualEPackage's contents return VirtualEClasses, the eContainer of the
VirtualEClass should return the VirtualEPackage.  I should /also/ implement
eContainmentFeature to return the ID of the containment feature of the
eContainer.

Overriding eContainer to return the container from the concrete feature, but
virtualized, works.

At this point however, I wonder if I should just not simply return to the
DelegatedEObject implementation strategy.

Hit an interesting Java problem when trying to be a little more strict in
Viewpoint.  Summed up my thoughts [[file:emfviews-test/src/another/MyMap.java][here]].

* [2017-09-18 lun.]
** Adding superTypes to a VirtualClass                             :emfviews:
Seems I did not anticipate this scenario in my previous tests.  Doing:

: klass.getESuperTypes().add((EClass) sup);

where ~sup~ is a VirtualEClass fails because the VirtualEClass cannot be cast to
an ESuperAdapter.Holder.  I guess we can delegate this call to the concrete
EClass.

Yep that works.  Although, I also need to keep a list of virtual supertypes,
i.e., super types that only exist on the virtual metamodel.

Damned.  Can supertypes be filtered?  I guess they could.  Did not take that
into account.  If filters on classifiers are held by the package... we would
have to ask the package to know what to answer in VirtualEClass.getESuperTypes.

Similar issues arise with creating associations: the eOpposite feature should be
set on the virtual feature, but what happens if one end is filtered?

Updated the tests to work with a mock viewpoint (really, an interface to keep
track of virtualized objects).  How filters behave on supertypes is an
interesting problem.  Might not be solved right away, but it's a good thing to
register it.

Tomorrow: resolve the eOpposite issue.

* [2017-09-19 mar.]
** Resolving the eOpposite issue                                   :emfviews:
When we add a bidirectional reference, it's only after we first create a
synthetic EReference.  That synthetic reference can be altered (it's part of the
virtual package).  However, it can point to a synthetic EReference, or it can
point to a concrete EReference.  In both cases, the eOpposite link should only
exist virtually and not alter the concrete metamodel elements.

That means, that we have to add a virtualEOpposite feature on EReference.  Or,
setting the eOpposite on a VirtualEReference does not set it on the concrete
ERef.

Okay, that works.  Went with the first option, as having an explit
setVirtualOpposite is less confusing for the moment.

Now all viewpoint tests pass.  On to EView.

** Converting EView to use VirtualEObject                          :emfviews:
When "projecting" virtual links in EView:

Trying to set VirtualERef 184 (-> ERef detailedProcess 196)
-> dynamicSet with featureID = 16
absoluteID = 16  (VirtualERef 184)

But 29 features on the concrete EClass, not 15.

Using getEStructuralFeature(int) on EClassImpl will look up the feature among
/all/ structural features, local and inherited.  While my lookup for absolute ID
only accounts for local features.

Okay, implemented EAllStructuralFeatures.

Now I need to override VirtualEObject.eContents to virtualize objects from
containment references.

Need to implement EAllContainments... which uses EAllReferences...

Now VirtualEObject needs a virtualizer.. which is the EView.

And that's it!  It works!  All tests pass!

Tomorrow: make sure that the constructed viewpoint/view are all virtualized, and
add tests for cases where they are not.

* [2017-09-20 mer.]
** Double-checking correctness of the new class diagram            :emfviews:
A note about correctness of the virtualization: looking at getAllContents of a
viewpoint, the objects that are not virtualized mostly seem to be:

- EAnnotationImpl
- EGenericTypeImpl
- EStringToStringMapEntryImpl

All model elements can have annotations.  Since it's a containment reference, an
EAnnotation has an eContainer attribute pointing to the containing EClass, say.
Not the virtual one.  You should be able, however, to iterate over annotations
of a VirtualEClass.  The containment link is just broken in the other way.

I fear it's the same for other containment links; we don't take extra care of
maintaining consistency for these currently.  How hard would it be, in practice?
I think it's just a matter of wrapping EAnnotationImpl, override eContainer(),
and return the corresponding VirtualEClass.

So, not really hard to do if we find out that it's something we need further
down the road.

Samething goes for EGenericType.  The StringToString map are contained by
annotations.. so if annotations are virtualized, these map entries should also
point to the virtualized container.

Found a few more:

- EEnumLiteralImpl
- EEnumImpl

Same remarks: we can obtain the concrete versions from getAllContents on a
virtualized package.  Going up the eContainer reference, we'll end up on the
concrete metamodel.

Hmm.  Calling getAllContents on an EView raises an NPE.  There's an eGet in
VirtualEObject.eContents that returns null.  Should it?  Can it?  It looks as if
the TreeIterator is not happy about it in any case, because it tries to get the
contents of null.

Maybe I should just not add a null value to the eContents list.  Yeah, it looks
like calling getAllContents on the original model does not produce any null.

But now, EMF wants to cast a VirtualEReference to
an... EStructuralFeatureExtendedMetaData.Holder.  Yuck.

But this only happens because we end up in BasicEObjectImpl.eOpenGet, because
the featureID is -1.

The reference is reqif10.Identifiable.alternativeId.  It's not found on the
structural features of the virtual EClass, hence the -1.

If it was filtered, it shouldn't have appeared in the getAllContents in the
first place.  So it's not filtered.  But then, if it's not filtered, why does it
appear in the getEStructuralFeatures list?

So actually, we are looking for the feature ID of 'alternativeID' on the EClass
'DatatypeDefinitionStringImpl', which is a subtype of Indentifiable.  And the
feature is on the parent, so we miss it.

Well, I guess getFeatureID should look into getAllEStructuralFeatures and take
inherited features into account.  That's what EClassImpl does.

Same thing for getEStructuralFeature(int) and getEStructuralFeature(String):
they both look up into the /all/ structural features, including inherited.

Hmmmm.  eGet in eContents returns a boolean.  Shouldn't we always have EObjects
in features?

Oh, okay.  in VirtualEObject.dynamicGet, we return a completely wrong feature.
(Damn you, feature IDs!).

dynamicFeatureID = 17
feature is the VirtualERef for the 'monitoring' ERef
absoluteFeatureID = 17
concreteFeature = 'isExecutable' EAttribute <-- wrong

'monitoring' is the feature of index 3 in the eStructuralFeatures list of
Process.  And feature of index 13 in eAllStructuralFeatures.

Something is clearly very wrong here.  But these IDs are getting on my nerves,
so I will simply lookup the feature by name in the concrete class.  Feature
names are unique anyway.

Yeah!  Tests pass.  And the eAllContents of my objects are all VirtualEObjects
in front of DynamicEObjectImpl or instances of generated classes.

Next time: make a class diagram of the ways things work now.  Also make diagrams
for on a couple of examples of what is created and what the links look like when
we have a viewpoint and a view.

* [2017-09-22 ven.]
** Notes on the new class diagram                                  :emfviews:
[[file:doc/emfviews-new-class-diagram3.svg]]

I tried to simplify the original architecture while keeping its functionality
intact, and bringing it closer to the original idea.

The main idea is to mirror EMF classes to virtual counterparts.  So we have
VirtualEObject, VirtualEPackage, VirtualEClass, etc.

The point of these virtual classes is twofold.  First, they act as proxies to
the objects in the contributing models and metamodels.  We can decide whether to
write back changes to the original objects, or make the proxies read-only.  We
can intercept any method call, any feature set, and change its behavior.
Second, the virtual classes allow us to /filter/ and /extend/ the contributing
metamodels without modifying them.

Any classifier in a VirtualEPackage can be filtered, any feature in a
VirtualEClass can be filtered.  Since a VirtualEObject has a VirtualEClass as
metaclass, the filtered features are reflected on the VirtualEObject as well.
These filters are dynamic: we can filter or unfilter a feature and see the
changes on the next call to getEStructuralFeatures, without the need to
reconstruct the virtual metamodel.

Following the weaving model, we can also add features to existing concepts: a
VirtualEClass has a list of such virtual features.  A virtual feature is part of
the VirtualEClass eStructuralFeatures feature, but does not affect the backing
concrete class.  All virtual links specified by the weaving model exist in this
new virtual layer (virtualFeatures, virtualClasses, virtualSuperTypes,
virtualOpposite): these links exist solely between the virtual objects, without
modifying the concrete metamodels underneath.

The Virtualizer is needed to maintain consistency between objects; it maps a
concrete object to its virtual counter part.  For instance, a VirtualEPackage
contains classifiers.  These classifiers are VirtualEClass instances.  When we
ask for the super type of a VirtualEClass, these are classifiers from the same
package, hence, the super types should be the /same VirtualEClass instances/
found in the list of classifiers from the VirtualEPackage.  The VirtualEPackage
and VirtualEClass objects need to communicate somehow to make sure these
instances are the same: that's the role of the Virtualizer.

In the previous architecture, this role was filled by the VirtualLinkManager and
MetamodelManager.  They managed the maps between concrete and virtual objects,
for the model and metamodel layers respectively.  The MetamodelManager made
distinctions between the types of objects, and a larger API surface as a result.
The Virtualizer is much simpler, conceptually and implementation-wise, and is
used the same way by both layers.

** Advantages over the the previous architecture                   :emfviews:
Mostly, the previous architecture suffered from growing organically, in the face
of new requirements.  It was baroque, and in need of a good cleaning.  Dead
code, duplicated code, coupling between classes, and many questionable,
and crucially undocumented, decisions.

In rethinking the whole thing, we have shed a lot of weight, to obtain something
that's conceptually clearer and leaner.

We also gained some flexibility.  The main defect of the previous architecture
was in the Viewpoint, where it duplicated all the contributing metamodels
elements.  This had a upfront cost when loading the Viewpoint resource, both in
time and memory.  More importantly, it meant we didn't have a symmetry between
the implementations of the virtual model and virtual metamodel.

Now, instead of duplicating contributing metamodel objects, we simply wrap them,
but lazily.  When we load the Viewpoint resource, we simply wrap the
contributing metamodel packages in VirtualEPackage.  That's all we do with the
contributing metamodels.  We don't virtualize the contents of these package
until they are actually requested.  So, when we call getEClassifiers on
VirtualEPackage, that's when we virtualize the classifiers to provide the list.

(And ideally, we could provide a lazy list, which would only virtualize the
classifier when one is accessed.  This is useful in case only one element in the
list is accessed, or if the list is actually never used!).

So, instead of having the full contributing metamodels duplicated, in practice,
we only build what's used.  Furthermore, even if we end up building virtual
objects to stand in for every object in the contributing metamodels, these
virtual objects still have a smaller memory footprint than the default EMF one,
since they contain less information.  The virtual objects can delegate most of
their knowledge to the backing concrete object.

For the model layer, the previous architecture was already using
ReproduceElement instances as proxies for concrete model objects.  I simplified
the implementation by using only View and VirtualEObject, whereas before we had
View, EView, VirtualElement, ReproduceElementImpl, ReproduceRule,
TranslationRule, and VirtualAssociation (without mentioning the unused
MergeElementImpl and FilterElementImpl).

** Rationale for cutting classes from the previous architecture    :emfviews:
I'm referring to [[file:doc/emfviews-class-diagram.svg][this diagram of the previous architecture]].

- LinksProjector.  This was easy, it's only job was to populate the contents of
  virtual features at the model level.  When virtual features are created on the
  viewpoint, the weaving model of the model tells us what model objects to put
  into them.  The LinksProjector did that.  It was a one-time thing, when the
  View is loaded.

  I turned it into 25 lines in View.doLoad.

- MetamodelManager.  I'm not quite sure why we needed to separate features and
  classes in these maps.  Or why we needed this class at all.  We built these
  maps ahead of time, which made them inflexible.  It seemed to me we didn't
  need to keep track of all this; the View only has to use the Viewpoint as
  metamodel.

  In fact, in VirtualEObject, it's quite easy to find out if a feature is
  virtual or not without these maps.  And we could put the information into
  Viewpoint itself.

- View/EView.  I merged them into View, since it was really a split class, with
  View being intended to act as a superclass for Viewpoint, I think.  But this
  was never the case.

- VirtualContents.  These was an awkward list implementation that allowed
  multiple list to behave as one.  Unnecessary, since we had to build the
  sublists anyway, we might as well flatten them and put all the objects
  directly into a plain list.

- VirtualModelList.  This was one was more interesting.  It was a kind of lazy
  list that virtualized its contents.  Used by ReproduceRule to turn the
  contents of a reference into ReproduceElementImpl instances.

  We can build this list ahead of time without any change in functionality.  But
  I may resuscitate a similar "delayed" list for virtualizing only objects that
  are accessed if we find that premature virtualization has impacts performance.

- TranslationRule, VirtualElement, ReproduceRule, ReproduceElementImpl,
  VirtualAssociation.  All these are replaced by the single VirtualEObject
  class, which is not very complex.  The Rule/Element split was needed because
  that's how EStore works.  Other causes were unncessary abstraction, and
  duplicated code.

* [2017-09-25 lun.]
** Writing more tests for virtual objects                          :emfviews:
I reluctantly added a mock virtualizer to the virtual objects, since at first I
didn't need one to construct the tests.  However, the virtualizer is necessary
to maintain consistency in the design.  When writing a new test for inherited
virtual features for instance, if I create the VirtualEClass instances in the
tests (to add attributes to them), then they won't be registered by the
virtualizer, and the virtual features will never be seen by the virtual class.

So, the virtualizer is integral to the design.  Something to keep in mind when
writing tests.

Hmm, in fact, it turns out my mock virtualizer was too complex: it actually
/did/ virtualize objects, while this was not necessary for most of the tests I
had already written.

Instead, it can just return its argument.  For tests that require a virtualizer
that does actuall creation of virtual objects, then I can use Viewpoint and View
directly, instead of testing my duplicate implementation in MockVirtualizer.

I added the tests for the cases I encountered while rewriting Viewpoint/View.
I'm thinking of adding a few more tests, one for each feature of the EMF class
diagram.

** Filtering classifiers in getESuperTypes                         :emfviews:
So, the list of filtered classifiers is held by the virtual package, since it
allows us to filter any classifier, not just EClass, without having to define a
VirtualEClassifier class.

However, we can ask to filter any classifier, including:

1. classifiers that are not even part of the package, and
2. virtual counterparts for classifiers that are part of the package (and
   vice-versa).

The first is easy to solve: just check the classifier is in the package.  The
second...  Conceptually, when you ask for the classifiers of a VirtualEPackage,
you get virtual elements.  Well, a mix of virtual elements and concrete elements
that are not virtualized (EDataType).  So the question is really: when we
filter an EClass A, should its virtual counterpart be filtered as well?

Maybe, to be consistent and not too lenient, we should just be able the
classifiers that are returned by getEClassifiers.  That way, it's predictible.

Ah, now it seems the mock virtualizer is not enough.  Most of the tests fail
because they lack the internal consistency brought by the viewpoint/view
virtualizers.  Too bad.

But, otherwise, it works.  Except that classifiers are now required to have a
package, although conceptually that was already the case in EMF anyway.

** Interesting property of getVirtual                              :emfviews:
For methods that return an EObject, getVirtual has the following property:

: getVirtual(concreteEClass.getEPackage()) == getVirtual(concreteEClass).getEPackage()
: f(g(c)) = g(f(c))

* [2017-09-26 mar.]
** Adding tests for all proper features of the EMF diagram         :emfviews:
Most of this is straightforward.  Just a bit tedious to write.

Tried to add a reference via getEReferences(), to see if it would work.  It
does, but EMF outputs on stderr:

#+BEGIN_SRC java
public boolean add(EReference object)
{
  System.err.println("Please fix your code to add using EClass.getEStructuralFeatures() instead of EClass.getEReferences()");
  return getEStructuralFeatures().add(object);
}
#+END_SRC

"I will add the reference, but please don't do that".  I'm not sure what's the
effectiveness of such a statement.  But I guess there's no way to deprecate an
override... so, better than nothing.

* [2017-09-27 mer.]
** More tests                                                      :emfviews:
Finishing the tests for each Virtual* component I reimplemented.

** EOpposite troubles                                              :emfviews:
A thorny issue arises once again with EOpposite.  Consider the situation where R
and R2 are two concrete references, opposite of each other, and VR and VR2 are
their respective virtual counterparts.

Clearly, VR and VR2 are opposite of each other.  The concrete opposite link is
reflected at the virtual level.

Now, consider the situation where R is a concrete reference, and R2 is created
as part of a new association, and has R as opposite.  The opposite link then
only exists at the virtual level, not the concrete one.  Hence, it must be
materialized, and that's why I added a virtualEOpposite attribute.

But now, what should VR2.getEOpposite() return, in general?  The value of the
virtual opposite, or the value of the concrete one?

It seems strange to me that this is the only place where this problem arises.
But I guess it's also the only feature of arity 0..1 that's reflected at the
virtual level.

For now, returning the virtual opposite if present and falling back on the
concrete one seems the most natural solution to me.

** Another snag with bidirectional references                      :emfviews:
When adding a virtual feature to a VirtualEClass, we do not set the
eContainingClass of the feature propertly.

Problem: how do we actually set the eContainingClass feature?  There is no
setter, and eSet says it "is not a changeable feature", even though, indirectly,
it is.

I should probably trace how EMF effectively changes the feature value, and hack
it into VirtualEClass.addVirtualFeature.

** Next time                                                       :emfviews:
- Solve the bidirectional reference issue
- Compute how many virtual objects are created when loading the viewpoint on a
  large model/metamodel like TOGAF
- Add javadoc for core methods/classes
- Update README
- Update copyright notices (and close TODO)
- Merge back in master

* [2017-09-29 ven.]
** Solving the bidirectional reference issue                       :emfviews:
When we do A.getEStructuralFeatures().add(r), we end up in r.eInverseAdd(A, 17),
where 17 is the feature ID of eContainingClass.

Ultimately, that sets eContainer to A, and eContainerFeatureID to 17.

Yep, calling eInverseAdd works.  We don't want to do that in
VirtualEClass.addVirtualFeature though, since we could potentially modify a
contributing metamodel that way.  Instead, I do it in the Viewpoint, since there
I know the structural feature is synthetic and can be modified.

The only other place where we would have the same issues is the
eClassifiers/ePackage containment reference.  But there, we actually add the
VirtualEClass to the virtual package using getEClassifiers().add(), because we
can modify the virtual package.

* [2017-10-02 lun.]
** Working through Xtext tutorial                                  :emfviews:
[[http://www.eclipse.org/Xtext/documentation/102_domainmodelwalkthrough.html][There]].  Xtext is a language engineering platform (or "language workbench", as
others have called it).   Given a grammar, it will generate a bunch of code for
parsing, syntax highlighting, serialization...  It will infer an EMF model based
on the grammar, or you can use an existing one.

It's tailored to define your own DSLs inside Eclipse.  And it seems it's a
low-friction way to do so.

That's exactly what we use it for in EMFViews: it for our two DSLs.

** Next time                                                       :emfviews:
Make our SQL2Views DSL work with the new weaving model

* [2017-10-03 mar.]
** Exploring VPDL                                                  :emfviews:
ViewPoint Description Language: our DSL for creating a viewpoint using a
SQL-like language instead of writing Eviewpoint files and weaving models by
hand.

In the VPDL file, like this one:

#+BEGIN_EXAMPLE
create view myEAviewpoint on
'http://www.obeonetwork.org/dsl/togaf/contentfwk/9.0.0' as togaf,
'http://www.omg.org/spec/BPMN/20100524/MODEL-XMI' as bpmn,
'http://www.omg.org/spec/ReqIF/20110401/reqif.xsd' as reqif
select togaf.EnterpriseArchitecture.architectures, togaf.StrategicArchitecture.strategicElements,
  togaf.BusinessArchitecture.processes, togaf.Process.processCritiality, togaf.Process.isAutomated,
  togaf.Requirement.statementOfRequirement, togaf.Requirement.rationale, togaf.Requirement.acceptanceCriteria,
  bpmn.Process.isClosed, bpmn.Process.isExecutable, bpmn.Process.processType
from togaf.Process join bpmn.Process as detailedProcess,
  togaf.Requirement join reqif.SpecObject as detailedRequirement
where togaf.Process.name=bpmn.Process.name and
 togaf.Process.isAutomated = 'false'
#+END_EXAMPLE

It lists the contributing metamodels, so we can put that into the eviewpoint.

It lists the features that should be visible (via 'select'), and that
should go into the weaving model for the viewpoint.

It also tells us how we should merge the contributing models, and that should go
into the ECL file.

(Sidenote: Cheng remarked that the syntax for comparison in the ~where~ could be
more expressive, using OCL expressions.  Looking at the grammar, it looks like
we accept arbitrary ECL expressions, in the form of strings).

Looking at what we have, beside the grammar, there is a generator that writes
the Eviewpoint file and the ECL file.  However, the ECL generation is hardcoded
to output only the ECL for the three model composition...

If there is a metamodel for ECL, we should preferably use that in the generator.

And lastly, there is a SQL2VirtualLinks ATL transformation that should create a
weaving model from the VPDL.  There is Java code to run the transformation, but
it's not plugged in the generator in any way.

Regarding the grammar, we could define the metamodel for the parsed language
ourselves, as currently it generates a lot of useless nodes that are needed by
the grammar, but not by the end model.

Alternatively, we could map to WeavingModel directly, but I don't think that's a
good idea, since it's clear now that one VPDL maps to 3 differents models:
EViewpoint, ECL, and weaving model.

** Xtend font troubles                                        :eclipse:xtext:
The Xtend plugin (in addition to having its own syntax coloring scheme, separate
from the base Java one) was stuck using Monospace-10, not picking up my default
editor font.  There was a promising setting under General->Appearance under
"Xtend editor font", but changing it had no effect.  In fact, it resets when I
load the preferences panel again (!).

Curiously, each syntax class under Xtext->Editor->Syntax coloring /also/ has the
possibility to set the font.  By default, it's set to Dina-Regular-8.  But it
has no effect (!!).

However, /changing/ the value from the default does have an effect.  And
luckily, Dina 7pt and 8pt are exactly the same, since it's a bitmap font with
only three actual different sizes.  Changing every syntax category to
Dina-Regular-7 solves the problem.

Except!  Javadoc-style comments (beginning with ~/**~) still use the default
Monospace-10 font, and there's no syntax class to change it (!!!).  Removing the
second star changes it into a regular multi-line comment, which has syntax class
set to Dina.

And people shrug off Emacs...

* [2017-10-04 mer.]
** Syntax tryouts for VPDL                                         :emfviews:
Trying to feel natural, without redundant information.

In this one we move the contributing metamodels to the FROM, and allow a
shortcut syntax with brackets to select multiple attributes from the same
concept.  New associations are also moved to SELECT.

#+BEGIN_EXAMPLE
create view myEAviewpoint as

select togaf.EnterpriseArchitecture.*, togaf.StrategicArchitecture.strategicElements,
  togaf.BusinessArchitecture.processes, togaf.Process[processCritiality, isAutomated],
  togaf.Requirement.statementOfRequirement, togaf.Requirement.rationale, togaf.Requirement.acceptanceCriteria,
  bpmn.Process[isClosed, isExecutable, processType],

  togaf.Process join bpmn.Process as detailedProcess,
  togaf.Requirement join reqif.SpecObject as detailedRequirement

from 'http://www.obeonetwork.org/dsl/togaf/contentfwk/9.0.0' as togaf,
'http://www.omg.org/spec/BPMN/20100524/MODEL-XMI' as bpmn,
'http://www.omg.org/spec/ReqIF/20110401/reqif.xsd' as reqif

where togaf.Process.name=bpmn.Process.name and
 togaf.Process.isAutomated = 'false'
#+END_EXAMPLE

Here we declare concepts in FROM, metamodels in ON, and select individual
attributes in SELECT.

#+BEGIN_EXAMPLE
create view myEAviewpoint on
'http://www.obeonetwork.org/dsl/togaf/contentfwk/9.0.0' as togaf,
'http://www.omg.org/spec/BPMN/20100524/MODEL-XMI' as bpmn,
'http://www.omg.org/spec/ReqIF/20110401/reqif.xsd' as reqif

select tEA.architectures, tSA.strategicElements,
  tBA.processes, tP.processCritiality, tP.isAutomated,
  tR.statementOfRequirement, tR.rationale, tR.acceptanceCriteria,
  bP.isClosed, bP.isExecutable, bP.processType,

  tP join bP as detailedProcess,
  tR join rS as detailedRequirement

from togaf take
       Process as tP,
       EnterpriseArchitecture as tEA,
       StrategicArchitecture as tSA
       BusinessArchitecture as tBA,
       Requirement as tR
     and
     bpmn take Process as bP
     and
     reqif take SpecObject as rS

/* optional */
where tP.name=bP.name and tP.isAutomated = 'false'
#+END_EXAMPLE

Turns out, declaring the concepts is redundant, and using labels is less
readable.  So we go back to the bracket solution, but allow you to declare new
associations inside the brackets as well.

Labels could still be useful in WHERE, so we let you use AS there:

#+BEGIN_EXAMPLE
create view myEAviewpoint as

select togaf.EnterpriseArchitecture.architectures,
       togaf.StrategicArchitecture.strategicElements,
       togaf.BusinessArchitecture.processes,
       togaf.Process[processCritiality, isAutomated, join bpmn.Process as detailedProcess],
       togaf.Requirement[statementOfRequirement, rationale, acceptanceCriteria],
       bpmn.Process[isClosed, isExecutable, processType]
       togaf.Requirement join reqif.SpecObject as detailedRequirement

from
  'http://www.obeonetwork.org/dsl/togaf/contentfwk/9.0.0' as togaf,
  'http://www.omg.org/spec/BPMN/20100524/MODEL-XMI' as bpmn,
  'http://www.omg.org/spec/ReqIF/20110401/reqif.xsd' as reqif

/* optional */
where togaf.Process as tP and
      tP.name=bpmn.Process.name and
      tP.isAutomated = 'false'
#+END_EXAMPLE

** Next time                                                       :emfviews:
We should already be able to load an EViewpoint file in the Sample Ecore editor
and see the viewpoint.  Why is this not working yet?

Generate the weaving model from the ATL transformation.

Generating the matching model can wait.

* [2017-10-06 ven.]
** Loading an Eviewpoint in Ecore editor                           :emfviews:
Throws a NullPointerException because getContents returns virtualContents, which
at this point is null.

The sample editor does not try to load the resource before listing its contents,
strangely.

Ah, no.  It does attempt to load the Viewpoint first, but doLoad throws an
exception that is caught by EMF, which continues happily after that.

The exception is thrown because the TOGAF metamodel cannot be found.  Forgot to
open the TOGAF project!

When that is fixed, doLoad still fails because the weaving model is empty, which
is expected.

** Calling the ATL transformation in the code generation           :emfviews:
Was not too complicated (though ceremonious):

#+BEGIN_SRC java
var injector = new EMFInjector()
var factory = new EMFModelFactory()

var sourceMM = factory.newReferenceModel()
injector.inject(sourceMM, "http://www.inria.fr/atlanmod/emfviews/vpdl")

var targetMM = factory.newReferenceModel()
injector.inject(targetMM, "http://inria.fr/virtualLinks")

var sourceModel = factory.newModel(sourceMM)
injector.inject(sourceModel, r)

var targetModel = factory.newModel(targetMM)

var launcher = new EMFVMLauncher();
launcher.initialize(null)
launcher.addInModel(sourceModel, "IN", "VPDL")
launcher.addOutModel(targetModel, "OUT", "VL")
launcher.launch(ILauncher.RUN_MODE, new NullProgressMonitor(), new HashMap(),
  new URL("platform:/plugin/fr.inria.atlanmod.emfviews.vpdl/transformation/SQL2VirtualLinks.asm").openStream)

var extractor = new EMFExtractor()
// FIXME: return an outputstream or a string instead of hardcoding the path here
extractor.extract(targetModel, "platform:/resource/test-vpdl/src-gen/" + viewpointName(r) + ".xmi")
#+END_SRC

The hardest part was getting the path right... which I didn't solve yet.

But, Hugo suggested I use [[https://wiki.eclipse.org/ATL/EMFTVM][EMFTVM]] instead.

** Using EMFTVM                                                    :emfviews:
Apparently, I had a plugin for EMFTVM that came with the ATL install from the
regular Eclipse update site, but, after adding the line:

: -- @atlcompiler emftvm

to the top of the ATL file, I get the error:

"No compiler found for ATL emftvm.  You may need to install a compiler plugin"

So I tried using the snapshot version on the marketplace, with this update site:

: https://hudson.eclipse.org/shared/job/mmt-atl-master/lastSuccessfulBuild/artifact/releng/org.eclipse.m2m.atl.update/target/repository/

And there was an additional "ATL EMFTVM" feature.  I also have an "ATL EMFTVM
Compiler" plugin, and a bunch of other stuff.

After that, it compiled the ATL file to EMFTVM instead of ASM.

The ceremony to invoke the transformation is different this time:

#+BEGIN_SRC java
var factory = EmftvmFactory.eINSTANCE
var rs = new ResourceSetImpl()

var env = factory.createExecEnv();

// Load metamodels
var sourceMM = factory.createMetamodel()
sourceMM.resource = rs.getResource(URI.createURI("http://www.inria.fr/atlanmod/emfviews/vpdl"), true)
env.registerMetaModel("VPDL", sourceMM)

var targetMM = factory.createMetamodel()
targetMM.resource = rs.getResource(URI.createURI("http://inria.fr/virtualLinks"), true)
env.registerMetaModel("VL", targetMM)

// Load models
var sourceModel = factory.createModel()
sourceModel.resource = r
env.registerInputModel("IN", sourceModel)

var targetModel = factory.createModel()
targetModel.resource = rs.createResource(URI.createFileURI("platform:/resource/test-vpdl/src-gen/" + viewpointName(r) + ".xmi"))
env.registerOutputModel("OUT", targetModel)

// Run the transformation
var mr = new DefaultModuleResolver("platform:/plugin/fr.inria.atlanmod.emfviews.vpdl/transformation/",
  new ResourceSetImpl())

var timing = new TimingData()
env.loadModule(mr, "SQL2VirtualLinks")
timing.finishLoading
env.run(timing)
timing.finish

targetModel.resource.save(null)
#+END_SRC

But, unfortunately, even though the ~targetModel~ resource has been successfully
created in memory, the ~save~ call does not write the XMI file!

** Next time                                                       :emfviews:
Find out why the output model is not written.

Update the ATL transformation to get all the information from the VPDL.

* [2017-10-09 lun.]
** Writing the output model with EMFTVM                            :emfviews:
So, I'm calling:

: targetModel.resource.save(null)

And I've checked that the targetModel actually contains what I want.  It might
be an issue with the path of the file?

Well, I tried to trace the call, and it didn't seem to raise any error.

But then I don't even want to save to a file at this point, because I'd rather
use FileSystemAccess in doGenerate to write the file for me.  I just need to
write the model to a string:

#+BEGIN_SRC java
var out = new ByteArrayOutputStream()
targetModel.resource.save(out, null)
return new String(out.toByteArray())
#+END_SRC

which I found [[https://www.eclipse.org/forums/index.php/t/1070698/][here]].

** Updating the ATL transformation                                 :emfviews:
Fighting with ATL syntax to find out how to write what I want.  But I'm a
beginner, so that's expected.

References:
- https://www.eclipse.org/atl/atlTransformations/
- http://wiki.eclipse.org/ATL/User_Guide_-_The_ATL_Language

I've managed to generate the elements to be filtered, but now I've got to put
them in the WeavingModel class proper.

Unless I'm mistaken, it seems that ATL happily generates model that do not
conform to the specified target metamodel at all.  That there is no static check
for this, I understand, but no runtime checks as well?

** Next time                                                       :emfviews:
Complete ATL transformation

* [2017-10-10 mar.]
** Working on ATL transformation and grammar                       :emfviews:
Interesting point: when adding a new relation between A and B, both A and B
concepts must be included in the view, and not be filtered out, otherwise the
viewpoint creation fails.

Does it make sense?  It seems best that all concepts used by the viewpoint are
actually present in there, otherwise the view cannot conform to the viewpoint.

But does the user have to actually spell out explicitly which attributes they
want out of the concept?

Also, by default, it seems I have opted for the whitelisting mode, since it's
the most intuitive with the ~select~ syntax.  There could be syntax to reverse
it on a concept basis.

** Xtext goodies                                             :emfviews:xtext:
Tried to implement auto completion for classes, since we already have metamodels
and their namespace URI.  For some reason, this only works if I put the ~from~
section before the ~select~: then the metamodels are defined before we attempt
to look up things in them.  Otherwise, the metamodel reference in a
SelectFeature is a proxy that I can't seem to resolve, and the nsURI is null.

I also tried to implement a ScopeProvider for that (thinking that
auto-completion and validation would both come from implementing the scope
provider correctly).  But I would need to return EClasses, so I think I have to
change the grammar to make references to actual EClass, not my ad-hoc 'class'
thing.

** Fixing an exception when reloading the viewpoint in sample editor :emfviews:
I got UnsupportedOperationEx after clicking in the sample Ecore editor on the
viewpoint resource after the viewpoint file has been updated by the VPDL
generator.

Why?  Because it the editor calls Viewpoin.doUnload, which calls
getContents.clear(), which is unsupported.  Implementing it to clear our
virtualContents, and to rebuild them lazily in getContents does the job.

We can now refresh the contents in the sample editor with right click->Refresh.
This works fine for renaming stuff, but it seems the editor is confused when we
add/remove children.  We may have to build our own viewer in the end.

One feature that would be nice would be to have different colors for virtual
associations when loading a Viewpoint in the sample Ecore editor.  Maybe this is
feasible by providing a custom label provider or something?

** Next time                                                       :emfviews:
Try to get the validation/completion working by tweaking the grammar.  If
unsuccesful, ask around.

* [2017-10-11 mer.]
** Validation and completion with Xtext                      :emfviews:xtext:
So, I'm trying to use [[http://www.eclipse.org/Xtext/documentation/303_runtime_concepts.html#scoping][scoping]] for that.  It seems the default implementation is
useful.  E.g., for the metamodels in the From clause:

#+BEGIN_EXAMPLE
from
  'http://www.obeonetwork.org/dsl/togaf/contentfwk/9.0.0' as togaf,
  'http://www.omg.org/spec/BPMN/20100524/MODEL-XMI' as bpmn,
  'http://www.omg.org/spec/ReqIF/20110401/reqif.xsd' as reqif,
#+END_EXAMPLE

If I do:

: select tof.Bar.none,

then it will:

1. Recognize ~tof~ as invalid, because it's supposed to be one the Metamodel
   from the ~From~ clause.
2. Suggest as quick-fixes to use either ~togaf~, ~bpmn~, or ~reqif~, which is
   exactly what I want.
3. Autocompletion also works outside of the box, as it only suggests the
   metamodels declared in the ~From~ clause.

Stepping through the code, the quick-fix suggestions are gathered from a default
ScopeProvider implementation.  Curiously, it only works if there are 5 or fewer
suggestions:

#+BEGIN_SRC java
if (discardedDescriptions.size() + addedDescriptions <= 5) {
  for(IEObjectDescription referableElement: discardedDescriptions) {
    createResolution(issueString, referableElement, ruleName, keyword, caseInsensitive);
  }
}
#+END_SRC

The discarded descriptions are those that are not similar enough.  So if I have
6 contributing metamodels, then I get no quick-fix suggestion anymore.
Auto-completion stills suggests the right things though.

Okay, so after input from Massimo, it turns out that auto-completion works when
we remove the two alternatives in this rule:

#+BEGIN_EXAMPLE
SelectFeature: metamodel=[Metamodel|ID] '.' class=ID '.' features+=Attribute
  | metamodel=[Metamodel|ID] '.' class=ID features+=Relation
  | metamodel=[Metamodel|ID] '.' class=ID '[' features+=Feature (',' features+=Feature)* ','? ']';
#+END_EXAMPLE

The issue seems to be that Xtext is confused somehow by the fact the same
reference feature appears in multiple variants.  Or the ambiguity of rule is
enough to throw Xtext off the rails?

Strangely, this parses just fine, and the model builds without any issue.

Factoring the commong prefix:

#+BEGIN_EXAMPLE
SelectFeature: metamodel=[Metamodel] '.' class=[ecore::EClass] rest=SelectFeatureRest;

SelectFeatureRest: '.' features+=Attribute
  | features+=Relation
  | '[' features+=Feature (',' features+=Feature)* ','? ']';
#+END_EXAMPLE

makes completion works.  Sort of.

I actually just implemented the ScopeProvider, and auto-completion appears to
work automatically from the default generated code.

When I type:

: togaf.|

and trigger auto-completion, then I get nothing.  However, if I do:

: togaf.a|

then I get a bunch of classifiers, beginning with 'A'.

If I now put the cursor back:

: togaf.|a

then I get all the classifiers in the TOGAF metamodel.

Auto-completion works for metamodel names, which do not benefit from my own
version of ScopeProvider.

Maybe I need to override the default auto-completion behavior to capture just
these cases?

** Next time                                                       :emfviews:
Make auto-completion work perfectly.

After that, wildcards?  I would need to go back on the weaving model though...

Or take a look at generating a proper ECL file.

* [2017-10-13 ven.]
** Creating a View from a VPDL-generated viewpoint                 :emfviews:
I should just have to create an eview file, give it the viewpoint and three
models, and it should work, right?

First ordeal: relative paths do not work here, although they work in the tests.

Second ordeal: the sample reflexive editor cannot cast VirtualEClass to
EClassifierExtendedMetaData.Holder.

Then VirtualEReference cannot be cast to
... EStructuralFeatureExtendedMetadata.Holder!

After that, it loads.  Hmm except I'm still struggling with paths for loading
the ECL.

Hardcoding it to see what we get...  Yep, the View loads, but there are lots of
unforeseen consequences of filtering.

The editor throws a bunch of UnsupportedOperationException due to things I've
not implemented in VirtualEClass/VirtualFeature...  implementing now.

Strange thing: attributes have no name, because presumably the name feature is
an inherited one, and not filtered in.  Even when filtering in the feature from
the parent class, the name does not appear in the editor picked up.

References to objects cannot be followed, as the objects do not appear if their
containers are not filtered in, and recursively.

I'm not sure if we should solve the problem by including the container
implicitly... No, that's almost always the wrong approach.

Rather, the user should include the attributes themselves.  However, doing that
seems to have no effect.

** Next time                                                       :emfviews:
Resolve the filepath issue with the ECL delegate.

Figure out how the sample Ecore editor determines a name for something?  Because
maybe we could provide our own label editor rather than rely on the default one.

The bigger issue to resolve is: how to make an inherited attribute appear in the
sample Ecore editor?

And/or, how to filter it in using VPDL?

* [2017-10-16 lun.]
** Fixing paths in ECL delegate                                    :emfviews:
So, I've got a string supposedly pointing to a matching model file, as a
'platform:/resource' URI.  Then, I want to open that file using the java.io.File
API.  How hard can that be?

Seems EMF has no issues opening resources when an EMF URI is created using this
platform URI scheme.

Okay, so I got it with:

#+BEGIN_SRC java
IContainer wsroot = ResourcesPlugin.getWorkspace().getRoot();
IFile ifile = wsroot.getFile(new Path(URI.createURI(filePath).toPlatformString(true)));
File f = new File(ifile.getLocationURI());
#+END_SRC

That's a lot of objects, but whatever.

I'm wondering though, it seems this assumes that an EView file will always
contain platform URI?  What if we want to specify any kind of URI, and
especially, ~file:~ directly?

Looking into URIConverter and what EMF does when I try to load a resource from a
relative URI:

: new ResourceSetImpl().getResource(URI.createURI("foo.xmi"), true)

getResource is supposed to look in the already loaded resources for the resource
set and return it.  If not, it creates and loads the resource.

To find the resource, it normalizes the URI using the URIConverter.

I got an ExtensibleURIConverterImpl instance for the resource set used by
Viewpoint.  Here is what it does on a relative URI:

#+BEGIN_SRC java
if (scheme == null)
    {
      if (workspaceRoot != null)
      {
        if (result.hasAbsolutePath())
        {
          result = URI.createPlatformResourceURI(result.toString(), false);
        }
      }
      else
      {
        if (result.hasAbsolutePath())
        {
          result = URI.createURI("file:" + result);
        }
        else
        {
          result = URI.createFileURI(new File(result.toString()).getAbsolutePath());
        }
      }
    }
#+END_SRC

It constructs a file URI only if there is no workspace root, i.e., we are not
running inside Eclipse.  Otherwise, it constructs a platform resource URI, but
/only/ if it's absolute, i.e., begins with ~/~.

If it's relative and you are inside Eclipse... tough luck.  It returns the URI
as-is.

I think the intent is clear: in the Eviewpoint file, if the path is relative, it
is relative to the location of the EViewpoint file on the file system.
Otherwise... let EMF handle it.

Hmm, but if do that, and resolve relative paths in the Eviewpoint and Eview
files with the Viewpoint/View as a base, the test fails.

Because in the tests, I give relative paths for the Viewpoint and View
resources.  And EMF is happy to construct them.  But they can't be used to
resolve other URIs.  In fact, the docstring to Resource.getURI states that the
URI should be hierarchical and absolute.

So maybe I have to fix my tests... but that is very convenient to be able to
pass a project-relative path as URI for the resource.

Easiest fix would be to test if the resource URI is in fact hierarchical and
absolute.  This works without needing to change the tests.

But then, if I use a relative path inside an Eviewpoint, it cannot be resolved.
That's dumb.

So the issue is to find the actual, absolute location of the Resource, and
resolve relative paths from that.  If there is no absolute location for the
resource, then we cannot resolve relative paths.  Simple as that?

** Next time                                                       :emfviews:
Solve out this paths issue.

* [2017-10-17 mar.]
** Constructing a VPDL for a coherent model                        :emfviews:
The current VPDL example I'm using is incoherent: targets of references point to
nowhere, because we haven't included them in the metamodel.

We could include such targets implicitly, as syntactic sugar.  Or we could catch
such errors in the validation phase for the VPDL and have the user write them
out.

But first of all, I should be able to manually build a VPDL that gives us a
coherent view.

First mystery: where are the BPMN instances?  I see ReqIF and TOGAF, but not
BPMN.

Although, there is a strange annotation.  Could that be it?  When I open the
BPMN model, it has custom icons in the TreeViewer... it may be related.

I traced the creation of the labels in the Sample Reflective Editor to
org.eclipse.emf.edit.ui.provider.DecoratingColumLabelProvider.

Which calls org.emf.edit.ui.provider.AdapterFactoryLabelProvider.getText.

With a BPMN object (DefinitionsImpl), it fails to adapt to a label provider, and
just uses toString.  On ReqIF and TOGAF instances, it gets a
ReflectiveItemProvider which prints the EClass name followed by the "label"
feature, which is determined by the following code:

#+BEGIN_SRC java
protected EStructuralFeature getLabelFeature(EClass eClass)
 {
   EAttribute result = null;
   for (EAttribute eAttribute : eClass.getEAllAttributes())
   {
     if (!eAttribute.isMany() && eAttribute.getEType().getInstanceClass() != FeatureMap.Entry.class)
     {
       if ("name".equalsIgnoreCase(eAttribute.getName()))
       {
         result = eAttribute;
         break;
       }
       else if (result == null)
       {
         result = eAttribute;
       }
       else if (eAttribute.getEAttributeType().getInstanceClass() == String.class &&
                result.getEAttributeType().getInstanceClass() != String.class)
       {
         result = eAttribute;
       }
     }
   }
   return result;
 }
#+END_SRC

A "name" feature, or the latest feature of type String, or the latest feature.

When I open the BPMN model in the sample editor, then the adapter factory finds
a DefinitionsLabelProvider.

So, how does that factory works?

In adapt(DefinitionsImpl), the delegateAdapterFactory is
Bpmn2ItemProviderAdapterFactory.  Then it calls this factory to adapt the
DefinitionsImpl into a DefinitionsItemProvider.

Now, when opening the view, the delegateAdapterFactory is still
Bpmn2ItemProviderAdapterFactory.  And this time it returns null.  But I don't
have the code -_-

But I think what's going on is that the adapter factory tests checks for
instances of metamodel objects, and VirtualEObject isn't one of them.

Indeed, when stepping, we end up in the default case:

#+BEGIN_SRC java
public Adapter createEObjectAdapter() {
    return null;
}
#+END_SRC

Since the adapter factory has already been chosen, it won't fall back on the
ReflectiveAdapterFactory, and we have a dumb toString.

So one solution is to force it to use the ReflectiveAdapterFactory.

I tried to add the ReflectiveItemProvider to the adapters of virtual objects,
and it works if I cheat a bit:

#+BEGIN_SRC java
eAdapters().add(new ReflectiveItemProvider(null) {
  @Override
  public boolean isAdapterForType(Object type) {
    return true;
  }
});
#+END_SRC

this forces the ReflectiveItemProvider to be used in lieu of the Bmpn2 adapter.
That way I can load the BPMN model elements just fine.

This is not the proper way to fix this however, because forcing an adapter for
the Ecore editor has nothing to do in the core of EMFViews.

It seems we have to define our own editor for views, which could just use the
stuff that's already implemented and available from EMF.edit.

* [2017-10-18 mer.]
** Virtual objects and switches generated by EMF                   :emfviews:
The trouble with that BPMN item provider seems to stem from the default BPMN
switch implementation which does not recognize virtual objects.

When we fall into the switch, it's because we didn't find any adapter that
matched in the adapter factory.  Then the generated switch implementation is:

#+BEGIN_SRC java
protected T doSwitch(EClass theEClass, EObject theEObject) {
    if (theEClass.eContainer() == modelPackage) {
        return doSwitch(theEClass.getClassifierID(), theEObject);
    } else {
        List<EClass> eSuperTypes = theEClass.getESuperTypes();
        return eSuperTypes.isEmpty() ? defaultCase(theEObject) : doSwitch(eSuperTypes.get(0),
                theEObject);
    }
}
#+END_SRC

The first test fails, because our virtual objects are in a virtual package,
which is not the BPMN2 package.

Somehow, EMF has all these patterns about making things ultra-modular and
flexible, but they use reference equality which we cannot override through
subclassing.

So virtual objects cannot be used by switches in the default implementation,
as-is.  I'm not sure there's a nice way to make them compatible.  Using AspectJ
would work...

Anyway, I think I need to implement our own viewer in any case.  Since hacking
VirtualEObject to work the BPMN2 case is not a good idea.  And I would like to
have instant refresh of views/viewpoints when the resources changes as well.

But for now, I want to fix the tests so that relative paths work.

** Fixing relative paths in tests                                  :emfviews:
And making sure they work with VPDL.

I think the issue is that, when resolving the relative paths with the Viewpoint
URI as base, I'm assuming the Viewpoint URI is absolute.  This is not the case
in the test.  However according to the docstring, this is an abuse on my part.

But since EMF doesn't actually enforces this restriction, maybe the more robust
solution is to find out where the Viewpoint is actually located on the
filesystem (since it has been opened, after all).

But maybe that wouldn't work with other kinds of URIs?

Okaaaay.  I think I got it right this time.  I gave absolute paths to Viewpoint
and View in the tests, and I think I accepted relative paths anywhere relevant.
Still have to write tests though.

Added tests.  Found bugs.  Moving on.

** Next time                                                       :emfviews:
Need to write my own TreeViewer for View/Viewpoint.  We may have some code in
the ui plugin already.

* [2017-10-20 ven.]
** Writing our custom viewer                                       :emfviews:
I've got a basic editor going on.  At least it shows us BPMN instances out of
the box.  No icons, no thrills.

- should reload dynamically when the file changes

I may have found how the BPMN adapter factory registers itself to EMF: the
extension point

: org.eclipse.emf.edit.item.itemProviderAdapterFactories

This may allow us to give an adapter factory for VirtualEObjects.  Need to
check.

Spent a long time figuring out how to get the properties working... for some
reason.  In the end I'm not quite sure what I did wrong.  I fiddled with it
until, somehow, it worked.

** Next time                                                       :emfviews:
Make a coherent VPDL file.

See if we can plug our own adapter factory for VirtualEPackage/VirtualEObject.

* [2017-10-23 lun.]
** Making a coherent VPL                                           :emfviews:
There are indeed a bunch of stuff we have to explicitly include if we want to
see anything in the view.  And finding out what to include is not an easy
process.

An "auto" mode that implicitly includes containers for instance would be great.

I managed to build a coherent VPDL.  There are still some references missing,
but I guess they shouldn't even be in the view to begin with.

** Next time                                                       :emfviews:
Investigate why including one attribute from a parent actually displays /all/
features in the subtypes.

Look into using the adapter factory mechanism for EMF.

* [2017-10-24 mar.]
** First day                                                       :megamart:
*** TEK use case
Ultra-wide band positioning.

Use hardware emulators for testing.  They want to not have to write the
emulators in order to create the tests.

Issue with team collaboration when changing models used by other models.

*** IKER use case
Product: smart warehouse supervision system.

Problems: physical and logical failures.

Use modelling and want to use modelling for code generation, versioning, and
tracking model changes.

*** NOK use case
Product: Nokia Pico Base Station.

Want continuous feedback in their development process.  300-400 persons working
on the same project.

Want: versioning of models (branches, delta view for traceability of features)

Want: Feedback in model, as in integrating runtime data back into models.

Focus on real-time UML modeling.  (UML RT is a UML profile tailored to real-time
and embedded systems).

*** Bombardier use case
Train producer.

Need coordination between different types of trains (electric, high-speed,
local, ...) and over distributed teams.

Current workflow: functional requirements, UML/SysUML models. then test, then
generation to C code, and more tests.  These are done by different teams,
different persons.  Opportunities for loss of signal.

Ideally, they want to refine models between each steps.  E.g., refine software
requirements refined from the functional requirements, rather than writing the
former by eyeballing the latter.

Technos: Simulink, SysML, SIL2.

*** CSY use case
Coppilot platform screen door control.  Screen doors like in the M14 line in
Paris: they are the doors on the train platform, not the doors of the train.

They must open when the train doors open, close when the train doors close, etc.

The safety-critical parts were created with a B model, and are not modifiable
anymore, apparently.

They have troubles with keeping track of logs, especially for network usage.
They want to aggregate logs at runtime whenever nodes are idling.

*** AINA use case
Product: SMS gateway.

Want: methods and tools for run-time analysis, to find out root causes.  (Is
that realstic?)

*** Volvo use case
Specifically: Volvo Construction Equipment branch, which manufactures machines
for construction.

Need traceability from requirements to the runtime, possibly with feedback from
runtime data back to requirements.

Technos: UPDM, SysML, Simulink.

*** Thales
Product: Flight Management System.  This is the system that computes a flight
plan according to localization information gathered by sensors.  This flight
plan can be used by the guidance sub-system for autopilot, or its information
can be used by the human pilot.

Need: earlier verification, reduce risk of costly errors.  (Don't we all?)

Hard requirements:
- every command instructed by the pilot to the system shall have a visual
  display in less than 1s.
- new flight plan should be computed in 3sec or less.

Want: tools that estimate the worst time in a simulation, and report the causes
of deadline misses to the designer.

Technos: Capella

*** Architecture presentation by Andrey
They harvested the requirements and the tool interfaces and put that into
models.  So they can track what kind of format the tool expects, what format
they output, and requirement providers could peruse this info to synchronize.

* [2017-10-25 mer.]
** Second day                                                      :megamart:
*** Hugo: work package 4
On global model & traceability management.

Goal: link WP2 (design time models) and WP3 (run time traces and logs)
together.

Upcoming deadline: architecture report (waiting on WP2 and WP3).

Tools in WP4:
- Modelio Constellation (Softeam)
- EMF Views
- AM3/MoScript (legacy)
- NeoEMF
- JTL (traceability)
- PADRE (anti-patterns detection)
- MDEForge (repository solutions for models)

Interfaces in WP4:
- model storage,
- model versionning,
- model access control,
- model query,
- model cartography,

Still open questions about traces between design-time and run-time models: need
more use info from use cases.

Volvo guy: focus more on methodologies.  They want to use views and viewpoints,
but they need more info?

Guy 1: if we have tools first, then we can build the methodology.

Industry Guy: methodology is more important than the tools.

Guy 1: if we have matched use-cases to tasks first, then we can build the
methodology.

Guy 2: as a company, would you prefer a complete tool, or to change your whole
methodology?

Industry Guy: things will evolve, so we'd rather support a methodology than a
specific tool that would become obsolete, and is more rigid.

Hugo: how do use case providers feel about filling a questionnaire about what
models you use?

Room: [silence]

Hugo: okay so we'll do that.

*** Orlando (ATOS): WP5
They are in charge a making a framework bundling most of the tools.

Principles: we should try to orchestrate version pinning so it's transparent to
the users.

Use loose coupling and asynchronous messaging: interact with publish/subscribe,
message queues rather than direct API calls.

There should be a joint release of tools bundled in the framework at M22
(initial release) and M36 (final release).

Hugo: will the framework include documentation on the processes (methodologies)?

Orlando: no, that's not our responsibility.

Bombardier gal: we are really missing info about processes.  What you should
model, how you should model, how you can validate the model.

Hugo: even if it's not the responsibility of WP5, the info should be reflected
in the framework

[more back and forth]

Orlando agrees to include information about methodologies in the framework

*** Gunnar: WP7 administration stuff
All partners need to sign an agreement of acceptance for joining Artemis.

Gunnar says European money won't come until the PCA (consortium agreement) is
signed by all partners.

For some countries, national funds won't be unblocked before the PCA either.

On the continuous reporting portal, we should:
- report the number and gender of researchers involved
- update any relevant risks
- make sure publications are visible, but don't edit directly the tab (contact
  Clara or Gunnar?)

Planning of next meetings:
- April 2018 in Finland (city not determined yet)
- May 2018 Brussels (first review, not plenary)
- October 2018 France?
- ...rest unclear

Local partners should decide exactly on the location, venue, week, and cost.

Suggestion: draw up a list of probable locations with full costs, and let other
partners take a look.

Gunnar is not convinced.

Clara insists.

** Trying to preserve the state of tree viewer when refreshing     :emfviews:
I got refreshing to work on right-click.  It's already not bad and more reliabe
than the sample editor.

Apparenty using the expand* methods is useless.  But using setExpandedState
works.

The main issue with preserving the open state is that currently I'm rebuilding
the whole resource, and all the virtual objects are re-created anew.  So, how
can we match the new virtual objects to the previous ones?

If we use numbers, then it breaks when a node is added or removed between
updates.  We can't use names, because some nodes don't have any.

So we need to determine an ID that should be preserved between updates.  Note:
we only need nodes that bear children to have consistent IDs.

* [2017-10-26 jeu.]
** Third day                                                       :megamart:
*** Wrap-up
Tool provides should set up meetings/demos with use-case providers.  These
meetings should be synchronized at the work package level.

WP4 task: writing a survey to find out what models are used by use-case
providers, in what amount, etc.  Share with other work packages.

Dragos: also, focus on methods, not just tools.

Bombardier girl & Volvo guy: yes.

* [2017-11-13 lun.]
** Back in the saddle                                              :emfviews:
So I was working on a custom EView editor, for two reasons:

- the sample Ecore editor does not handle refresh from resources graciously
- a custom EView editor could indicate synthetic elements with a different color

Currently, the EView editor has basic functionality.  What to improve:

- the presentation in the "Properties" tab is a bit lacking.  Although it allows
  for expanding references inline.
- One issue in the "properties" tab is that I'm not quite sure to understand why
  some properties are present.  It looks like inherited properties are present,
  sometimes?  Investigate.
- auto-refresh from resource change as an option
- add button for manual refresh from resource

A thorny issue is how to preserve the state of opened nodes when reloading the
resource.  I think it amounts to making a diff of the trees.  But we could get
by with a simple hash function that preserves identity between resource reloads.

Another path I've yet to explore is to use an adapter factory to provide the
sample Ecore editor with our own info on the Eview.  But now that I'm looking at
it again, it seems the sample Editor has no issue opening up a viewpoint or
view.  So this might not be necessary.

Also, test with the Modisco browser to see if it works.

Before working on Eview, my task was to improve the VPDL language.  Still left:

- create a full, coherent VPDL file
- see if we can make containing references implicit
- the ECL part is still untouched
- auto-completion only works after we give one letter

But for the short term, we established that I should first plug the metamodel
extension language to the new weaving model.

After that, if I have time, I should look into the ECL part of the VPDL.

** Plugging the metamodel extension language                   :emfviews:mel:
Generated a new project with the old grammar.  Added a simple example file.
Seems to work.

Now to generate the weaving model with ATL.

Things I noted while sketching out the ATL transformation:

The namespace URI of the contributing metamodels is never provided.  Need to
extend the grammar.

The weaving model supports specializing a virtual concept, but with the current
syntax:

: add class Y specializing uml.C

It's unclear how we could know in the ATL transformation whether the parent
class is virtual or concrete.  Maybe if it doesn't have a prefix, it's virtual?

I should generate an Eviewpoint file alongside the weaving model.

The syntax:

: extending UML:uml, BPMN:bpmn

does not actually tie the metamodel to its prefix in the parsing model (unless
the order of addition in containments references is deterministic).

There's renaming to be done in the grammar.

As I did not want to touch the grammar just yet, but still wanted to generate
the Generalize clause correctly, it resulted in this gem:

#+BEGIN_SRC atl
subConcepts <- s.prefix->iterate(p; tu : OclAny = Tuple{idx=0, concept=Sequence{}} |
            Tuple{idx = tu.idx + 1,
                  concept = tu.concept->append(thisModule.LazyRule(p, s.class.get(tu.idx)))
             }).concept
#+END_SRC

Just to be able to get the corresponding element from the ~s.prefix~ and
~s.class~ lists.  A zip would have been more elegant, but it's not in the
language.  For a one-shot, might as well use an accumulator to populate a list
and keep track of the current index.

Have to fix the grammar.

* [2017-11-14 mar.]
** Working on MEL                                              :emfviews:mel:
On AddProperty, the type probably shouldn't be optional.

I'm not sure what ModifyProperty is for.  The example

: modify property propertyC attribute="name", value="propertyC2"

is not very telling.

Oh wait, I think I get it.  It modifies the name of the propertyC to be
propertyC2 instead.  Ok.

Well, that's a bit too open unfortunately.  We should either fix what we can
modify on an attribute in the language, or remove this construct.  Besides
changing the name, we could change the cardinality, or the type.  Is there
anything else?

Also, this raises the question: what happens to models that use this property?
We should probably update the models to display the value for "propertyC" under
"propertyC2" now.

So, modifying a property would not be functionally equivalent to filter+add in
this case, because the latter would not have this explicit link preserved at the
model level.

* [2017-11-15 mer.]
** Further work on MEL grammar/syntax                          :emfviews:mel:
We could Constraint as annotation?

Reworked the grammar a bit to use namespace URIs.

But now concrete concepts are not unique anymore in the weaving model after the
transformation.

* [2017-11-16 jeu.]
** Setting up new machine                                              :dell:
A Dell Latitude 7480.

First gotcha: Arch does not see the nvme drive when booting in UEFI.  Legacy
mode works fine.

Switching the SATA configuration from RAID to AHCI fixes it (but breaks
windows).

After that, it seems the D6000 universal dock is badly supported.  I followed
the [[https://wiki.archlinux.org/index.php/DisplayLink][wiki]] and installed the displaylink and evdi-git AUR packages.  The display
ports work: the monitors are recognized by xrandr, and I can use both of them.
However, there is unbearable lag on the monitors.

This seems to be a known issue.  I tried switching drivers (without being sure
that I had succeeded), and turning off Vsync (but with glxgears always reporting
that it was on...).  No success so far.

My best bet would probably be to use the HDMI output for one screen, and the
USB dongle for the other HDMI screen.

The dock can still be useful for the ethernet and USB ports.

** MEL progress                                                :emfviews:mel:
Can't specialize and supertype at the same type when creating a new class.  Is
this useful?  I don't know, but the grammar does not support it, although the
weaving model does.

Solving the uniqueness of concrete concepts: this worked when I used a unique
lazy rule with the tuple (metamodel, name) as argument.  From my understanding
of ATL, the fact that it's unique will ensure that each tuple (metamodel, name)
will always produce the same output (concreteConcept).

If I pass a TargetClass instead as argument to the rule, then for different
target class instances which have the /same content/, I get different outputs:

: TargetClass1 (uml, A) -> ConcreteConcept1 (uml, A)
: TargetClass2 (uml, A) -> ConcreteConcept2 (uml, A)

So the lazy rule has to stick to the contents of the target class.  But since
that's a little verbose to call, I added a helper:

#+BEGIN_SRC atl
helper context MEL!TargetClass def : toLink : VirtualLinks!ConcreteConcept =
	thisModule.ConcreteConcept(self.metamodel, self.name)
;

unique lazy rule ConcreteConcept {
  from m : MEL!Metamodel, name: ECORE!EString
  to   t : VirtualLinks!ConcreteConcept (
      model <- m,
      path <- name
  )
}
#+END_SRC

Aaaargh.  Another issue with dupes.  The problem is that I have duplicates of
VirtualConcepts now.  One 'add class A' statement will create a VirtualConcept A
in the transformation, but I can also use that new class as a target for a new
association.  In this case, the association need to refer to the virtual
concept, but not create it.

I think I found a way to solve it by finding the corresponding AddClass
declaration.

Ah yes, it works.  I can only have one rule for creating classes though.  But
that's more flexible that way anyway.

** Next time                                                   :emfviews:mel:
Cardinality for reference.

Tackle modify property.

* [2017-11-17 ven.]
** Another look at the new machine                                     :dell:
*** Second take on the DisplayLink issue                        :displaylink:
: pacaur -S displaylink

Requires evdi>=1.5, but package is evdi 1.3.  Using evdi-git:

: pacaur -S evdi-git

Succeeds after kernel headers are installed.

After that, installing displaylink works.

: systemctl start displaylink


Regarding the display link: noticeable lag using the modesetting driver just by
moving the mouse cursor around and selecting stuff in Firefox.  No perceivable
lag in urxvt.  Small lag doing the same things in Emacs.

Now, playing a video on the screen and the video plays fine but the mouse lag is
pretty bad.

No lag when doing the same thing on the laptop screen.

The whole hub also disconnected and reconnected itself without me touching it.
Though the USB-C connector looks flimsy.

Same lag with PageFlip off.

Also,

: env vblank_mode=0 glxgears

reports ~627FPS on the laptop monitor, and ~510FPS on the displaylink monitor.

Let's try the Intel driver.

glxgears reports ~584FPS on laptop monitor.

Displaylink looks like it refuses to work with the intel driver actually.  That
matches with the info on the wiki (except the suggested workaround does not
work).

Well, so that's it then.  Displaylink driver is garbage for linux.  I'm not
plugging my screens this way.

No noticeable lag when plugging the monitor via VGA using an USB-C dongle (also
with ethernet plugged-in).

*** Disabling the PC speaker
Wow, somehow there is a PC speaker on that thing, and it's really loud.
Blacklisting:

: blacklist pcspkr

in /etc/modprobe.d/nobeep.conf

*** Emacs and hi-dpi                                              :emacs:dpi:
Emacs seems to obey the dpi argument to startx just fine:

: startx -- -dpi 120

Using xrandr also works:

: xrandr --dpi 120

but you have to restart the application to see changes.

Although, the only thing that gets scaled is the text size, not the interface.

Well, the toolbar and menu bar can be scaled with:

: env GDK_SCALE=2

Though the tooltips are off by a large amount.

But the fringe stays the same size, regardless of all these settings.

In any case, even if the fringe scaled, all the regular images inside are
bitmaps, so they would need high-resolution versions to go along.

*** Multiple monitors and hi-dpi                                        :dpi:
Also, I have a (mid) high-DPI laptop screen and regular DPI monitors.  The [[https://wiki.archlinux.org/index.php/HiDPI][wiki]]
suggests to use the scale option of xrandr.  But scaling means a blurry output,
which is also unacceptable.

So I guess when I use the laptop alone I can set the dpi to 144 to be
comfortable (but then I have to switch to a TTF font in Emacs and in the
terminal).  But when plugging the external monitors I should stick to 96 DPI and
forego the laptop screen.

This, unfortunately, is not easy to change dynamically, since some applications
require to be restarted after a DPI change.  And Firefox for instance does not
listen to the X server DPI settings, but to the Xft.dpi resource value.

This period between ubiquitous 96 DPI and ubiquitous 300 DPI has more annoyances
than benefits.
** MEL improvements                                            :emfviews:mel:
Handled cardinality.

Added scoping for class names, so now we get validation and auto-completion for
free.

We could do the same for properties, but I haven't gotten back to modify the
syntax for modify property yet.

Opening the eviewpoint file now... and it seems to work!  I see the new classes,
the new properties.

There is a strange bug though, if I do:

: add class Z supertyping uml.Activity, uml.Action

and later:

: modify class uml.Activity

then loading the viewpoint fails because uml.Activity cannot be found.  When
debugging, I see that it's only the second search for uml.Activity using
EMFViewUtil.findElement that fails to find it.

So not using the same class twice makes the viewpoint loads without issues:

: modify class uml.Behavior

Probably related: the Behavior classifier is nowhere to be found, even though it
was never filtered.  If I write:

: modify class uml.Behavior {
:   filter property propertyB
: }

Then the Behavior classifier is absent.  If I remove the "filter property" line,
then the classifier appears.  Something funky in the filtering algorithm when
the property does not exist.

* [2017-11-20 lun.]
** Fixing the filter property bug                              :emfviews:mel:
Okay so the bug was on the ATL transformation side.  I told the weaving model to
filter the Behavior class, instead of the Behavior.propertyB feature.

It's a bit of a hassle to do in ATL, because I have again to use a unique lazy
rule to ensure that there are no duplicate filters.  But hey, bug fixed.

** About the duplication in the ATL transformation         :emfviews:atl:mel:
I think the pattern I've encountered boils down to this.  The source model has
'add attributes' elements contained in a 'modify class':

- modify class A
  - add attribute P
  - add attribute Q

and the target model needs one element for each 'add attribute', that refer to
the same class A.  So:

#+BEGIN_SRC atl
rule R {
  from s : IN!AddAttribute
  to
  t : OUT!NewAttribute (
    name <- s.name,
    class <- class
  ),

  class : OUT!ReferredClass (
    name <- s.refImmediateComposite().className
  )
}
#+END_SRC

This won't work, because it will generate two instances of 'ReferredClass A'.

Instead, here, we can clearly have to matched rules:

#+BEGIN_SRC atl
rule R {
  from s : IN!ModifyClass
  to   t : OUT!ReferredClass (
    name <- s.className
  )
}

rule R2 {
  from s : IN!AddAttribute
  to   t : OUT!NewAttribute (
    name <- s.name,
    class <- s.refImmediateComposite()
  ),
}
#+END_SRC

No duplicates, as R is matched only once, and the links are taken care of in R2.

So this is not the issue I'm having.  My problem is that the class A can be
referred to in other places, like:

- modify class A
  - add attribute P
  - add attribute Q
- modify class A
  - add attribute R
  - add attribute S

and I want, as a target:

- new attribute P
  - parent -> A
- new attribute Q
  - parent -> A
- new attribute R
  - parent -> A
- new attribute S
  - parent -> A
- referred class A

Now, if I'm using the two-rules solution above, 'ReferredClass A' will appear
twice in the target model, which I don't want.

Instead, I'm doing:

#+BEGIN_SRC atl
rule R {
  from s : IN!AddAttribute
  to   t : OUT!NewAttribute (
    name <- s.name,
    class <- thisModule.R2(s.refImmediateComposite().className)
  ),
}

unique lazy rule R2 {
  from name : ECORE!EString
  to   t : OUT!ReferredClass (
    name <- name
  )
}
#+END_SRC

Now, for each AddAttribute, I have a corresponding NewAttribute, which has the
correct link to the class.  But the class is generated only once, since the rule
is unique.

I'm suggesting that instead of having to use the lazy rule, I keep using the
solution with two matched rules, but somehow I can tell ATL that rule R should
only match for distinct ModifyClass.className.

Asking around, Massimo suggested this:

#+BEGIN_SRC atl
rule R {
  from s : IN!ModifyClass
   (s = s.getRepresentative())
  to   t : OUT!ReferredClass (
    name <- s.className
  )
}

rule R2 {
  from s : IN!AddAttribute
  to   t : OUT!NewAttribute (
    name <- s.name,
    class <- s.refImmediateComposite().getRepresentative()
  ),
}
#+END_SRC

The key part being restricting the match of R using the guard:

: (s = s.getRepresentative())

#+BEGIN_SRC atl
helper context IN!ModifyClass def: getRepresentative(): IN!ModifyClass =
	IN!ModifyClass.allInstances()->select(c | c.className = self.className)->first();
#+END_SRC

** Further MEL improvements                                    :emfviews:mel:
- Multiple inheritance
- Validation of property names
- Implement 'modify property'

Out of scope for now: making sure that we are not adding duplicate properties or
classes.

I thought the command:

: add class B specializing Y and supertyping Z

would work, but it doesn't.  I'm not seeing in the grammar why it shouldn't
work.

Oh rightn.  There's no need for that "and" in there.  Silly me.  Were I of bad
faith, I would say the default parsing error did not help me in locating the
error.

Anyway, multiple inheritance now.  Yep, that works as well.  It even allowed me
to get rid of that ugly condition masking the lack of option type in ATL:

#+BEGIN_SRC diff
-        superConcepts <- if s.parent.oclIsUndefined()
-                        then Sequence{}
-			 else Sequence{s.parent.toLink()}
-			 endif,
+        superConcepts <- s.parents->collect(e | e.toLink()),
#+END_SRC

Fixed a couple of bugs with the EView editor.  Forgot to implement the ID
feature for attributes in VirtualEAttribute.dynamicGet.  This also affected
other viewers, of course.

There is still an issue when wanting to print the value of the eFactoryInstance
attribute for EPackages.  It currently throws as unimplemented, because I still
don't know what to provide, if anything.  I guess we could provide a factory
that does... nothing?  Or throw when you want it to actually create instances?

Hmm so about that modifyProperty clause.  It seems to me the most natural way of
writing it would be so:

#+BEGIN_SRC
modify class bpmn.Activity {
  modify property propertyC {
    name propertyC2
    type X
    cardinality 0..1
    relation type association
  }
}
#+END_SRC

So that it echoes the syntax for modify class.

On the implementation front, since it's supposed to be syntactic sugar for a
filter+add property, I was hoping to be able to desugar this it with Xtext.  Not
sure if there are any default facilities for that.

Otherwise the cleanest approach would be to do one ATL transformation for
desugaring, then one transformation for turning it into a weaving model.

Since it seems wasteful to launch and run two transformations instead of one, I
guess I'll have to translate the sugar myself.

Got validation for property names working instead, since it was simpler.

** Next time                                                   :emfviews:mel:
Finish 'modify property' ATL transformation.

* [2017-11-21 mar.]
** Handle modify property clause in ATL transformation         :emfviews:mel:
So I managed to make a simplification in the grammar for the 'modify property'
clause.  At first I went with:

#+BEGIN_SRC xtext
ModifyProperty:
  'modify' 'property' property=[ecore::EStructuralFeature] '{'
    operators+=ModifyPropertyOperator*
  '}'
;

ModifyPropertyOperator:
  ChangeName | ChangeType | ChangeCardinality | ChangeRelationType
;

ChangeName: newName=ID;
...
#+END_SRC

But then it was a hassle to handle in the ATL transformation, because to get the
value of name to assign to the new property, I had to find the 'ChangeName'
instance, and use its value.

Then I realized that the grammar was wrong, in that it allowed us to write
the nonsensical:

#+BEGIN_EXAMPLE
modify property foo {
  name bar
  name baz
  name null
}
#+END_EXAMPLE

And, the grammar could be more restrictive /and/ be easier to handle as well
using unordered groups:

#+BEGIN_SRC xtext
ModifyProperty:
  'modify' 'property' property=[ecore::EStructuralFeature] '{'
    (('name' newName=ID)?
     & ('type' type=Type)?
     & ('cardinality' cardinality=Cardinality)?
     & ('relation' 'type' relationType=RelationType)?)
  '}'
;
#+END_SRC

The extra parenthesis ensures the unordered group constraint only applies inside
it (and does not mess with what's around the curly brackets).  And, I don't have
to dig in order to get the values in ATL.  Although... another issue arises.

The point of having clauses inside 'modify property' be optional, is that you
shouldn't have to repeat what is already there.  In this case, if I want to
rename a property, I should just have to say:

#+BEGIN_EXAMPLE
modify class A {
  modify property foo {
    name bar
  }
}
#+END_EXAMPLE

Conceptually, this is syntactic sugar for a filter + add property:

#+BEGIN_EXAMPLE
modify class A {
  filter property foo
  add property bar [..]
}
#+END_EXAMPLE

And this is done the ATL transformation.  But the devil is in the elision:
`[..]` should specify the type and cardinality of the new property.  These
values are known: whatever was already present for the property A.foo in the
metamodel.  Except the ATL transformation does not have knowledge of the content
of this property.

But maybe it could.  The ATL transformation can accept multiple sources.  So we
could, e.g., add an ECORE input model, and navigate it to populate the missing
entries in 'modify property'.

Ah, but Hugo raises a good objection: What happens if you use 'modify property'
on different metamodels?  Your ATL transformation cannot handle N varying
inputs, unless you first generate the transformation from... another
(higher-order) transformation?  And you still need to sort out the
correspondence between the input models and the modify property clauses.

So, maybe the desugaring phase is the simplest solution after all.  If I find a
'modify property' clause, I expand it into 'filter + add' and navigate the
metamodels to find the default values.

Aaaah, but I'm dumb.  I realized that we already had a reference to an EStructuralFeature
in the grammar, so I don't need to navigate anything: I just look up its
attributes to populate the default values in a modify property.

One remaining issue with the 'modify property' clause, is that the grammar
potentially allows you to turn an attribute into a reference and vice-versa.  If
we allow that semantically, then it's a pain to handle in the transformation.

But, we agreed with Hugo that it seems unuintuitive to allow changing the nature
of a feature with 'modify property'.  It's fine for small changes like the name,
the type, or cardinality.  But if you want to change its nature, then you can
always add a new property/association, and hide the previous one (if you want
to).

So I need to change the grammar to reflect this distinction, and disallow
invalid programs.

** At some point                                          :emfviews:mel:vpdl:
I'll have to Write tests for both languages.  Test valid programs, invalid ones,
and also test the generator.

I'm not sure it would have sped up prototyping, since the syntax is fluid at
this point.

But once we design starts to gel, it's time to make sure the basic and corner
cases work.

** Next time                                                   :emfviews:mel:
I've left the cardinality to handle tomorrow.  Maybe I can update the grammar
here as well to forbid using an unbounded one for attributes.

* [2017-11-22 mer.]
** Finishing MEL cardinality                                   :emfviews:mel:
We assumed that attributes could only be typed using primitive types, but it is
not the case in the UML metamodel.  uml.Behavior.isReentrant has type Boolean,
which is part of an EENum in the same metamodel.

And I see that it's perfectaly valid in Ecore, to define an EEnum, and use that
as the type for an EAttribute.  It makes me think that we could revisit the
weaving model to use EDataType as the type for virtual attributes, rather than
strings.  It would make the model less prone to errors, and would get rid of
dumb code for the translation.

Same thing for using EClass in ConcreteClass, rather than this messy business of
finding elements using a fully-qualified name.  I believe the reasoning against
that is that we didn't want to tie the weaving model to EMF.  But it would only
tie it to /Ecore/, which is different.

Actually, it maeks me wonder why we don't also link metamodels directly in the
Xtext grammar for MEL/VPDL.

Another solution might be to include Ecore as contributing model in the weaving
model, and refer to the EDataType as concrete elements.  But we'd still need to
update the weaving model VirtualProperty to accept a different type.

Handled cardinality.

The type issue is still open: if we redefine uml.Behavior.isReentrant without
specifying the type, we try to reuse the Boolean enum, and we have no way to
instruct ATL or the weaving model to use that.  That's just not something we
have considered.

** Generating ECL from VPDL                                   :emfviews:vpdl:
If there is an ECL Xtext grammar fragment, then that's great.  If not, I can
just put the ECL expression in a string for the time being.

Ah, [[https://git.eclipse.org/c/epsilon/org.eclipse.epsilon.git/tree/plugins/org.eclipse.epsilon.ecl.engine/src/org/eclipse/epsilon/ecl/parse/Ecl.g][it looks like]] they are using ANTLR for their grammar.  So... too bad.
String it is.

Also, from the 'How to' page:

#+BEGIN_QUOTE
Epsilon languages do not have Ecore-based metamodels.
#+END_QUOTE

So I can't generate the concrete syntax from a model.  Need generate text
directly, or maybe I can build an in-memory representation of the AST, and
pretty-print that.

Okay so I did the minimal incremental functionality: pass strings in the 'where'
clause of the VPDL directly to ECL.  So now we write:

#+BEGIN_SRC vpdl
where "s.name=t.name and
       s.isAutomated = false"
      for detailedProcess
      "t.values.exists(v | v.theValue=s.name)"
      for detailedRequirement
#+END_SRC

And it generates:

#+BEGIN_SRC ecl
//alias_togaf=http://www.obeonetwork.org/dsl/togaf/contentfwk/9.0.0
//alias_bpmn=http://www.omg.org/spec/BPMN/20100524/MODEL-XMI
//alias_reqif=http://www.omg.org/spec/ReqIF/20110401/reqif.xsd

rule detailedProcess
match s : togaf!Process
with  t : bpmn!Process
{
  compare
  {
    return s.name=t.name and
           s.isAutomated = false;
  }
}
rule detailedRequirement
match s : togaf!Requirement
with  t : reqif!SpecObject
{
  compare
  {
    return t.values.exists(v | v.theValue=s.name);
  }
}
#+END_SRC

I'm pretty sure it's not composable, and is very prone to breaking.  Discovering
ECL syntax errors only when launching the view is very bad, but it's still
better than what we had until then.

(But I'm not sure it's better than letting the user write the ECL file by
hand...)

One observation: from the look of it, I'm not really sure why we need the
metamodels in the header.  We can certainly pass them to the ECL delegate, so
they really don't need to appear here.

* [2017-11-23 jeu.]
** Comparison of ATL VM performance                                     :atl:
Specifically, if there is a significant speedup when running a transformation
written in Java, versus running a transformation using the ATL VM or EMFTVM.  If
the speedup is significant, then it could valuable to compile ATL
transformations directly to Java code (and rather interesting to boot).

So, premilimary results on a really dumb transformation:

#+BEGIN_SRC atl
rule Metamodel {
    from s : MEL!Metamodel
    to   t : VirtualLinks!ContributingModel
}
#+END_SRC

The source model has 1000 Metamodel elements.  Using EMFTVM: 65.79ms to load the
bytecode, 57.45ms to execute the transformation, 123.24ms total.

Using Java to populate the resource: 7.32ms.  Not bad.

Let's get some more numbers (time in milliseconds):

| EMFTVM (load) | EMFTVM (run) | EMFTVM (total) | Java |
|---------------+--------------+----------------+------|
|         63.67 |        56.09 |         119.76 | 7.27 |
|         70.92 |         76.5 |         147.42 | 5.69 |
|         69.96 |        66.06 |         136.02 | 7.27 |
|         59.77 |        56.11 |         115.88 | 9.88 |
|         62.34 |        55.92 |         118.26 | 7.47 |
|         67.10 |        64.89 |         131.99 | 7.20 |
|         70.77 |        60.79 |         131.56 | 7.80 |
|         63.31 |        58.04 |         121.35 | 7.66 |
|         65.82 |        59.61 |         125.43 | 7.33 |
|         65.99 |        74.26 |         140.25 | 5.99 |
|---------------+--------------+----------------+------|
|         65.97 |        62.83 |         128.79 | 7.36 |
|          3.78 |         7.51 |          10.40 | 1.12 |
#+TBLFM: $2@=$3-$1::@12=vmean(@I..II);%.2f::@13=vsdev(@I..II);%.2f

So the average speedup of the Java transformation is:

: 128.79 / 7.36 = ~17.5

That's one order of magnitude and more.  Pretty good.  Even if you only count
the transformation time, it's still a 8.54 speedup.

Now another question is: how does the performance scale with the number of
elements model size?

Time still in milliseconds:

| Nb of elements | EMFTVM (total) |    Java |
|----------------+----------------+---------|
| 1k             |         128.79 |    7.36 |
| 10k            |         229.62 |   21.16 |
| 100k           |         778.40 |  118.61 |
| 1000k          |        4968.08 |  330.65 |
| 10000k         |              - | 2833.60 |

EMFTVM crashed with OutOfMemoryError after a few minutes of burning all my CPU
cores.  The performance dip at 1 million elements is also strange; it seems to
exhibit non-linear behavior here.  Maybe most of the time is spent in GC; I
would need to profile to see what's happening exactly.

The numbers are also a bit raw.  Getting more samples:

|        | EMFTVM (total) |    Java |
|--------+----------------+---------|
| 10k    |         229.62 |   21.16 |
|        |         247.58 |   18.31 |
|        |         256.83 |   19.78 |
|        |         241.50 |   19.24 |
|        |         287.65 |   14.61 |
|        |         255.47 |   15.12 |
|        |         229.89 |   16.11 |
|        |         262.84 |   16.30 |
|        |         228.45 |   15.80 |
|        |         253.97 |   14.91 |
|--------+----------------+---------|
| Mean   |         249.38 |   17.13 |
| Stdev  |          18.36 |    2.31 |
|--------+----------------+---------|
| 100k   |         778.40 |  118.61 |
|        |         654.89 |  129.84 |
|        |         662.66 |  112.93 |
|        |         703.02 |  116.72 |
|        |         661.14 |  130.02 |
|        |         654.88 |   98.90 |
|        |         672.87 |   69.80 |
|        |         686.44 |   90.11 |
|        |         732.92 |   89.95 |
|        |         676.96 |  100.69 |
|--------+----------------+---------|
| Mean   |         688.42 |  105.76 |
| Stdev  |          39.89 |   19.34 |
|--------+----------------+---------|
| 1000k  |        4968.08 |  330.65 |
|        |        4328.87 |  308.59 |
|        |        4413.08 |  325.68 |
|        |        4395.61 |  312.63 |
|        |        4754.49 |  321.02 |
|        |        4729.85 |  316.54 |
|        |        4439.81 |  355.46 |
|        |        4213.97 |  327.41 |
|        |        4391.01 |  333.40 |
|        |        4377.02 |  326.70 |
|--------+----------------+---------|
| Mean   |        4501.18 |  325.81 |
| Stdev  |         234.96 |   13.09 |
|--------+----------------+---------|
| 10000k |                | 2833.60 |
|        |                | 4371.31 |
|        |                | 5404.33 |
|        |                | 4417.67 |
|        |                | 4577.32 |
|--------+----------------+---------|
| Mean   |           0.00 | 4320.85 |
| Stdev  |           0.00 |  930.53 |
#+TBLFM: @12$2..@12$3=vmean(@I..II);%.2f::@13$2..@13$3=vsdev(@I..II);%.2f
#+TBLFM: @24$2..@24$3=vmean(@14..@23);%.2f::@25$2..@25$3=vsdev(@14..@23);%.2f
#+TBLFM: @36$2..@36$3=vmean(@26..@35);%.2f::@37$2..@37$3=vsdev(@26..@35);%.2f
#+TBLFM: @43$2..@43$3=vmean(@38..@42);%.2f::@44$2..@44$3=vsdev(@38..@42);%.2f

So, to recap:

#+tblname: recap
| Nb of elements (k) | EMFTVM (total) |    Java | Speedup |
|--------------------+----------------+---------+---------|
|                  1 |         128.79 |    7.36 |   17.50 |
|                 10 |         249.38 |   17.13 |   14.56 |
|                100 |         688.42 |  105.76 |    6.51 |
|               1000 |        4501.18 |  325.81 |   13.82 |
|              10000 |              - | 4320.85 |         |
#+TBLFM: $4=$2/$3;%.2f

#+begin_src gnuplot :var data=recap :file doc/atl-bench.png
set logscale x

set xlabel 'model size (k)'
set ylabel 'time to transform (ms)'

plot data u 1:2 w lp title 'EMFTVM', \
     data u 1:3 w lp title 'Java'
#+end_src

#+RESULTS:
[[file:doc/atl-bench.png]]

So, both curves seem to have the same profile, although Java is clearly faster
all along.  The spikes at the end may come from the GC dominating, but I haven't
profiled yet.

This is with ~java -version~:

: openjdk version "1.8.0_151"

Anyway, interesting!  There is definitely something to be gained by translating
transformations to Java from the look of it.

I probably need more tests for less trivial transformations, and to profile
both, in order to understand where the time is spent.

Also, I could add the regular ATL VM to the mix.

** Summarizing what's left for EMFViews when I come back           :emfviews:
*** EMFViews
- Resolve filtering interference with inheritance, references, etc.
  Interference with metamodel coherence.
- Resolve the data type conundrum
- EFactory for viewpoints/views.  Do we want one?  Do we return null?
- UI and Editor plugin are still broken.  Do we want wizards?

*** VPDL/MEL
- Add parser tests for valid programs, invalid ones
- Add integration tests from the output of the generator
- Add static checks for things that are not caught by the grammar?
- Properly integrate ECL in VPDL

*** Deployment
- Continuous integration on Travis
- Provide an easy way to use EMFViews outside of Eclipse (maven central?)
- Start using version numbers
- Cleanup copyright (esp. for VPDL and MEL)
- Upgrade to EPL 2.0

*** Documentation
- Add Javadoc on EMFVIews core
- Cleanup examples.  Remove outdated one, update the ones that can be salvaged.
- Update or remove videos
- Add tutorial for setting up
- Repopulate website
- [X] Cleanup wiki

* [2017-11-24 ven.]
** Screen backlight on Latitude 7480                                   :dell:
For the brightness key to work, it seems I need to use the intel graphics driver
after all.  No kernel option needed after that.

* [2017-12-01 ven.]
** The search for a new wiki                             :wiki:
I've narrowed down the list to:

- Gollum (the thing Github uses).  Ruby.
- Gitit.  Haskell.
- ikwiki.  ??

All of these support multiple editing formats, web-based editing as well as
direct file editing, use Git to store revisions, and can be self-hosted.

Gollum is not free software though.

ikwiki looked rather minimal, even though a handful of plugins seem provided
with the base install.

I opted to try Gitit first since there's a Github OAuth example on the README,
which is already promising.

Trying to install with cabal...

: pacman -S ghc cabal-install
: cabal update
: cabal install gitit

warns me about dependencies that would break existing installs, notably of ghc.
Hmm, let's try stack then.  It's a folder-local install, so it should be more
robust?

: pacman -S stack
: git clone https://github.com/jgm/gitit.git
: cd gitit
: stack install

I didn't checkout any tag; building straight from master.  Looks like stack is
installing GHC 9.3 or something?

Fails with:

#+BEGIN_EXAMPLE
Progress: 13/133
--  While building package digest-0.0.1.2 using:
      /home/fmdkdd/.stack/setup-exe-cache/x86_64-linux-tinfo6-nopie/Cabal-simple_mPHDZzAJ_1.24.2.0_ghc-8.0.2 --builddir=.stack-work/dist/x86_64-linux-tinfo6-nopie/Cabal-1.24.2.0 build --ghc-options " -ddump-hi -ddump-to-file"
    Process exited with code: ExitFailure 1
    Logs have been written to: .stack-work/logs/digest-0.0.1.2.log

    Configuring digest-0.0.1.2...
    Building digest-0.0.1.2...
    Preprocessing library digest-0.0.1.2...
    /usr/bin/ld: .stack-work/dist/x86_64-linux-tinfo6-nopie/Cabal-1.24.2.0/build/Data/Digest/CRC32_hsc_make.o: relocation R_X86_64_32 against `.rodata' can not be used when making a shared object; recompile with -fPIC
    /usr/bin/ld: final link failed: Nonrepresentable section on output
    collect2: error: ld returned 1 exit status
    linking .stack-work/dist/x86_64-linux-tinfo6-nopie/Cabal-1.24.2.0/build/Data/Digest/CRC32_hsc_make.o failed (exit code 1)
#+END_EXAMPLE

Trying with:

: git checkout 0.12.2.1

same thing.  Grmbl.

I'll let cabal do its thing then.

: cabal install --force-reinstalls gitit

#+BEGIN_EXAMPLE
cabal: Leaving directory '/tmp/cabal-tmp-14769/byteable-0.1.1'
cabal: Error: some packages failed to install:
Diff-0.3.4-9Jk7nn49ORI32Gqvs1rQQi failed during the building phase. The
exception was:
ExitFailure 1
HStringTemplate-0.8.6-4urbpt7EoCN94ZLHEgcUuE depends on HStringTemplate-0.8.6
which failed to install.
SHA-1.6.4.2-BFIBO7i65E6HsO6Fy2Jbdf failed during the building phase. The
exception was:
ExitFailure 1
...
#+END_EXAMPLE

Come on...

There is a recipe in AUR.  I guess I'll try that.

: pacaur -S gitit

#+BEGIN_EXAMPLE
AUR Packages  (2) ghc-pristine-8.2.1-4  gitit-0.12.2.1-3
Repo Packages (3) ghc-8.2.1-3  ghc-libs-8.2.1-3  ghc-static-8.2.1-3

Repo Download Size:     85.86 MiB
Repo Installed Size:  1332.75 MiB
#+END_EXAMPLE

That's a lot of GHCs.

#+BEGIN_EXAMPLE
cabal: The following packages are likely to be broken by the reinstalls:
process-1.6.1.0
haskeline-0.7.4.0
ghc-8.2.1
Cabal-2.0.0.2
ghci-8.2.1
directory-1.3.0.2
hpc-0.6.0.3
ghc-boot-8.2.1
Use --force-reinstalls if you want to install anyway.
==> ERROR: A failure occurred in build().
    Aborting...
:: failed to build gitit package(s)
#+END_EXAMPLE

Oh well.

But what's this ghc-static anyway?  Looking at the Archwiki on Haskell, it looks
like the packaged GHC does not support dynamic linking.

So, retrying:

: pacaur -S ghc-pristine
: pacman -S cabal-install

But the wiki also says I also need ghc-static...

: pacman -S ghc-static

Cabal still bugs me about dependencies and --force-reinstalls... ok let's try
it.

Seems to go further in the build now.

After a few minutes, the install fails because pandoc failed to build.

The last resort is to install the Haskell platform, and retry the build from
there (or from source).

Hahahaha

#+BEGIN_EXAMPLE
# ./install-haskell-platform.sh
Unpacking ./hp-usr-local.tar.gz to /...
Running /usr/local/haskell/ghc-8.2.1-x86_64/bin/activate-hs ...
#+END_EXAMPLE

Returned with status 127.

: ghc --version

#+BEGIN_EXAMPLE
/usr/local/haskell/ghc-8.2.1-x86_64/lib/ghc-8.2.1/bin/ghc: error while loading shared libraries: libtinfo.so.5: cannot open shared object file: No such file or directory
#+END_EXAMPLE

* [2017-12-02 sam.]
** Gitit: part 2                                                       :wiki:
Okay, last chance.  It seems gitit is packaged in Debian, so it should be fine
to run on the Atlanmod server.

I see there are Docker recipes, so maybe I can try that?

Trying this one: https://hub.docker.com/r/marcelhuberfoo/docker-gitit/

: pacman -S docker
: docker pull marcelhuberfoo/docker-gitit
: sudo docker run -d --name gitit -e GIT_COMMITTER_NAME="Test User" \
:                                 -e GIT_COMMITTER_EMAIL="test@domain.com" \
:                 -p 60000:5001 marcelhuberfoo/docker-gitit

Oh hey, look at that.  Browsing to localhost:60000, and it works!

Now, it seems we still have a bunch of pages in the MediaWiki syntax, and Gitit
doesn't support it apparently.  I misread: it supports /exporting/ to MediaWiki,
which is decidedly not the same thing.

Well, ultimately, I just want to make sure our mediaWiki won't get hacked.
Maybe just using OAuth there would do?

Seems [[https://www.mediawiki.org/wiki/Extension:OAuth][there is one]].  Not sure how it applies to us though.x

* [2018-02-01 jeu.]
** Back in the saddle                                      :emfviews:eclipse:
One of my first order of business would be to tackle some infrastructure.  I
want to be able to ~make~ the project, distribute it, test it, all on the
command-line so I can use Travis, and the build is reproducible.

Eclipse launch configurations drive me mad with the amount of hand
configuration.  You'd think it would be enough to add the plugins you want to
run, and click "Add Required Plug-ins", but no!  You get a mystifying error:

: Unresolved requirement: Require-Capability: osgi.extender; filter:="(&(osgi.extender=osgi.component)(version>=1.2)(!(version>=2.0)))"

[[https://bugs.eclipse.org/bugs/show_bug.cgi?id=494913][The solution]] is to add the ~org.eclipse.equinox.ds~ plugin, and its required
plug-ins again.

* [2018-02-05 lun.]
** Cleaning up dependencies                                :eclipse:emfviews:
Turns out we have dependencies in plugins that are not even used.  Eclipse
helpfully provides a "Find unused dependencies" tool, but why is that not
automatic?

I've tried to use Import-Package instead of Require-Bundle.  You have to version
packages independently, and for some reason EMF tends to break if we only import
packages.  So, at least for EMF, it's better to include bundles.  Also, most
Eclipse packages do not export package versions, so...

** Fixing copyrights                                               :emfviews:
We should upgrade to EPL-2.0, as it can be made compatible with GPL via the
Secondary License clause.

[[https://www.eclipse.org/legal/epl-2.0/faq.php#h.2487uaelxj1l][The FAQ]] is very helpful in answering tricky questions, and providing clear
examples of how to do that.

Here is the template I've settled onto:

#+BEGIN_SRC java
/*******************************************************************************
 * Copyright (c) 2018 Armines
 *
 * This program and the accompanying materials are made available under the
 * terms of the Eclipse Public License 2.0 which is available at
 * https://www.eclipse.org/legal/epl-2.0/
 *
 * This Source Code may also be made available under the following Secondary
 * Licenses when the conditions for such availability set forth in the Eclipse
 * Public License, v. 2.0 are satisfied: GNU General Public License, version 3
 * which is available at https://www.gnu.org/licenses/gpl-3.0.txt
 *
 * Contributors:
 *   fmdkdd - initial API and implementation
 *******************************************************************************/
#+END_SRC

** Cool trick with dired                                              :emacs:
To replace text in multiple files:

- find-name-dired
- Press 't' to select all files
- dired-do-query-replace-regexp

Interactive search/replace in all selected files!

* [2018-02-06 mar.]
** Tycho + Xtext                                       :maven:xtext:emfviews:
Just copying the POM from [[https://github.com/xtext/maven-xtext-example][this example project]] works.  The [[https://www.eclipse.org/Xtext/documentation/350_continuous_integration.html][documentation]] is also
helpful.

There are still some rough edges: it's very slow (maven queries repositories
every time...), and it regenerates the DSL plugins every time, but at least it's
reproducible!

* [2018-02-07 mer.]
** Skipping DSL generation from grammar if the grammar hasn't changed :maven:xtext:
Because currently, if I run `mvn integration-test`, it will regenerate all the
DSL plugins...

If I RTFM'd correctly, `mvn integration-test` will run all phases up to
(including) `integration-test`, and that includes `generate-sources`, which is
the phase where we declared that MWE2 generation should happen.  So, it makes
sense that the MWE2 generation happens every time.

Now, is there a way to conditionally exec this generation?

It doesn't seem that like this is configurable.  And maven doesn't support
arbitrary conditional execution either.

I could write a script that would conditionnally execute the MWE2... but a
simpler solution would be to /not/ bind that execution to the default life
cycle.

I've added a flag, so I can do:

: mvn -o -Dmwe2-skip-generate=true verify

and not regenerate the plugins from the grammar.  It's faster, but honestly, the
only downside of regenerating aside from speed, is touching the dependencies in
the manifest files.

* [2018-02-08 jeu.]
** Running integration tests for DSLs on Travis          :emfviews:atl:maven:
I've written the tests that use the generator.  They pass locally, but not on
travis yet because we have not instructed the maven build to compile ATL files!

I found [[https://wiki.eclipse.org/ATL/User_Guide_-_Building_ATL_With_ANT_And_Maven][this guide]] that use an ANT task.  However, the code is obsolete.

Instead, I think it wouldn't be too hard to write a simple Maven plugin that
discovers ATL files and does whatever the Eclipse ATL builder does.

For inspiration: [[https://github.com/eclipse/xtext-xtend/blob/master/org.eclipse.xtend.maven.plugin/][Xtend maven plugin]], [[http://git.eclipse.org/c/mmt/org.eclipse.atl.git/tree/plugins/org.eclipse.m2m.atl.adt/src/org/eclipse/m2m/atl/adt/AtlBuildVisitor.java][ATL Eclipse builder]], [[http://git.eclipse.org/c/mmt/org.eclipse.atl.git/tree/plugins/org.eclipse.m2m.atl.engine/src/org/eclipse/m2m/atl/engine/compiler/AtlCompiler.java][ATL compiler]].

Hmm, one issue is that AtlCompiler requires the eclipse.core plugins to run.
Calling ~AtlCompiler.compile~ first fails because maven has to find the ATL
dependency.

I found [[https://repo.eclipse.org/content/groups/atl/org/eclipse/m2m/atl/org.eclipse.m2m.atl.engine/][this maven ATL repository]].  It only contains snapshot releases, but it's
an official one.

Adding that as a repository and declaring the dependency, I can't get the maven
plugin to compile, because:

: cannot access org.eclipse.core.resources.IFile
: [ERROR]   class file for org.eclipse.core.resources.IFile not found

So I should add org.eclipse.core.resources as dependency... but I don't have a
Maven repository for that!  Oh wait, they are on maven central.  Strange that
the ATL maven artifact does not declare them as dependencies...

Adding ecore as dependency, which is also on central... Haha!  It compiles!

But explodes when launching the plugin goal:

: [ERROR] Failed to execute goal fr.atlanmod.atl:maven-plugin:0.1-SNAPSHOT:compile (default-cli) on project atl-maven-plugin: Execution default-cli of goal fr.atlanmod.atl:maven-plugin:0.1-SNAPSHOT:compile failed: A required class was missing while executing fr.atlanmod.atl:maven-plugin:0.1-SNAPSHOT:compile: org/eclipse/emf/common/notify/Notifier

Can I add it to the runtime dependencies?  Yep.  Adding them one by one...

Antlr dependency is tricky, as 3.5 fails with a MethodNotFound.  Presumably some
method was removed (breaking API, without increasing major version!).  Testing
with 3.4, 3.3... Ah 3.3 raises an exception.  3.2, 3.1, same.  3.0... success!

Now, unfortunately, that generated an ASM, because EMFTVM was not recognized.
Can I make it work?

Including emftvm.compiler as dependency is not enough.  Probably the detection
that ATL does is not fooled so easily.

It seems ATL uses the extension point to detect installed compilers...  but I'm
not even running an Eclipse instance here.  Can I get the compiler directly?

I can call AtlToEmftvmCompiler, but it ultimately fails with:

: Failed ATL compilation: Error during module loading: Module ATLWFR not found

Even though these transformations are included in the emftvm.compiler JAR, which
is a runtime dependency.

Argh, wait, I can't declare the dependency twice.  But how do I declare a
dependency that's required at compile time /and/ runtime?

Nevermind, it doesn't seem to make a difference.

However, testing the code used by EMFTVM to load the resources in the plugin, I
can see that it uses a "platform:" EMF URI, but I am not running Eclipse!  So we
end up with:

: Caused by: java.net.MalformedURLException: unknown protocol: platform

Solutions?  Override the AtlEmftvmCompiler class to not use platform URIs?
Otherwise, maybe use Tycho to build the maven plugin.. but I really don't see
how that would work.

* [2018-02-09 ven.]
** ATL Maven plugin                                      :atl:maven:emfviews:
After tinkering with class.getResource() for over an hour, I can't get to the
files inside the ~transformations~ folder, even though they are in the JAR.
Presumably, they are not on the classpath.

Other way: just grab these damn files and put them in the plugin.  Doesn't
work.

Maybe the issue is that the module resolver can't get files from inside a JAR?

I give up.  I've tried building the plugin using Tycho... that fails when
running the goal.  Tycho doesn't want to build the ~maven-plugin~ packaging
anyway, so I tried to build using the ~eclipse-plugin~ packaging, then switching
to ~maven-plugin~ for generating the descriptor and installing.  Or the other
way around.

Anyway, it doesn't work.

I've gone back to cleaning up the POM and using the straight ~maven-plugin~
approach, albeit with all the dependencies added because ATL dependencies are
not transitive.

I've added the logic to walk the project directory to find ATL files.  It was
surprisingly painful.  Not because of Java: Java 8 has a nice ~Files.walk~ that
outputs a stream, so it's rather easy to use.  Although, you can't put methods
that can throw exception in a ~forEach~, so it's not as nice as it could be.
But Maven was tricky to build with Java 8.

The descriptor plugin failed with a mysterious error code when I built the
plugin with the source set to 1.8.  Upgrading the descriptor plugin fixed it.

And I added logic to display errors.  It's not very ergonomic, but at least it's
informative if the build ever fails on Travis.

That was... more work than I would've liked to build that on Travis.  And I
still need to adapt the Xtend generator code to use the standard ATL VM rather
than EMFTVM.

* [2018-02-12 lun.]
** Converting the Mediawiki into a Gitlab/Gollum wiki                  :wiki:
Our mediawiki is a security liability.  We are moving all internal pages to a
private gitlab repository instead.

Using pandoc, we can convert pages to Markdown automatically:

: pandoc --columns=80 -f mediawiki -t gfm file.mediawiki > file.md

We also had a bunch of files uploaded to the mediawiki.  There is a list
containing all files.  From that, I can get the URLs with this JS code in the
console:

: $$('table a').filter(a => a.text === 'file').map(a => a.href)

In Firefox, right-click the result, "Copy object", and you've got a JSON array
of URLs to fetch the files from.  Clean up the file to have one URL per line
without quotes, and just invoke:

: wget -i urlfile -P dir

Done.

* [2018-02-13 mar.]
** Running a second Eclipse instance                       :eclipse:emfviews:
This has been broken ever since I got back it seems.

I really don't want to re-install Eclipse or wipe my configuration, as there is
no easy way to re-use an parts existing configuration, as far as I can tell...

Anyway, loading all plugins in the runtime Eclipse works.  But of course I don't
want to do that.

Adding "required plugins" isn't enough: I need to /at least/ add ~equinox.ds~.
But that triggers a runtime error saying that the ~ide.worbkench~ application
cannot be found.  Adding that one still triggers the error.

I went the other way around: add everything, remove features that are useless.
Add "required plugins" just in case.  Good to go.

** Improving DSL tests                                       :xtext:emfviews:
I wanted to use the ValidationTestHelper to go further than just testing the
parsing.  Even though we have no custom validation rules, we do have custom
scoping rules, and I wanted to see if we could test that.

Turns out, the ValidationTestHelper does not take scoping into account!
Checking through the debugger, we never go through our custom scope provider.

I don't think we need to turn that into an UI integration test.  Maybe we need
to call the scope provider explicitly.

Anyway, I found out that our tests were also a bit fishy: not checking for
correct parsing before running the generator was a bad idea.

After struggling with obscure ATL errors, it turned out that I was not loading
the UML2 plugin.  Added that as dependency, and all is fine.  That's why I
always use the "required plugins" button: it catches that stuff before the CI!

Strangely though, our MEL tests also use the UML2 metamodel.  But they seem to
run fine without it being loaded at runtime?

* [2018-02-14 mer.]
** Thinking about a S-expr syntax for writing tests on EMF models :emfviews:emf:
For the MEL generator test for instance, it makes no sense to test against the
resulting XMI.  It's not readable.

Creating an expected WeavingModel using the EMF-generated factories is even more
verbose.

I think something like this would work:

#+BEGIN_SRC elisp
(WeavingModel
 :name "extension1"
 :virtualLinks [(VirtualConcept :name "X" :superConcepts #1#)]
 :contributingModels
   [(ContributingModel
     :uri "http://www.eclipse.org/uml2/5.0.0/UML"
     :contreteElements [#1=(ConcreteConcept :path "Class")])])
#+END_SRC

Since we can have cycles, I've found this nifty ~#1#~ syntax on [[http://wiki.c2.com/?EssExpressions][the wiki]].

I don't think we can easily translate to XMI, because there are tricks used in
EMF at least to reuse tags when they contain a single child.  So the mapping is
not 1-1.

However, if we turn that into calls to the reflective EMF API (or even the
generated one):

#+BEGIN_SRC java
val f = VirtualLinksFactory.eINSTANCE
val wm0 = f.createWeavingModel()
val vc0 = f.createVirtualConcept()
val cm0 = f.createContributingModel()
val cc0 = f.createConcreteConcept()

wm0.setName("extension1")
wm0.getVirtualLinks().add(vc0)
wm0.getContributingModels().add(cm0)

vc0.setName("X")
vc0.getSuperConcepts().add(cc0)

cm0.setURI("http://www.eclipse.org/uml2/5.0/UML")
cm0.getConcreteElements().add(cc0)

cc0.setPath("Class")
#+END_SRC

Create all objects first to account for eventual cycles.  Then just set
properties.  We cannot set lists outright in EMF (maybe with the reflective
API?), but since we know whether we have a list dynamically... there is no
ambiguity.

Now the question is: do I want to use Xtext for that...?  Probably not.

* [2018-02-15 jeu.]
** Adding example wizards                                  :eclipse:emfviews:
I've looked at the ones that are in NeoEMF, ATL and Xtext.  They all create a
projects in your workspace, fully set up.

In NeoEMF, they provide the demo projects in the repo.  You can clone that and
import the projects in your Eclipes, and you should have the exact same thing as
what the wizards provide.

The wizards actually contain these projects in zips, and simply inflate them in
the workspace.

So as a first step I can just clean up the examples folder, make sure they work,
document them the best I can.

Then I can create the wizards.  I don't like the zip duplication however.  I
guess I could zip them when building the plugin only?

** Revamping examples                                              :emfviews:
There were simple examples using Books and Publications already to showcase
creating a viewpoint and a view.

It's not obvious what the are the benefits of the use case.  But it shows the
mechanisms.

** Move the matching model to view?                            :emfviews:ecl:
It's only used by the view.  You could imagine different ways to populate the
view using different matching models, but using the same viewpoint.

In the book/publication example, the matching model has this constraint:

: return ps.title = c.title and ps.nPages >20 and not ps.isTOC;

If you wanted to create another view using a different predicate, you would have
to create another viewpoint currently.  But if the matching model is part of the
view, then you don't have to!

** Cleanup up the ECL delegate                                 :emfviews:ecl:
The ECL delegate does two things:

1. execute the ECL matching model
2. create a weaving model from the matches

Here is an example ECL rule:

#+BEGIN_SRC ecl
rule ruleName
match s : mm1!Feature
with  t : mm2!Feature
#+END_SRC

In order to do #1, we have to know:

1. where the ECL file is
2. what are the input models
3. what are the metamodels
4. which metamodels ~mm1~/~mm2~ refers to
5. where to write the weaving model

#1, #2 and #5 are directly provided to the delegate.  #3 can be figured out by
#looking in the models.

But #4 is tricky, because that information is nowhere else.  We don't need to
names the metamodels in the viewpoint.  Arguably, metamodels don't even need
names because they already have namespace URIs which uniquely identify them.

However, it's cumbersome to use the URI to refer to them multiple times in the
same ECL file.  In ATL, you use an alias (in the comments, for some reason).
Here we also use an alias.  But that's a duplication of information, right?  We
have already specified the metamodels in use in the viewpoint.

I think it's reasonable in this case, since it makes the ECL autonomous.

One downside of the current implementation is that you refer to metamodels only
using namespace URIs.  But they might not be loaded in the global registry.  ECL
allows you to specify Ecore files instead.  Let's try to add that.

Okay so the problem with that idea, is that we need the namespace URI to match
the input models with their metamodels.  The syntax would become quite
cumbersome if we had to specify the namespace URI /and/ the path to an eventual
Ecore.

Hmm, but we could do the same thing the Viewpoint does: load the metamodel from
that alias.  Then use that namespace URI.

Okay that works.  But it's a little dumb?  Let's see:

- If given an Ecore file, the viewpoint creates a resource and resource set in
  order to load the EPackage into a private list.
- If given an Ecore file, the ECL delegate does the same thing, but throws away
  the EPackage since it only needs the namespace URI /and/ the location of the
  Ecore to give to EclModule
- EclModule then creates a resource/resourceSet to load the Ecore file again.
  Although they seem to have a pool in EmfUtil.register to avoid loading
  packages twice from the same Ecore file.

It seems that putting the EPackage in the global registry would not be a bad
idea after all.

* [2018-02-22 jeu.]
** Extending the test suite for DSLs                           :emfviews:mel:
I found a strange bug, which I attributed to the lazy rule having funky
behavior.  Turns out, lazy rules do not work the same with EMFTVM and the
regular ATL VM.  As [[https://wiki.eclipse.org/ATL/EMFTVM#Lazy_rules][the documentation states]], in EMFTVM lazy rules are matched
against the arguments, and I can see that with the regular VM, the unique lazy
rule seems to ignore further arguments.

The [[https://wiki.eclipse.org/ATL/User_Guide_-_The_ATL_Language#Unique_Lazy_Rules][ATL reference]] is not explicit on this, only stating:

#+BEGIN_QUOTE
When a unique lazy rule is executed, it always returns the same target element
for a given source element.
#+END_QUOTE

I guess it's back to EMFTVM... but I will need to find a way to build it using
Maven.

* [2018-02-26 lun.]
** Prototyping new NaoMod website                                    :naomod:
Simple, but effective.

I had to resort to JS for fetching things publications from HAL.  It's either
that or generating them statically... which requires some intervention on our
part at some point.

At least with JS, they are always up to date!

For reference, since I had to track this down, here is [[https://api.archives-ouvertes.fr/docs/search/schema/fields/#fields][the list of fields]] we can
query and fetch from HAL entries.

* [2018-03-02 ven.]
** Adding wildcards to VPDL                                   :emfviews:vpdl:
For attributes, when you want to select all of them, it's cumbersome having to
write them all out by hand.

So instead of writing

: select a[b,c,d,e,f,g]

You want to just write:

: select a.*

To implement that, I extended the VPDL grammar.  At first, I wanted to do a
syntactic sugar pass, or handle the extension of wildcards in the ATL
transformation, but that is not possible.  The ATL transformation would need to
load the metamodel in order to know what features to include.

It could be doable as syntactic sugar, but I haven't found how to do that with
Xtext.

So I just output an 'a.*' path in the weaving model, and modified emfviews
to handle that case.

There's a tricky bit because of the way blacklisting and whitelisting are
implemented in EMF Views at the moment.  The filters path are used differently
in each case.

** Building an example from design to runtime models               :emfviews:
For the project, we want to showcase an example including four models:

- A requirements model (ReqIF metamodel)
- A component diagram (UML metamodel)
- A Modisco model of Java source (Modisco/Java metamodel)
- A model of runtime trace (custom metamodel)

I built very simple models for each.  One pain point is I forgot to add the
~viewpoint=~ line in the eview file, and got an NPE.

Of course, EMF silently collects exceptions when loading a resource, so the
resource still loads.  Then when EMF tries to populate the editor, the View
fails because it has uninitialized fields.

Adding custom exception when parsing View does not help: they are captured by
EMF.  Don't know where or when they are supposed to appear though.

When loading the view with the Sample Ecore Editor, there are two issues:

1. The UML model raises an error when clicking on any component.

   : EObjectContainmentElist$Resolving cannot be cast to EObject

2. The reqif model cannot be browsed.  There's only a VirtualEObject top-level
   element:

   [[file:doc/reqif-fail.png]]

When loading the view with the Modisco Model Browser, I get another error:

: ECollections$UnmodifiableElist cannot be cast to InternalEList

and the editor does not allow me to browse the model.

When loading the view in the EView editor, everything is fine: the reqif can be
browsed, and there are no errors.

* [2018-03-05 lun.]
** Fixing views with Modisco                                       :emfviews:
So the problem is a cast from UnmodifiableElist to InternalElist.

Is there an unmodifiable list that implements this interface, or do we have to
define one?

Yes, there is one: EcoreElist.UnmodifiableEList.  Using that, the view can be
browsed using Modisco.

There are still a bunch of exception in the console:

#+BEGIN_EXAMPLE
java.lang.UnsupportedOperationException
	at org.atlanmod.emfviews.elements.VirtualEPackage.getEFactoryInstance(VirtualEPackage.java:153)
	at org.eclipse.gmt.modisco.infra.browser.editors.MetaclassViewer$MetaclassLabelProvider.getImage(MetaclassViewer.java:378)
	at org.eclipse.jface.viewers.WrappedViewerLabelProvider.getImage(WrappedViewerLabelProvider.java:101)
	at org.eclipse.jface.viewers.WrappedViewerLabelProvider.update(WrappedViewerLabelProvider.java:146)
        ...
#+END_EXAMPLE

We do not provide an EFactory instance.  We could provide a dumb one that throws
UnsupportedOp for each method.  Although why is modisco asking for one?  For
images?

Here is MetaclassViewer.java:378:

#+BEGIN_SRC java
// instantiate an element in order to be able to use the image provider
final EObject dummyInstance = eClass.getEPackage().getEFactoryInstance()
  .create(eClass);
#+END_SRC

A dummy instance indeed.

So providing an EFactory instance would not solve the issue.

Looking at the rest of the code:

#+BEGIN_SRC java
// instantiate an element in order to be able to use the image provider
final EObject dummyInstance = eClass.getEPackage().getEFactoryInstance()
  .create(eClass);

// icon provided by an extension
final IconProvidersRegistry iconProvidersRegistry = IconProvidersRegistry
  .getInstance();
final Image icon = iconProvidersRegistry.getIcon(dummyInstance);
if (icon != null) {
  return icon;
}

// icon provided by an adapter from the registry
final IItemLabelProvider itemLabelProvider =
  (IItemLabelProvider) MetaclassViewer.this.browserConfiguration
  .getAppearanceConfiguration().getAdapterFactory()
  .adapt(dummyInstance, IItemLabelProvider.class);

if (itemLabelProvider != null) {
  return ExtendedImageRegistry.getInstance()
    .getImage(itemLabelProvider.getImage(dummyInstance));
}
#+END_SRC

It appears we would need to provide at least a class instance in order for this
mechanism to work.

In this case, we could probably delegate to the concrete EFactory, and the
proper icons would be found.  However, it doesn't seem like the proper solution,
because the EFactory could be used for other purposes as well...  and the
returned object would not actually reflect that they are part of a view.

Maybe the proper solution is to augment these image registries / label providers
with knowledge of the virtual objects?  Go through them, and if we find an
object that has been virtualized, also associate the virtual object to the same
images/labels?

Another issue with Modisco, at least for the UML model, is that I get many
duplicated attributes:

[[file:doc/modisco-duplicates.png]]

Also in the properties view:

[[file:doc/modisco-duplicate-properties.png]]

This duplication also happens with the EView editor.

** Fixing the test view in the Sample ECore Editor                 :emfviews:
For the exception thrown for the UML model, here is the incriminating cast:

: EObject otherEObject = (EObject)((EObject)value).eGet(eOtherEnd);

At runtime, ~value~ is an instance of TemplateSignatureImpl (from the UML2
package), and ~eOtherEnd~ is a VirtualEReference for the 'template' feature of
the TemplateSignature eClass.

The eGet returns an (empty) EObjectContainmentElist$Resolving, which cannot be
cast into EObject.

Interestingly, opening the UML model directly with the same editor, it seem sthe
~eGet~ calls returns ~null~ instead of this resolving elist.

But I don't have control over what TemplateSignatureImpl is doing.

However, I don't know /why/ the editor ended up with TemplateSignatureImpl
instance and not a virtual object.

After debugging for a while, I'm lost in a sea of
CreateChildCommand.createCommand thingies.  It looks like these UML2 objects are
coming from inside CommandParameters, but I don't see when these are created.

Breaking on constructors for CommandParameter, I see that ultimately these
objects come from EcoreUtil.create.

So delegating to the EFactory fixes icons in Modisco, but triggers an error in
Sample ECore Editor.

But wait, I had the exception before I made the changes?

Ahah, still the same error.

The raw eClass comes from the StereotypeApplicationItemProvider of UML2.  How
does it extract it?

It builds a list of all runtime EPackages and EClasses.  So the raw UML2 package
is there, as are all its classes.

In ReflectiveItemProvider.getAllConcreteSubclasses, it iterates over all known
classes to find the subclasses of a given class (here, a VirtualEClass).  Since
VirtualEClass correctly handles the isSuperTypeOf call, the item provider
gathers the raw subclass and runs with it.

Yet another pain created by item providers.

Can we override them?  That's what the EView editor does, but is there a way to
override such a provider in other editors?

In [[*Constructing%20a%20VPDL%20for%20a%20coherent%20model][a previous episode]], I found a trick for displaying BPMN correctly.  It still
works for BPMN, but does not solve the UML issue.

That's because the adapter factory for the item provider is ultimately chosen by
ComposedAdapterFactory.getFactoryForTypes, which, at a glance, does:

1. find a factory that has isFactoryForType true for each type
2. if it's not a catch-all EObject/Object factory, return it
3. if there is an adapter factory registered from the extension point, return it
4. otherwise return the catch-all factory

So the factory from the extension point (UMLItemProvider) overrides the
ReflectiveItemProvider adapter factory.

I tried to define an extension point, but it asks for an URI, and in the code it
matches against the VirtualEPackage instance!  It's not the only case in the
registry: I spotted a BPMN (or ReqIF?) instance as a key in there as well.

And there doesn't seem to be a way to add in the registry by using code.

I think I would be better off just polishing up the EView editor for now.

* [2018-03-06 mar.]
** Fixing duplication of features                                  :emfviews:
This only happens with the UML model apparently.

One culprit is VirtualEClass.getAllFeatures method that returns duplicate
features.  Seems wrong to me.  It's an easy fix: just use a Set and return a
list from that.

I don't know why exactly this only happens with UML though.

I probably want to extend the Set fix to other methods in there as well:
superTypes, etc.

Yep, super types also had duplicates in a situation of diamond inheritance.
Turns out, using a set for features started to break tests, because HashSet has
an unspecified order of iteration.  I could fix the tests to not care about the
order... but some internal parts of EMF (e.g. DynamicEObjectImpl) assign numbers
to features, so it's better to keep the order of features as stable as possible.

Using a LinkedHashSet gives us the insertion order, which is what we want here.

* [2018-03-08 jeu.]
** Creating HTML from a view                           :emfviews:eclipse:egl:
Tried with Acceleo.  Looks bloated.

Found some ~.egl~ files in the old examples.  That's from the Epsilon project.

Trying that instead.  I need to install the Epsilon DevTools in order to have
the EGL launch configuration.

But I don't see any model I can load in there.  Installing the Epsilon EMF
DevTools.

This provides the elusive "Register EPackage" on right-click on ECore file!  It
wasn't in Modisco after all.

Adding all plugins in the examples configurations did the trick.  Don't know
which is used by Epsilon though!

Trying to generate a template from a view...

Using the 'EMF Model' type, I can give the Eview file and its metamodels.  EGL
barks when I'm using an unwknown class, like this:

: [% for (b in Banana.allInstances()) { %]

so when I'm using ~TimedAction~, it does not error, indicating that it's able to
load classes from the viewpoint at least.

But the loop does not create anything!  If I try with the Trace model directly,
it works.

Debugging...

Ah!  In AbstractEmfModel.getAllOfKindFromModel:

#+BEGIN_SRC java
final EClass eClass = classForName(kind);
final List<EObject> allOfKind = new ArrayList<EObject>();

for (EObject eObject : (Collection<EObject>)allContents()) {
	if (eClass.isInstance(eObject)){
		allOfKind.add(eObject);
	}
}
#+END_SRC

It's using isInstance, which returns false, since all the Eobjects here are
virtual, and the eClass comes from the concrete metamodel.

Changing to use the Eviewpoint file as metamodel.

But isInstance still fails!  I have implemented it wrong:

#+BEGIN_SRC java
public boolean isInstance(Object object) {
  return concreteEClass.isInstance(object);
}
#+END_SRC

It only works if the given object is concrete.

Hmm, trying the following:

#+BEGIN_SRC java
if (object instanceof VirtualEObject) {
  return isSuperTypeOf(((VirtualEObject) object).eClass());
} else {
  return concreteEClass.isInstance(object);
}
#+END_SRC

Doesn't work because we have /different instances/ of the same EClass
ExecutedAction at runtime.

What I think is happening is that the EView is creating one Viewpoint, which in
turn loads the metamodel from file and instantiate it.  But since we also give
the EViewpoint to EGL, there's another Viewpoint instance, which does the same
thing.

So, does this happen with metamodels not loaded from file?  No.

Well, we do have duplicated virtual EClasses instances.  But the concrete
eClasses are the same, so we can match on that.

Now...

: Property 'name' not found in object Component [name=Component1, ...

Well, looks like the property is there!  What are you doing EGL?

#+BEGIN_SRC java
// Look for a getX() method
ObjectMethod om = registry.findContributedMethodForEvaluatedParameters(object, "get" + property, new Object[]{}, context);
if (om != null) return om;

// Look for an X() method
om = registry.findContributedMethodForEvaluatedParameters(object, property, new Object[]{}, context);
if (om != null) return om;

// Look for an isX() method
om = registry.findContributedMethodForEvaluatedParameters(object, "is" + property, new Object[]{}, context);
if (om != null) return om;
#+END_SRC

Okay.  So you are looking for a Java method ~getname~ on my ... DynamicEObject.
Of course that cannot work.  Why don't you use eGet?

When creating a template for the Trace model, which has no generated code, there
are no issues.

There, it looks up the structural features of the EClass.  If the feature is
found by name, it uses an EmfPropertyGetter to look up the properties.

Hmm, that's puzzling.  In debugging, I can see that it's actually finding the
property without issues.  But at some point it can't?

* [2018-03-09 ven.]
** Human-readable models                                       :emfviews:emf:
When I did sexp2emf, I didn't find any alternatives.

Since then, I stumbled upon the [[https://www.omg.org/spec/HUTN][HUTN OMG specification]].  It looks like this:

#+BEGIN_EXAMPLE
Family “The McDonalds” {
  address: “7 Main Street”
  migrants
  familyFriends: “The Smiths”
  petFish: female Fish “Wanda”;
  petDog: “Spike”
  CarOwnership: “755-BDL” {
    state: QLD
    make: “Mitsubishi Magna”
    year: 1992
  }
}
nuclear Family “The Smiths” {
  ...
#+END_EXAMPLE

The important part being that references are made with string identifiers.  Here
"The Smiths" is the alias given to the second Family object, so it can be
referred to under familyFriends.

I'll admit the reference syntax is more readable than what I came up with.
Using names instead of numbers would go a long way to making mine more readable.
I wonder if there's a good way to tone down the sigils as well.

* [2018-03-13 mar.]
** Hackathon                                                       :megamart:
*** Use cases
**** CSY - Coppilot
State machines for M1 processor (transition table).  M2 has sensors that can
fail.  Excel file contains an example of inputs/outputs for these sensors.

**** UCataloña + Thales
Goal: single-source embedded system design; use-case: IoT.  Based on UML +
MARTE.

Thales has a flight system that's 1M LoC in Ada.  Provides a C version that has
80k LoC for the hackathon.

**** Bombardier
Train control system.  V model of design/development.

Challenge is in variability: create variants by mixing and matching requirements
models.

**** Camea
Traffic monitoring.  Parts of C, parts FPGA.

**** Volvo
Focus on diesel engine.  Constraints of emission standards.

Same base engine is used for different machines.

Links are not obvious between engines, variations of machines, and end machines.

Need: an overloaded model combining SysML models.  But also some feature model
for

*** Tools
Andrei demos [[https://but4reuse.github.io/#documentation][BUT4reuse]], a tool for feature detection in variability modeling.

Talk about [[http://pure-variants.de/downloads-6.html][pure variants]] for variability at the code level, and co-evolution of
the artefacts with the design model.

*** Post-mortem
We started to draw some requirements, to add to the SysML models that VLV
already had.

They were interested by variability, so Andrei suggested we use FeatureIDE for
making a feature model.

Our intention was to be able to, in the end, generate different products (SysML
models) using that feature model, combined with the SysML as inputs.

We didn't find how to do that.

However, we could make a configuration model using FeatureIDE, and managed to
export that to XMI (thanks to Gwendal).  So we were able to build a view
combining a product configuration with an "overloaded" model of SysML (combining
all the variants).

Now, what to make of this view?  To achieve the original goal, we should use the
configuration model to /filter/ elements from the overloaded model.

I don't know if we can do that using an ECL matching model.  I think ECL only
applies to virtual associations.

We'll have to do it directly using filters in the weaving model.

But in the process, I have showcased EMF Views with the runtime-to-design demo.
At least couple of people appeared to be interested.

It allowed me to highlight some areas to work on for EMF Views:

1. We are lacking an editor for even creating a viewpoint or view.  Just a
   wizard, where you select contributing metamodels would go a long way to ease
   users in the process.

2. EMF Views should integrate with model editors.  If you have a view that
   contains an UML model, then you want to see that UML model using your usual
   editor.  Same for SysML.  Otherwise, EMF Views is of dubious value.

3. We should be able to serialize a view.  Not the weaving model, but the
   full constructed view.  That way, we could maybe export part of it and open
   that back into model editors.

* [2018-03-14 mer.]
** Loading NeoEMF models with EMF Views                     :neoemf:emfviews:
After a quick 1 hour try with Gwendal, we made it work.  It's a bit of a hack,
but it proves that there are no hard barriers for NeoEMF models to load in EMF
Views.

The requirements are:

- Install NeoEMF (duh).  Gwendal directed to [[https://github.com/SOM-Research/NeoEMF][this fork]] since he was not familiar
  with the extensive refactoring on NeoEMF master.

  : clone
  : mvn install
  : cd plugins/eclipse
  : mvn install

  Then in Eclipse, install NeoEMF by pointing to the archive in the local .m2
  repository.  (fr/atlanmod/neoemf/plugins/...-update/....jar).

- The metamodels need to have code generated by NeoEMF (so it generates code
  that talks to Neo4j).

  Here we wanted to load a trace model, so I took the trace.ecore file, added a
  NeoEMF genmodel (that's a new option that appears in the wizard if NeoEMF is
  installed), right-click on the root in the editor, and "Generate Model code".

  That creates an Eclipse plugin-in, as usual.  I added that to my Eclipse 0.

- The /models/ need to be in databases.  Here we went for a straight conversion
  of the log.xmi resource into a log.graphdb using code that Gwendal had lying
  around.

  It loads the XMI resource in memory, and dumps it into a NeoEMF resource
  backed by a graph database.  It's not something feasible for very large models
  (because you have to load them into memory first).  The proper solution then
  would be to pump them out element by element without loading the whole thing,
  using another method.

  But it will do for the purposes of this hack.

  We can check that NeoEMF can open the resource correctly with right-click on
  the log.graphdb file and NeoEMF -> Open resource.

- Then we created a view that has this log.graphdb resource as a contributing
  model.

  First, EMF has to be made aware of what a 'graphdb' resource is.  We can add a
  resource factory.

  There was some dance needed as we couldn't just use ~Resource.getResource~ to
  load the resource.  Nothing too complex, but still a special case that will
  need to be integrated better at some point.

  Then ECL complained.  Fine, we can make a view without ECL anyway.

Then we ran into strange issues with NeoEMF and NullPointerException and failure
to get the store_lock file of the graphdb.

It seems the correct procedure was:

- Open the log.graphdb instance with NeoEMF directly (right-click, etc.)
  Twice.  The first one may fail with NPEs.  The second should open the resource
  for browsing correctly.

- Load the view.  This should work.  If not, regenerate the log.graphdb file
  from the XMI exporter and retry.

But it worked!  We had a view with a model loaded from NeoEMF.

* [2018-03-16 ven.]
** Fixing EGL templates with EMF Views                         :emfviews:egl:
First issue was duplication of Viewpoints.  I'm using a ViewpointRegistry to
ensure that loading from the same resource will give the same Viewpoint
instance.

This allows EGL to populate its getAllKindOf correctly.

However, it still fails at IntrospectionManager.getModelThatKnowsAboutProperty,
falls back on Java property getter and fails, because EmfModel.owns fails:

#+BEGIN_SRC java
if (instance instanceof EObject) {
  EObject eObject = (EObject) instance;
  Resource eObjectResource = eObject.eResource();

  if (eObjectResource == null) return false;

  if (expand) {
    return modelImpl.getResourceSet() == eObjectResource.getResourceSet();
  }
  else {
    return modelImpl == eObjectResource;
  }
}

return  false;
#+END_SRC

eObject is a VirtualEObject here, and eResource returns null.  What happens on a
regular object?

On a DynamicEObjectImpl, eResource returns the XMI resource that contains it.

eResource is computed by looking up the eContainer chain in BasicEObjectImpl.
Overriding eContainer in VirtualEObject makes sense, but is not enough, because
BasicEObjectImpl actually uses internalEContainer.

If I return the Virtualizer, which I know is a View here, in eResource, then the
template works!

I don't think it's correct though.  The eResource should be set as a side effect
of a VirtualEObject being added to some container.  But the problem here is that
I have concrete objects, which are part of concrete resources.  How do I link
them to the EView resource?

On the other hand, in the current implementation, Virtualizers will always be
resources, and a VirtualEObject will always be part of some View.  So...

** Trouble with virtualizer signature            :emfviews:
Namely:

:  E getVirtual(E)

While convenient, that contract is actually not respected by Viewpoint or View.
In View, we always return a VirtualEObject instance, so if we pass an
EAttribute, we would expect an EAttribute back.  That is not the case.

This leads to unexpected cast errors.  This is wrong.

However, switching to:

: EObject getVirtual(EObject)

leads to sprinkling casts everywhere in the code...

A third option is to define an EcoreVirtualizer, which has methods:

: EPackage getVirtual(EPackage)
: EClass   getVirtual(EClass)

etc.  More verbose, and leads to code duplication in the interface
implementation.

It also requires me to implement VirtualEClassifier.

Large refactoring ahead...

* [2018-03-20 mar.]
** Creating views with large models                         :emfviews:neoemf:
I created phony trace models of different sizes by duplicating their contents.
Now to convert them to Neo4j resources, and see how EMF Views handles them.

| Model size | Time to convert (s) | Time to load (s) |
|------------+---------------------+------------------|
| 1.6K       |                   1 | < 1              |
| 1M (978K)  |                  21 | ~1               |
| 10M (9.6M) |                1644 | ~10              |
| 100M (97M) |    not gonna happen |                  |

Time to load is eyeballed taken from right-click->NeoEMF->Open Model Database.

First thing to note is that the XMI -> Neo4J importer seems to be a very
inefficient way to create a model, and the complexity looks concerning.

Opening a view with the small model works.  Only once.  I should get to closing
the resource.

Okay so that works.  Had to modify my custom EView editor to call
~resource.unload~, since the Sample Ecore Editor does not call that.

Loading the 1M resource: Loading the view is instantaneous.  Clicking on the
twisty takes a few seconds before we can see the contributing models.  Clicking
on the Trace model takes another few seconds before we see all the Log
instances.

There is significant wait when closing the view as well.  Probably the ~clear()~
call of unloading the contents.  Not sure why that is necessary actually.

Where is that time spent?

Using VisualVM and JProfiler.  VisualVM is very capable when using sampling;
profiling hangs the Eclipse instance however.

At first glance from the profiler, it looks like enumerating all contents like
we do in the editor takes its toll on Neo4j.  Also EViewEditor.hasChildren has
to first create an eContents list, just to dump it afterwards...

On the other hand, the NeoEMF viewer has no trouble opening the 1M resource.
They are using some lazy content provider of TreeViewer.

Maybe it's worth using that as well.  Or maybe it's more interesting to do
queries on the view (rather than enumerating its contents).

The code (for the fork I'm using) is [[https://github.com/SOM-Research/NeoEMF/blob/master/plugins/eclipse/ui/src/main/java/fr/inria/atlanmod/neoemf/eclipse/ui/editor/NeoEditor.java][here]].  Looks simple enough to adapt.

* [2018-03-21 mer.]
** Adapting the EView editor                                :emfviews:neoemf:
For using a LazyTreeContentProvider.

Performance is even worse now.  Eclipse is hanging on the 1M model.  Good job!

I don't even know how a LazyTreeContentProvider does less work than a regular
content provider.  I mean, we still need to go through the contents in order to
populate the list.  What /could/ be done is to only create tree items for the
/visible portion/ of the viewer.  From the doc, it looks like this is the case.

I should also test if the latency comes from the EMF Views or from the viewer
itself.

It takes 12s to walk through getAllContents of the 1M model.  That is for a view
which is a simple a wrapper around this model.

Enumerating the contents of the NeoEMF directly (without a view) takes... 12s!

No significant CPU overhead for virtualization.

One significant difference between the two is when just querying the contents
size (of the Trace element, which contains thousands of Log instances):

: r.getContents().get(0).eContents().size()

This takes 11s in the view, but 0s in NeoEMF.

That's because we iterate over the contents in order to virtualize them in
VirtualEObject.  Even though they might not be used yet!  We can defer the
virtualization by returning an InternalEList rather than building the list ahead
of time.

By doing so, getting the size is instantaneous.  This should help when building
the view in the editor.

* [2018-03-22 jeu.]
** Lazy contents list                                       :emfviews:neoemf:
So, instead of iterating through all the contents and virtualizing them
outright, we now return an EList that will try to ~get~ (and virtualize) only
the element requested:

#+BEGIN_SRC java
public EObject get(int index) {
  for (EReference ref : eClass().getEAllContainments()) {
    if (ref.isMany()) {
      List<EObject> list = (List<EObject>) eGet(ref);
      int s = list.size();
      if (index < s) {
        return virtualizer.getVirtual(list.get(index));
      } else {
        index -= s;
      }
    } else {
      if (index == 0) {
        return virtualizer.getVirtual((EObject) eGet(ref));
      } else {
        --index;
      }
    }
  }

  throw new IndexOutOfBoundsException("" + index);
}
#+END_SRC

That brings us on par with the NeoEMF viewer for navigating the 1M model in the
view.

But we can do probably do better with caching, by relying on the fact that
~get(i)~ will probably be followed by ~get(i+1)~, etc.  So instead of looping
every containment reference each time, we could go to the correct bucket
directly based on the given index, and return the virtualized object in there.

Another layer of caching would be to memoize the ~get(i)~ results directly.
This would avoid accesses to the database, if no caching is done in lower
layers.

Ah!  In fact we don't need to virtualize anything in ~eContents~, since we call
~eGet~, which will already take care of virtualizing the objects through
VirtualEList.

I implemented LazyEContentsList.  Now it takes 12s to enumerate the contents of
the 1M model in a view, 11s in NeoEMF.  But since contents and sizes are lazily
computed, the EView editor is on par with the NeoEMF editor.

We could do consider doing caching of the elements, but we have to think about
eviction strategies.  We certainly don't want to keep them all in the cache, as
the whole purpose of NeoEMF is handling models that are too big to fit in RAM.

* [2018-03-23 ven.]
** Viewpoint registry may not be a good solution                   :emfviews:
Saving Viewpoint instances in a registry means that the same eviewpoint file
will map to the same Viewpoint instance.  Good.

One issue is that ~Viewpoint.load~ may be called multiple times, and in the
process adding virtual features multiple times as well.

This can be fixed by adding a flag in Viewpoint to not redo the initialization
on ~load~.

But then, it means that changes to the underlying eviewpoint file will not take
effect until we empty the registry.  This is bad.  We could add a way to empty
the registry... but this feels wrong.  Maybe adding the registry in the first
place was not the best way to handle the situation.

The issue was with EGL launchers needing a view and a viewpoint specified as
files, and the view having its own instance of viewpoint which did not match the
other instance loaded from the file.

In fact, the way it's currently set up, the view may not conform to one
eviewpoint, but only to it's own instance of Viewpoint loaded from the
eviewpoint file.

Is it the case that loading a metamodel twice from the same Ecore file leads to
duplicated metamodels?  I don't think so.

Ah, what do you know.  Just tested it, and loading the same Ecore file in two
different resource sets leads to two separate, incompatible instances of the
same metamodel.  Notably, instances of one metamodel are not instances of the
other metamodel.

So solving the issue with a registry was in fact wrong.

* [2018-03-26 lun.]
** Profiling opening up the EView editor on a NeoEMF model  :emfviews:neoemf:
First loading takes several seconds.  Why?  There shouldn't be much to do when
just opening the resource.

Here is the relevant portion of the profile:

[[file:doc/open-eview-editor-profile.png]]

So, 1430ms for loading 3 XMI files, and 750ms for loading a Neo4j DB.

Maybe we can speed up the XMI loading with options to ~Resource.load~.

Using the recommended XML Resource Options from the EMF manual has no effect at
all.  Exact same time to load the 3 XMI files.  All the time is spent in SAX.

So I guess the only alternative would be to defer loading the View in a
background thread, to avoid clogging up the UI thread.

But it's not crucial for now.

** Large models with NeoEMF                                 :emfviews:neoemf:
Testing out a trace with 100k elements, amounting to a 256MB GraphDB folder.

Opening up the model itself in NeoEMF takes more than 10 seconds.  Clicking on
the twistie is another 10 seconds wait (just to reveal the only child ~Trace~,
but I guess it's spending time counting the child elements).

Hmm, it's weird.  It seems all the time is spent in GTKTree and not anything
related to NeoEMF itself?

Okay so when opening the View in the EView Editor, we only need to know whether
a node has children in order to display the twistie.  We don't need to compute
the actual size or the exact number of children at this point.  So we can
just use ~eContents().isEmpty()~, and use an early-exit if we detect that we
have at least one element in LazyEContentsList.

That makes the 10M model load nearly instantaneously (much faster than NeoEMF!).
However, when we click to expand the children of the ~Trace~ node, we are back
to waiting tens of seconds.  All this time is spent in GTK Tree, so I'm guessing
that even though the content provider is lazy, and only fetching the values of
the 20 nodes that are on the screen, it's still allocating for 100k node items
internally?  And the underlying library has bad asymptotic performance?

So at this point there is not much that can be done, aside from rewriting
a lazier TreeViewer.

** ECL and NeoEMF resource                                  :emfviews:neoemf:
We need ECL to populate the view.  Unfortunately, ECL doesn't know how to load a
NeoEMF resource: we need to pass specific options to ~Resource.load~ otherwise
NeoEMF fails.  Why NeoEMF does not pass these default options directly is beyond
me.  It would definitely make our lives simpler in this case, as ECL does not
allow us to pass options to the ~load~ call.  And it insists of loading the
resource itself.

I found a workaround, by setting ReadOnLoad to false, the ~EmfModel.load~ call
only creates the Resource from the URI, but does not load it.  Then we can
override the resource with the one we have already loaded ourselves in
EclDelegate with ~EmfModel.setModelImpl~.  It's hackish, and fails one test in
EMF Views.  But it does work in the Eclipse instance.

Now, loading the small model with ECL is fine.  Loading the 1M model has been
running for 10 minutes, without any end in sight.  There's room for improvement.

* [2018-03-27 mar.]
** A faster way to populate a view                          :emfviews:neoemf:
Okay so the problem I'm hitting here is that populating a view with ECL is slow.
Even for a model with 9000 elements.

First question: is that an issue with ECL or NeoEMF?  What happens if try the
same thing with an in-memory model?  Same thing.  So ECL is the bottleneck here.

[[file:doc/ecl-large-model-profile.png]]

Now, how can we make that faster?  If we wrote some Java code that created a
WeavingModel using the same rules provided to ECL, how would it look like, and
how fast could it be?

For instance, this rule:
#+BEGIN_EXAMPLE
rule javaClass
match l : trace!Log
with  c : java!ClassDeclaration
{
  compare
  {
    return l.source.split("\\.")[0] = c.name;
  }
}
#+END_EXAMPLE

How fast can this be computed, in theory?  The naive but correct algorithm would
be to take all instances of Log, all instances of ClassDeclaration, make a
cartesian product, and keep the couples that match the predicate.

So, O(n*m), assuming the predicate is constant (n is the number of instances of
Log, m the number of instances of ClassDeclaration).

Were I to do that in Java, I would probably do some pre-processing and put the
ClassDeclaration instances in a HashMap, using ~c.name~ as key.  Then, I could
iterate on Log instances, just looking in the HashMap to find matches.  Should
be O(m+n).

I don't think the complexity can go any lower, since we have to look at all the
elements of each set at least once.  The only way to skip elements would be to
have some meta-information about elements of a set, like "Log instances all have
a null ~source~ property".

Wrote a proof of concept in Java.  This can create a weaving model for the 1M
model in under 1 second.  For the 10M model, it takes 5s.  We can then open the
view using the weaving model directly, which is infinitely faster (since it
loads successfully!).

Now this is all good and well, but that approach does not scale.  I don't think
we want to write a custom Java class for each rule.  The HashMap trick should
work for all rules which use comparison, but here is one rule that would not
fit:

#+BEGIN_EXAMPLE
rule requirements
match c : uml!Component
with  r : reqif!SpecObject
{
  compare
  {
    return c.name.toLowerCase().isSubstringOf(r.values[0].theValue.toLowerCase());
  }
}
#+END_EXAMPLE

In this case, the ~isSubstringOf~ prevents us from using a hashtable for O(1)
lookup.  We need a fancier data structure.  A Trie or Aho-Corasick would be good
candidates.

(But note that the models in my example case are very small, so we could get by
with a naive approach).

Coming up with data structures for each predicate also does not scale.  Unless
we can circumscribe the different data structures to use for all the possible
predicates available in ECL.  If that latter set is finite.

* [2018-03-28 mer.]
** Building a weaving model from NeoEMF resources           :emfviews:neoemf:
It takes 16s to build the weaving model for the 1M case, whereas it took under 1
second for the same model loaded in memory.

We decided with H. to delay finding a solution to efficiently populating a view.
We can assume for now that the view weaving model is created by some other
means.

While not uninteresting, it does not tie directly to the NeoEMF integration.

** Writing an ATL transformation on a view                         :emfviews:
Before testing a transformation on a view backed by NeoEMF, better test if it
works on any view at all!

And it works!  The hardest part was getting all the pieces necessary to load the
models.

Since I wrote a Java application that runs the transformation using EMFTVM, I
needed to load the EPackages for the contributing models:

#+BEGIN_SRC java
ReqIF10Package.eINSTANCE.eClass();
UMLPackage.eINSTANCE.eClass();
JavaPackage.eINSTANCE.eClass();
TracePackage.eINSTANCE.eClass();
VirtualLinksPackage.eINSTANCE.eClass();
#+END_SRC

I also needed to include the required packages as plugin dependencies.  And I
needed to register the resource factories in EMF:

#+BEGIN_SRC java
Resource.Factory.Registry.INSTANCE.getExtensionToFactoryMap()
.put("eviewpoint", new EmfViewsFactory());
Resource.Factory.Registry.INSTANCE.getExtensionToFactoryMap()
.put("eview", new EmfViewsFactory());
Resource.Factory.Registry.INSTANCE.getExtensionToFactoryMap()
.put("xmi", new XMIResourceFactoryImpl());
Resource.Factory.Registry.INSTANCE.getExtensionToFactoryMap()
.put("ecore", new EcoreResourceFactoryImpl());
Resource.Factory.Registry.INSTANCE.getExtensionToFactoryMap()
.put("reqif", new ReqIF10ResourceFactoryImpl());
Resource.Factory.Registry.INSTANCE.getExtensionToFactoryMap()
.put("uml", new UMLResourceFactoryImpl());
Resource.Factory.Registry.INSTANCE.getExtensionToFactoryMap()
.put("emftvm", new EMFTVMResourceFactoryImpl());
#+END_SRC

The troublesome one was Reqif10, since the resource factory was not part of the
reqif10 plugin, but in reqif10.serialization.  So I couldn't find the resource
factory, tried to use the XMI one, failed.  Ultimately I hooked up the debugger
to an Eclipse instance, looked at what resource factory was loaded when I opened
the XMI resource, and found the correct plugin.

* [2018-03-29 jeu.]
** Cleaning up technical debt                               :emfviews:neoemf:
Moved files around.  Cleanly separated tests made with the Java Trace metamodel
and the NeoEMF Trace one.

Had to touch a few of the helper files.

Also, wanted to expand on that ATL transformation to navigate all the virtual
links.  Trickier than expected.

The weaving model I built using Java code was not complete: it executed only the
first rule linking the Trace to the Java model.  Since I could not use the
hashtable trick to execute all the rules, I had to invoke ECL from that Java
code.

ECL was annoying to run outside Eclipse for technical, boring reasons.

I ran into an interesting issue in expanding the ATL transformation however.  We
have no provisions for duplicated classifiers in the resulting viewpoint.  So,
writing the following helper:

: helper context Viewpoint!Package def : ...

ATL complains about duplicate ~Package~ classifiers (there is one in UML and one
in MoDisco).  It seems Epsilon has a way to differentiate classifiers using the
~::~ operator.  I don't know if ATL has anything similar.

But it might be an issue down the road.

Tomorrow I should be able to test a transformation on a view backed by a NeoEMF
resource, and run some benchmarks for both cases.

* [2018-03-30 ven.]
** Running the transfo with a NeoEMF resource               :emfviews:neoemf:
It just works, no surprises there.

Now is the time to run some benchmarks.  But before that, I should streamline
the Trace creation a bit, so that I can easily re-create the benchmark situation
without editing a bunch of files manually.

Here is how long creating the resources took:

- Size :: number of Log elements in the resource
- Populate :: time to add all the elements to the provided resource
- Full :: time to create the resource, populate it, save it and eventually close
          it (for NeoEMF)

Here is the table when using a Java-generated Trace metamodel and serializing
the file to XMI (with default options).

|   Size | Populate | Full |
|--------+----------+------|
|     10 |      192 |  292 |
|    100 |        1 |    4 |
|   1000 |        5 |   24 |
|  10000 |       28 |   77 |
| 100000 |      135 |  265 |

All times in these tables are milliseconds.

Here is the table using a Java-generated (NeoEMF) Trace metamodel and
serializing to Neo4j.  Serialization options:

- BlueprintsResourceOptions.GRAPH_TYPE :: BlueprintsNeo4jResourceOptions.GRAPH_TYPE_NEO4J
- BlueprintsNeo4jResourceOptions.CACHE_TYPE :: BlueprintsNeo4jResourceOptions.CacheType.WEAK

|   Size | Populate |   Full |
|--------+----------+--------|
|     10 |       55 |   1459 |
|    100 |       83 |    906 |
|   1000 |      128 |   1125 |
|  10000 |     1015 |   4275 |
| 100000 |   126310 | 146641 |

Okay so it looks like the 10 model for Java has hit some warm-up in EMF or the
Java JIT.  Either way, generating the Java model is very fast.

For NeoEMF, populating the model is fast enough; the first four models seem to
be dominated by resource creation/saving.  The 10^5 case is concerning though:
over 120x for 10x the elements of the 10^4 case.

Generating the weaving models the same way now.  Here are the numbers for the
Java trace metamodel:

- Size :: same as above
- javaClass rule :: time to match the rule between the Trace and Java metamodels
- ECL rules :: time to match the remaining 2 rules with ECL (small models)
- Save :: time to serialize the WeavingModel resource
- Full :: time to do load the Trace model, match and save

|   Size | javaClass rule | ECL rules | Save | Full |
|--------+----------------+-----------+------+------|
|     10 |             89 |       430 |   23 |  557 |
|    100 |             22 |       103 |   13 |  144 |
|   1000 |             47 |       112 |   63 |  244 |
|  10000 |            183 |        56 |  206 |  538 |
| 100000 |           2290 |        53 | 4088 | 6761 |

These operations were done only once, but still timed:

| Task                                    | Time (ms) |
|-----------------------------------------+-----------|
| Initialize EMF                          |       910 |
| Load contributing models (except Trace) |       776 |

Here are the results using the Trace metamodel generated by NeoEMF:

|   Size | javaClass rule | ECL rules | Save |  Full |
|--------+----------------+-----------+------+-------|
|     10 |            262 |       398 |   24 |  1531 |
|    100 |            204 |       119 |   17 |   409 |
|   1000 |            812 |        74 |   58 |   975 |
|  10000 |          27478 |        64 |  189 | 27791 |
| 100000 |              ? |         ? |    ? |     ? |

I did not have time to run the 10^5 model weaving model to completion.

| Task                                    | Time (ms) |
|-----------------------------------------+-----------|
| Initialize EMF                          |       976 |
| Load contributing models (except Trace) |       991 |

Finally, here are the numbers for running the ATL transformation on that.

- Size :: as above
- Load :: time to load the viewpoint and view resources
- Init ATL :: time to populate the objects required by ATL, including loading
              the transformation module
- Transform :: time to run the transformation (execute ~ExecEnv.run~)

First, using the Java trace metamodel:

|   Size | Load | Init ATL | Transform |
|--------+------+----------+-----------|
|     10 |  878 |      270 |      1180 |
|    100 |  279 |       20 |       803 |
|   1000 |  291 |      191 |      1470 |
|  10000 |  519 |       13 |      6142 |
| 100000 | 3669 |       14 |    136019 |

Using the NeoEMF trace metamodel:

|   Size | Load | Init ATL | Transform |
|--------+------+----------+-----------|
|     10 | 1476 |       17 |       449 |
|    100 |  265 |       12 |       683 |
|   1000 |  446 |      292 |      1540 |
|  10000 | 1413 |       10 |     62705 |
| 100000 |  n/a |      n/a |       n/a |

(Don't have a weaving model for 10^5)

Okay so far the results are not encouraging.  But I should probably reproduce
that using JMH in order to get more runs, do averages, do some warm-up etc. for
more stable results.

* [2018-04-03 mar.]
** Trying to run JMH for benchmarks                         :neoemf:emfviews:
So, [[http://openjdk.java.net/projects/code-tools/jmh/][JMH]] is apparently the recommended option for doing benchmarking of Java
programs.  Other than cutting the boilerplate, it also does warm-up, multiple
iterations and averages results automatically.

That's what [[https://github.com/atlanmod/NeoEMF/tree/master/benchmarks][NeoEMF uses]].

I wanted to port the code I have in Eclipse that runs an ATL transformation to
JMH, in order to get a stabler picture of the results.  But the recommended way
to use JMH is to use maven.

Problem: I have several dependencies that are not in maven central.  For my own
code, I could add ~pom~ files to allow them to be built and added as maven
dependencies.  I would need to ~mvn install~ each of them before running
benchmarks though, so it would be cumbersome, but it would work.  For other
projects however (MoDisco, ReqIF), they are not in mvn central or have a maven
build (unlike ATL, which has a well-hidden maven repository).

I tried to find some info about adding Eclipse dependencies to a maven project,
but no dice.  The only relevant thing I could find was [[http://www.eclipse.org/articles/Article-Eclipse-and-Maven2/index.html][this old]] article, with
dead links to the code.  It was not really off-the-shelf either.

Soooo it leaves me with [[http://labs.carrotsearch.com/junit-benchmarks-tutorial.html][JUnit Benchmarks]], which has been deprecated (in favor of
JMH) 3 years ago.  Or rolling my own.

* [2018-04-04 mer.]
** Improving the benchmarks without JMH                     :emfviews:neoemf:
Running multiple iterations with warmups is easy.

Measuring memory is trickier.  We can do something like this:

: runtime.gc();
: start = runtime.totalMemory() - runtime.freeMemory()
: ...
: end = runtime.totalMemory() - runtime.freeMemory()

But there are no guarantees that the GC does not run between the two measures.
What are we measuring anyway?  What's memory consumption in a Java app?  Is it
the sum of the size of the objects allocated between two programs?  We cannot
measure that directly unless we do some (possibly heavy) profiling.  But that's
not an interesting measure if the GC is perfectly capable of recycling the
objects.

The sum of the size of /retained/ objects might more relevant for our use case.
It's not accurate, because we are not counting only objects allocated by
EMFViews or NeoEMF, but neither is timing.  The point is to get a sense of
scale.

Also, there is an issue of calling ~runtime.gc~ while we are measuring time,
because we might do eager collections and mess up the timings.  For now I'm not
calling the GC at all, which can give some wild variations between numbers.  But
at least the magnitude should be about right.

*** Building the weaving model with a Java trace
Numbers are milliseconds.

| Task                     | Time |
|--------------------------+------|
| Initialize EMF           | 914  |
| Load contributing models | 885  |

Benched with 5 warmups followed by 5 measures.

- Size :: model with 10^n elements
- javaClass :: time executing the javaClass rule on the trace model
- ECL :: time executing the two ECL rules (small models)
- full :: time to load the trace model, match the rules, and create and save the
          weaving model

#+name: weaving-model-java
|  Size | javaClass | ECL |   save |   full |
|-------+-----------+-----+--------+--------|
|     1 |        12 |  69 |      2 |     86 |
|     1 |        10 |  72 |      2 |     85 |
|     1 |        32 |  63 |      1 |     98 |
|     1 |         9 |  53 |      1 |     65 |
|     1 |        12 |  50 |      4 |     68 |
|-------+-----------+-----+--------+--------|
|  mean |        15 |  61 |      2 |     80 |
| stdev |        10 |  10 |      1 |     14 |
|-------+-----------+-----+--------+--------|
|     2 |        10 |  53 |      3 |     69 |
|     2 |        11 |  55 |      3 |     73 |
|     2 |        12 |  54 |      3 |     71 |
|     2 |        11 |  65 |     11 |     90 |
|     2 |        12 |  45 |      4 |     64 |
|-------+-----------+-----+--------+--------|
|  mean |        11 |  54 |      5 |     73 |
| stdev |         1 |   7 |      3 |     10 |
|-------+-----------+-----+--------+--------|
|     3 |        15 |  37 |     12 |     75 |
|     3 |        14 |  27 |      8 |     58 |
|     3 |        16 |  30 |      8 |     64 |
|     3 |        13 |  45 |      9 |     72 |
|     3 |        12 |  39 |      9 |     67 |
|-------+-----------+-----+--------+--------|
|  mean |        14 |  36 |      9 |     67 |
| stdev |         2 |   7 |      2 |      7 |
|-------+-----------+-----+--------+--------|
|     4 |        80 |  48 |    121 |    285 |
|     4 |        52 |  49 |     98 |    227 |
|     4 |        50 |  35 |    119 |    222 |
|     4 |        41 |  31 |     86 |    174 |
|     4 |        39 |  31 |     86 |    172 |
|-------+-----------+-----+--------+--------|
|  mean |        52 |  39 |    102 |    216 |
| stdev |        16 |   9 |     17 |     46 |
|-------+-----------+-----+--------+--------|
|     5 |      1827 |  26 |   3669 |   5689 |
|     5 |      1795 |  22 |   3801 |   5783 |
|     5 |      1918 |  29 |   3870 |   5990 |
|     5 |      1921 |  27 |   4009 |   6122 |
|     5 |      1879 |  22 |   3755 |   5803 |
|-------+-----------+-----+--------+--------|
|  mean |      1868 |  25 |   3821 |   5877 |
| stdev |        56 |   3 |    128 |    175 |
|-------+-----------+-----+--------+--------|
|     6 |    159238 |  21 | 348037 | 508814 |
|     6 |    160212 |  21 | 342338 | 504121 |
|     6 |    157618 |  23 | 344132 | 503694 |
|     6 |    159679 |  21 | 347811 | 509033 |
|     6 |    163705 |  21 | 346391 | 512258 |
|-------+-----------+-----+--------+--------|
|  mean |    160090 |  21 | 345742 | 507584 |
| stdev |      2241 |   1 |   2457 |   3626 |
|-------+-----------+-----+--------+--------|
#+TBLFM: @7$2..@7$>=vmean(@2..@6);%.0f::@8$2..@8$>=vsdev(@2..@6);%.0f
#+TBLFM: @14$2..@14$>=vmean(@9..@13);%.0f::@15$2..@15$>=vsdev(@9..@13);%.0f
#+TBLFM: @21$2..@21$>=vmean(@16..@20);%.0f::@22$2..@22$>=vsdev(@16..@20);%.0f
#+TBLFM: @28$2..@28$>=vmean(@23..@27);%.0f::@29$2..@29$>=vsdev(@23..@27);%.0f
#+TBLFM: @35$2..@35$>=vmean(@30..@34);%.0f::@36$2..@36$>=vsdev(@30..@34);%.0f
#+TBLFM: @42$2..@42$>=vmean(@37..@41);%.0f::@43$2..@43$>=vsdev(@37..@41);%.0f

#+NAME: weaving-model-java-avg
| Size | javaClass | ECL |   save |   full |
|------+-----------+-----+--------+--------|
|    1 |        15 |  61 |      2 |     80 |
|    2 |        11 |  54 |      5 |     73 |
|    3 |        14 |  36 |      9 |     67 |
|    4 |        52 |  39 |    102 |    216 |
|    5 |      1868 |  25 |   3821 |   5877 |
|    6 |    160090 |  21 | 345742 | 507584 |
#+TBLFM: @2$2..@2$>=remote(weaving-model-java,@7$$#)
#+TBLFM: @3$2..@3$>=remote(weaving-model-java,@14$$#)
#+TBLFM: @4$2..@4$>=remote(weaving-model-java,@21$$#)
#+TBLFM: @5$2..@5$>=remote(weaving-model-java,@28$$#)
#+TBLFM: @6$2..@6$>=remote(weaving-model-java,@35$$#)
#+TBLFM: @7$2..@7$>=remote(weaving-model-java,@42$$#)

#+begin_src gnuplot :var data=weaving-model-java-avg :file neoemf-integration/bench-weaving-model-java.png
set title Create weaving model (Java Trace)'
set key outside

set xlabel 'model size (10^n)'
set logscale y
set ylabel 'time (ms)'

plot data u 1:2 w lp title 'javaClass', \
     data u 1:3 w lp title 'ECL', \
     data u 1:4 w lp title 'Save', \
     data u 1:5 w lp title 'Full'
#+end_src

#+RESULTS:
[[file:neoemf-integration/bench-weaving-model-java.png]]

* [2018-04-05 jeu.]
** More benchmarks                                          :neoemf:emfviews:
*** Creating the models
Went to 10^6.  Results for the Java Trace:

- XMI :: size of the serialized XMI (default options)

#+NAME: create-java-trace
| Size | Populate | Full | XMI  |
|------+----------+------+------|
|    1 |      229 |  339 | 1.3K |
|    2 |        1 |    5 | 12K  |
|    3 |        7 |   28 | 118K |
|    4 |       36 |   98 | 1.1M |
|    5 |      135 |  332 | 12M  |
|    6 |      524 | 1485 | 115M |

#+begin_src gnuplot :var data=create-java-trace :file neoemf-integration/bench-create-java-trace.png
set title 'Create Java Trace model'
set key outside

set xlabel 'model size (10^n)'
set ylabel 'time (ms)'

plot data u 1:2 w lp title 'Populate', \
     data u 1:3 w lp title 'Full'
#+end_src

#+RESULTS:
[[file:neoemf-integration/bench-create-java-trace.png]]

Results for the NeoEMF Trace:

- GraphDB :: size of the serialized GraphDB folder (same options as previously)

#+NAME: create-neoemf-trace
| Size | Populate |   Full | GraphDB |
|------+----------+--------+---------|
|    1 |       37 |   1922 | 332K    |
|    2 |       45 |    831 | 568K    |
|    3 |      247 |   1263 | 3.2M    |
|    4 |     1290 |   3937 | 29M     |
|    5 |   157200 | 175355 | 286M    |
|    6 | 15262845 |    n/a | n/a     |

#+begin_src gnuplot :var data=create-neoemf-trace :file neoemf-integration/bench-create-neoemf-trace.png
set title 'Create NeoEMF Trace model'
set key outside

set xlabel 'model size (10^n)'
set logscale y
set ylabel 'time (ms)'

plot data u 1:2 w lp title 'Populate', \
     data u 1:3 w lp title 'Full'
#+end_src

#+RESULTS:
[[file:neoemf-integration/bench-create-neoemf-trace.png]]

I let it run all night because the NeoEMF 10^6 was taking a long time.  Indeed,
it took more than 4 hours to just populate the ressource.  Then it crashed when
serializing the database:

#+BEGIN_EXAMPLE
Exception in thread "GC-Monitor" java.lang.OutOfMemoryError: GC overhead limit exceeded
	at sun.text.resources.FormatData.getContents(FormatData.java:120)
	at sun.util.resources.ParallelListResourceBundle.loadLookupTablesIfNecessary(ParallelListResourceBundle.java:168)
	at sun.util.resources.ParallelListResourceBundle.handleKeySet(ParallelListResourceBundle.java:134)
	at sun.util.resources.ParallelListResourceBundle.keySet(ParallelListResourceBundle.java:143)
	at sun.util.resources.ParallelListResourceBundle.containsKey(ParallelListResourceBundle.java:129)
	at sun.util.resources.ParallelListResourceBundle$KeySet.contains(ParallelListResourceBundle.java:208)
	at sun.util.resources.ParallelListResourceBundle.containsKey(ParallelListResourceBundle.java:129)
	at sun.util.resources.ParallelListResourceBundle$KeySet.contains(ParallelListResourceBundle.java:208)
	at sun.util.resources.ParallelListResourceBundle.containsKey(ParallelListResourceBundle.java:129)
	at java.text.DateFormatSymbols.initializeData(DateFormatSymbols.java:716)
	at java.text.DateFormatSymbols.<init>(DateFormatSymbols.java:145)
	at sun.util.locale.provider.DateFormatSymbolsProviderImpl.getInstance(DateFormatSymbolsProviderImpl.java:85)
	at java.text.DateFormatSymbols.getProviderInstance(DateFormatSymbols.java:364)
	at java.text.DateFormatSymbols.getInstance(DateFormatSymbols.java:340)
	at java.util.Calendar.getDisplayName(Calendar.java:2110)
	at java.text.SimpleDateFormat.subFormat(SimpleDateFormat.java:1125)
	at java.text.SimpleDateFormat.format(SimpleDateFormat.java:966)
	at java.text.SimpleDateFormat.format(SimpleDateFormat.java:936)
	at java.text.DateFormat.format(DateFormat.java:345)
	at org.neo4j.helpers.Format$ThreadLocalFormat.format(Format.java:132)
	at org.neo4j.helpers.Format.date(Format.java:56)
	at org.neo4j.helpers.Format.date(Format.java:36)
	at org.neo4j.helpers.Format.date(Format.java:31)
	at org.neo4j.kernel.impl.util.StringLogger$ActualStringLogger.time(StringLogger.java:519)
	at org.neo4j.kernel.impl.util.StringLogger$ActualStringLogger.logMessage(StringLogger.java:503)
	at org.neo4j.kernel.impl.cache.MeasureDoNothing.run(MeasureDoNothing.java:84)
Exception in thread "main" java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.util.HashSet.<init>(HashSet.java:119)
	at com.tinkerpop.blueprints.impls.tg.TinkerElement.getPropertyKeys(TinkerElement.java:30)
	at com.tinkerpop.blueprints.util.wrappers.id.IdElement.getPropertyKeys(IdElement.java:39)
	at com.tinkerpop.blueprints.util.ElementHelper.copyProperties(ElementHelper.java:48)
	at com.tinkerpop.blueprints.util.GraphHelper.copyGraph(GraphHelper.java:63)
	at fr.inria.atlanmod.neoemf.data.blueprints.BlueprintsPersistenceBackend.copyTo(BlueprintsPersistenceBackend.java:414)
	at fr.inria.atlanmod.neoemf.data.blueprints.BlueprintsPersistenceBackendFactory.copyBackend(BlueprintsPersistenceBackendFactory.java:205)
	at fr.inria.atlanmod.neoemf.resource.DefaultPersistentResource.save(DefaultPersistentResource.java:162)
	at Creator.main(Creator.java:296)
#+END_EXAMPLE

*** Running the ATL transformation with a Java trace
Benching with 5 warmups / 5 measures to get more stable results.  10^6 took too
long for getting these numbers.

|  Size | Load | Init ATL | Transform |   Full |
|-------+------+----------+-----------+--------|
|     1 |  164 |       12 |       740 |    918 |
|     1 |  164 |       11 |       528 |    704 |
|     1 |  135 |       10 |       616 |    761 |
|     1 |  158 |       12 |       721 |    892 |
|     1 |  126 |       10 |       494 |    630 |
|-------+------+----------+-----------+--------|
|  mean |  149 |       11 |       620 |    781 |
| stdev |   18 |        1 |       111 |    123 |
|-------+------+----------+-----------+--------|
|     2 |  100 |        8 |       597 |    707 |
|     2 |  119 |        7 |       520 |    647 |
|     2 |  104 |        6 |       496 |    606 |
|     2 |   99 |       11 |       541 |    652 |
|     2 |  162 |        8 |       522 |    693 |
|-------+------+----------+-----------+--------|
|  mean |  117 |        8 |       535 |    661 |
| stdev |   27 |        2 |        38 |     40 |
|-------+------+----------+-----------+--------|
|     3 |  138 |        9 |       970 |   1118 |
|     3 |  126 |        6 |      1004 |   1137 |
|     3 |  137 |        6 |       959 |   1103 |
|     3 |  128 |        8 |      1007 |   1144 |
|     3 |  144 |        7 |       974 |   1127 |
|-------+------+----------+-----------+--------|
|  mean |  135 |        7 |       983 |   1126 |
| stdev |    7 |        1 |        21 |     16 |
|-------+------+----------+-----------+--------|
|     4 |  448 |        6 |      6778 |   7237 |
|     4 |  449 |        6 |      6744 |   7205 |
|     4 |  457 |        7 |      6641 |   7114 |
|     4 |  453 |        7 |      6653 |   7122 |
|     4 |  545 |        9 |      6653 |   7214 |
|-------+------+----------+-----------+--------|
|  mean |  470 |        7 |      6694 |   7178 |
| stdev |   42 |        1 |        63 |     56 |
|-------+------+----------+-----------+--------|
|     5 | 3528 |        5 |    137589 | 141168 |
|     5 | 3684 |        7 |    137637 | 141378 |
|     5 | 3508 |        6 |    139485 | 143046 |
|     5 | 3660 |        6 |    137036 | 140750 |
|     5 | 3565 |        6 |    137216 | 140836 |
|-------+------+----------+-----------+--------|
|  mean | 3589 |        6 |    137793 | 141436 |
| stdev |   79 |        1 |       979 |    935 |
|-------+------+----------+-----------+--------|
#+TBLFM: @7$2..@7$>=vmean(@2..@6);%.0f::@8$2..@8$>=vsdev(@2..@6);%.0f
#+TBLFM: @14$2..@14$>=vmean(@9..@13);%.0f::@15$2..@15$>=vsdev(@9..@13);%.0f
#+TBLFM: @21$2..@21$>=vmean(@16..@20);%.0f::@22$2..@22$>=vsdev(@16..@20);%.0f
#+TBLFM: @28$2..@28$>=vmean(@23..@27);%.0f::@29$2..@29$>=vsdev(@23..@27);%.0f
#+TBLFM: @35$2..@35$>=vmean(@30..@34);%.0f::@36$2..@36$>=vsdev(@30..@34);%.0f

*** ATL transformation with a Java trace with ~using~
This transformation uses a ~using~ clause to avoid computing the ~component~ of
a Log element multiple times.  Should be faster, but how much?

Benching with 1 warmup / 5 measures since measures do not seem to change
drastically after 5 warmups.  Very small models have higher deviation, but
that's because of constant overheads mostly.

#+NAME: run-atl-java
|  Size | Load | Init ATL | Transform |   Full |
|-------+------+----------+-----------+--------|
|     1 |  333 |       19 |       786 |   1138 |
|     1 |  179 |       16 |       541 |    737 |
|     1 |  173 |       13 |       662 |    848 |
|     1 |  246 |       15 |       607 |    868 |
|     1 |  169 |       14 |       652 |    835 |
|-------+------+----------+-----------+--------|
|  mean |  220 |       15 |       650 |    885 |
| stdev |   71 |        2 |        90 |    150 |
|-------+------+----------+-----------+--------|
|     2 |  157 |       11 |       560 |    729 |
|     2 |  133 |       10 |       587 |    732 |
|     2 |  164 |       24 |       600 |    788 |
|     2 |  159 |        9 |       652 |    820 |
|     2 |  146 |       10 |       877 |   1035 |
|-------+------+----------+-----------+--------|
|  mean |  152 |       13 |       655 |    821 |
| stdev |   12 |        6 |       128 |    126 |
|-------+------+----------+-----------+--------|
|     3 |  191 |        9 |      1148 |   1350 |
|     3 |  162 |        9 |      1009 |   1181 |
|     3 |  148 |        9 |      1149 |   1308 |
|     3 |  164 |       10 |      1039 |   1215 |
|     3 |  139 |        7 |      1046 |   1194 |
|-------+------+----------+-----------+--------|
|  mean |  161 |        9 |      1078 |   1250 |
| stdev |   20 |        1 |        66 |     75 |
|-------+------+----------+-----------+--------|
|     4 |  487 |        9 |      6045 |   6548 |
|     4 |  526 |        7 |      6142 |   6680 |
|     4 |  484 |        7 |      6175 |   6672 |
|     4 |  483 |        7 |      6139 |   6635 |
|     4 |  492 |        8 |      6173 |   6679 |
|-------+------+----------+-----------+--------|
|  mean |  494 |        8 |      6135 |   6643 |
| stdev |   18 |        1 |        53 |     56 |
|-------+------+----------+-----------+--------|
|     5 | 3755 |        8 |    129289 | 133101 |
|     5 | 3716 |        6 |    129073 | 132848 |
|     5 | 3767 |        6 |    134370 | 138201 |
|     5 | 3738 |        5 |    127816 | 131608 |
|     5 | 3865 |        6 |    131437 | 135355 |
|-------+------+----------+-----------+--------|
|  mean | 3768 |        6 |    130397 | 134223 |
| stdev |   57 |        1 |      2574 |   2603 |
|-------+------+----------+-----------+--------|
#+TBLFM: @7$2..@7$>=vmean(@2..@6);%.0f::@8$2..@8$>=vsdev(@2..@6);%.0f
#+TBLFM: @14$2..@14$>=vmean(@9..@13);%.0f::@15$2..@15$>=vsdev(@9..@13);%.0f
#+TBLFM: @21$2..@21$>=vmean(@16..@20);%.0f::@22$2..@22$>=vsdev(@16..@20);%.0f
#+TBLFM: @28$2..@28$>=vmean(@23..@27);%.0f::@29$2..@29$>=vsdev(@23..@27);%.0f
#+TBLFM: @35$2..@35$>=vmean(@30..@34);%.0f::@36$2..@36$>=vsdev(@30..@34);%.0f

So it does not change the complexity, but there's no reason not to use that one.

#+NAME: run-atl-java-avg
| Size | Load | Init ATL | Transform |   Full |
|------+------+----------+-----------+--------|
|    1 |  220 |       15 |       650 |    885 |
|    2 |  152 |       13 |       655 |    821 |
|    3 |  161 |        9 |      1078 |   1250 |
|    4 |  494 |        8 |      6135 |   6643 |
|    5 | 3768 |        6 |    130397 | 134223 |
|    6 |      |          |           |        |
#+TBLFM: @2$2..@2$>=remote(run-atl-java,@7$$#)
#+TBLFM: @3$2..@3$>=remote(run-atl-java,@14$$#)
#+TBLFM: @4$2..@4$>=remote(run-atl-java,@21$$#)
#+TBLFM: @5$2..@5$>=remote(run-atl-java,@28$$#)
#+TBLFM: @6$2..@6$>=remote(run-atl-java,@35$$#)

#+begin_src gnuplot :var data=run-atl-java-avg :file neoemf-integration/bench-run-atl-java-with-using.png
set title 'Run ATL transformation (Java Trace)'
set key outside

set xtics 1 to 5
set xlabel 'model size (10^n)'
set logscale y
set ylabel 'time (ms)'

plot data u 1:2 w lp title 'Load', \
     data u 1:4 w lp title 'Transform', \
     data u 1:5 w lp title 'Full'
#+end_src

#+RESULTS:
[[file:neoemf-integration/bench-run-atl-java-with-using.png]]

** Saving a weaving model as a NeoEMF resource              :neoemf:emfviews:
In order to do that, I need to generate the NeoEMF code for the WeavingModel.
That leads to a lot of duplication in the benchmark code.

Also, while the XMI had no issues with duplicate containers in XMI, NeoEMF
complains.  So I have to merge the contributing models and their contents when
building the weaving model.

Hitting a bug.. but the code is a mess.  It seems even with caching the
ConcreteConcepts that I build and re-using the map in ECLDelegateMut, I'm still
hitting duplicates when serializing?  Looking at the cache of concrete concepts,
I don't see duplicates.

I should try to use the same logic for XMI and look at the resulting XMI
directly to see if I've indeed solved the issue.

* [2018-04-06 ven.]
** Saving the 10^6 NeoEMF model crashed again                        :neoemf:
Even after using ~-Xmx20g~ to increase the available memory.  It crashed in a
different place however:

#+BEGIN_EXAMPLE
Exception in thread "main" Exception in thread "GC-Monitor" java.lang.OutOfMemoryError: Java heap space
	at org.neo4j.kernel.impl.util.RelIdArray$LowIdBlock.<init>(RelIdArray.java:418)
	at org.neo4j.kernel.impl.util.RelIdArray$LowIdBlock.<init>(RelIdArray.java:415)
	at org.neo4j.kernel.impl.util.RelIdArray.add(RelIdArray.java:141)
	at org.neo4j.kernel.impl.core.NodeManager.createRelationship(NodeManager.java:261)
	at org.neo4j.kernel.impl.core.NodeImpl.createRelationshipTo(NodeImpl.java:628)
	at org.neo4j.kernel.impl.core.NodeProxy.createRelationshipTo(NodeProxy.java:207)
	at com.tinkerpop.blueprints.impls.neo4j.Neo4jGraph.addEdge(Neo4jGraph.java:488)
	at com.tinkerpop.blueprints.util.wrappers.id.IdGraph.addEdge(IdGraph.java:201)
	at fr.inria.atlanmod.neoemf.data.blueprints.BlueprintsPersistenceBackend$AutoCleanerIdGraph.addEdge(BlueprintsPersistenceBackend.java:514)
	at com.tinkerpop.blueprints.util.GraphHelper.copyGraph(GraphHelper.java:68)
	at fr.inria.atlanmod.neoemf.data.blueprints.BlueprintsPersistenceBackend.copyTo(BlueprintsPersistenceBackend.java:414)
	at fr.inria.atlanmod.neoemf.data.blueprints.BlueprintsPersistenceBackendFactory.copyBackend(BlueprintsPersistenceBackendFactory.java:205)
	at fr.inria.atlanmod.neoemf.resource.DefaultPersistentResource.save(DefaultPersistentResource.java:162)
	at Creator.lambda$3(Creator.java:281)
	at Creator$$Lambda$2/693632176.apply(Unknown Source)
	at Util.time(Util.java:32)
	at Creator.main(Creator.java:278)
java.lang.OutOfMemoryError: Java heap space
	at sun.util.locale.BaseLocale.getInstance(BaseLocale.java:86)
	at java.util.Locale.getInstance(Locale.java:741)
	at java.util.ResourceBundle$Control$CandidateListCache.getDefaultList(ResourceBundle.java:2474)
	at java.util.ResourceBundle$Control$CandidateListCache.createObject(ResourceBundle.java:2451)
	at java.util.ResourceBundle$Control$CandidateListCache.createObject(ResourceBundle.java:2381)
	at sun.util.locale.LocaleObjectCache.get(LocaleObjectCache.java:60)
	at java.util.ResourceBundle$Control.getCandidateLocales(ResourceBundle.java:2376)
	at sun.util.resources.LocaleData$LocaleDataResourceBundleControl.getCandidateLocales(LocaleData.java:209)
	at java.util.ResourceBundle.getBundleImpl(ResourceBundle.java:1365)
	at java.util.ResourceBundle.getBundle(ResourceBundle.java:899)
	at sun.util.resources.LocaleData$1.run(LocaleData.java:167)
	at sun.util.resources.LocaleData$1.run(LocaleData.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.util.resources.LocaleData.getBundle(LocaleData.java:163)
	at sun.util.resources.LocaleData.getDateFormatData(LocaleData.java:127)
[INFO]  05:11:27,420 - PersistenceBackend closed: neo-blueprints:/home/fmdkdd/proj/monoge/neoemf-integration/neoemf-importer/models/neoemf-trace/1000000.graphdb
	at java.text.DateFormatSymbols.initializeData(DateFormatSymbols.java:710)
#+END_EXAMPLE

I double-checked that the VM had indeed access to 20GB with
~runtime.getMaxMemory()~:

: 19088801792

Well, I don't know how they do their calculations, but in my book 20GB stands
for GigaByte, which is 10^9 bytes, which is exactly 20*10^9. The 'g' in '20g'
might stand for GiB (GibiByte), which is 1024^3, but then it would amount to
20 * 1024^3 which is even larger:

: 20 GiB = 21474836480 B

(calc was missing these crucial unit prefixes, but that is now solved)

Anyway, I don't see where they get that amount, but if 19GB is not enough for
serializing the model... this is an issue.

G. to the rescue.  We are likely committing a transaction with 1M elements,
which is too big for Neo4j to handle.  The way to do it is [[https://github.com/SOM-Research/NeoEMF/blob/master/examples/fr.inria.atlanmod.neoemf.demo/src/fr/inria/atlanmod/neoemf/demo/importer/BlueprintsImporter.java][here]].  We should use
the ~autoCommit~ option when saving the resource to ensure smaller transactions.

Also, we need to save the resource once with the options *before* adding
elements to it, otherwise the autoCommit won't be applied.

** Saving the weaving model in NeoEMF                       :neoemf:emfviews:
Testing on the XMI instead... there is indeed only one instance of the MoDisco
contributing model.  There is one concept with a funky path however:

: <concreteElements xsi:type="virtualLinks:ConcreteConcept" path="/-1"/>

Ah, it was a silly copy/paste mistake.  That's what you get duplicating code!

Hmm, still this error!

: Exception in thread "main" java.lang.IllegalArgumentException: vertex with given id already exists: 'ConcreteConcept@http://www.atlanmod.org/emfviews/virtuallinks-neoemf/0.3.0'

I could try importing an XMI weaving model.

Ah wait, silly me.  G. comes to the rescue again.  It's because the previous
GraphDB weaving model was half-written to disk, so the creation of a new
resource picked that up and tried to add contents to it.

I should clean up the target before saving.  I don't know why it works for the
XMI, but I could clean that up as well, just to be safe.

** Solving the duplicate implementations of WeavingModel    :emfviews:neoemf:
Since NeoEMF generates its own implementation of Ecore models, if you want to
keep the Java implementation around, then you need to either: fully duplicate
the two packages (leading to code duplication everywhere we want to use both
implementations), or tweak the generated code to use a common interface.
Generate both implementations, then change one to use the interfaces of the
other.

* [2018-04-09 lun.]
** Creating the NeoEMF trace model                          :neoemf:emfviews:
Up to 10^6 this time, using the autoCommit option with a default value of
100000.  It worked.  Took 4 hours for the largest model though.

#+NAME: create-neoemf-trace-autocommit
| Size | Populate |     Full | GraphDB |
|------+----------+----------+---------|
|    1 |      198 |     1584 | 332KB   |
|    2 |      178 |      808 | 584KB   |
|    3 |      444 |     1151 | 3.3MB   |
|    4 |     2826 |     4222 | 31MB    |
|    5 |   152875 |   154747 | 341MB   |
|    6 | 14408336 | 14408919 | 3.4GB   |

#+begin_src gnuplot :var data=create-neoemf-trace-autocommit :file neoemf-integration/bench-create-neoemf-trace-autocommit.png
set title 'Create NeoEMF Trace model'
set key outside

set xlabel 'model size (10^n)'
set logscale y
set ylabel 'time (ms)'

plot data u 1:2 w lp title 'Populate', \
     data u 1:3 w lp title 'Full'
#+end_src

#+RESULTS:
[[file:neoemf-integration/bench-create-neoemf-trace-autocommit.png]]

** Merging NeoEMF and standard EMF generated EPackages           :neoemf:emf:
What happens if I just re-use the interfaces of the EMF-generated EPackage in
the NeoEMF generated EPackage?  Does it allow me to use the same interface to
interact with the two packages?  It should, but are there gotchas?

Removing the interfaces at the root of the NeoEMF EPackage, and doing the
associated renaming works fine.  The default factory assumes there's only one
implementation however:

: TraceneoemfFactory eINSTANCE = traceneoemf.impl.TraceneoemfFactoryImpl.init();

So we need to create a common supertype to both factories interfaces.  All in
all, a few changes that /could/ be automated if we wanted to.

Once that is done, it seems to work for my purposes.  In effect, we have two
registered EPackages, but they share the same interface.

Okay!  That cut down a lot of the crappy dupe code I had.  Now I can build
weaving models backed by NeoEMF.  Next step is to use them when building a view.

** Saving weaving models to NeoEMF                          :neoemf:emfviews:
Here are the numbers.  NeoEMF saving options are the same used for creating
NeoEMF models (Neo4j/autocommit).

First table is for creating a NeoEMF weaving model backed by a Java trace model.

- load :: How long to load the Trace resource
- GraphDB :: Size of the weaving model GraphDB folder on disk

#+NAME: neoemf-weaving-model-java-trace
| Size | load | javaClass | ECL |  save |   full | GraphDB |
|------+------+-----------+-----+-------+--------+---------|
|    1 |    4 |       158 | 324 |  1390 |   1886 | 484KB   |
|    2 |   14 |        72 |  82 |   737 |    908 | 967KB   |
|    3 |   52 |       195 |  81 |  1136 |   1466 | 6M      |
|    4 |  138 |      1484 |  72 |  4294 |   5991 | 57M     |
|    5 |  256 |    217313 | 351 | 28999 | 246919 | 624M    |
|    6 |      |           |     |       |        |         |

#+begin_src gnuplot :var data=neoemf-weaving-model-java-trace :file neoemf-integration/bench-neoemf-weaving-model-java-trace.png
set title Create NeoEMF weaving model (Java Trace)'
set key outside

set xlabel 'model size (10^n)'
set logscale y
set ylabel 'time (ms)'

plot data u 1:2 w lp title 'load', \
     data u 1:3 w lp title 'javaClass', \
     data u 1:4 w lp title 'ECL', \
     data u 1:5 w lp title 'Save', \
     data u 1:6 w lp title 'Full'
#+end_src

#+RESULTS:
[[file:neoemf-integration/bench-neoemf-weaving-model-java-trace.png]]

Second table is for creating a NeoEMF weaving model backed by a NeoEMF trace
model.

#+NAME: neoemf-weaving-model-neoemf-trace
| Size | load | javaClass | ECL |  save |    full | GraphDB |
|------+------+-----------+-----+-------+---------+---------|
|    1 |   62 |        52 |  46 |   569 |     810 | 484K    |
|    2 |   25 |        61 |  61 |   675 |     900 | 980K    |
|    3 |   33 |       314 |  30 |   887 |    1347 | 6M      |
|    4 |   47 |     26422 |  47 |  2386 |   28988 | 57M     |
|    5 |   31 |   3049003 | 113 | 26780 | 3076031 | 626M    |
|    6 |      |           |     |       |         |         |

#+begin_src gnuplot :var data=neoemf-weaving-model-neoemf-trace :file neoemf-integration/bench-neoemf-weaving-model-neoemf-trace.png
set title Create NeoEMF weaving model (NeoEMF Trace)'
set key outside

set xlabel 'model size (10^n)'
set logscale y
set ylabel 'time (ms)'

plot data u 1:2 w lp title 'load', \
     data u 1:3 w lp title 'javaClass', \
     data u 1:4 w lp title 'ECL', \
     data u 1:5 w lp title 'Save', \
     data u 1:6 w lp title 'Full'
#+end_src

#+RESULTS:
[[file:neoemf-integration/bench-neoemf-weaving-model-neoemf-trace.png]]

* [2018-04-10 mar.]
** Ran the (XMI) weaving model creation overnight           :neoemf:emfviews:
10^6 for NeoeMF trace never finished.  Sigh.

*** Creating an XMI weaving model with Java trace
#+NAME: xmi-weaving-model-java-trace
| Size | load | javaClass | ECL |   save |   full | XMI  |
|------+------+-----------+-----+--------+--------+------|
|    1 |    3 |        24 | 299 |     18 |    351 | 11K  |
|    2 |    3 |        22 | 118 |     11 |    156 | 36K  |
|    3 |   16 |        49 | 100 |     51 |    217 | 285K |
|    4 |   96 |       486 |  95 |    423 |   1100 | 2.8M |
|    5 |  524 |      2482 |  92 |   2167 |   5266 | 28M  |
|    6 | 1557 |    155217 |  52 | 154123 | 310950 | 277M |

#+begin_src gnuplot :var data=xmi-weaving-model-java-trace :file neoemf-integration/bench-xmi-weaving-model-java-trace.png
set title Create XMI weaving model (Java Trace)'
set key outside

set xlabel 'model size (10^n)'
set logscale y
set ylabel 'time (ms)'

plot data u 1:2 w lp title 'load', \
     data u 1:3 w lp title 'javaClass', \
     data u 1:4 w lp title 'ECL', \
     data u 1:5 w lp title 'Save', \
     data u 1:6 w lp title 'Full'
#+end_src

#+RESULTS:
[[file:neoemf-integration/bench-xmi-weaving-model-java-trace.png]]

*** Creating an XMI weaving model with a NeoEMF trace
#+NAME: xmi-weaving-model-neoemf-trace
| Size | load | javaClass | ECL | save |    full | XMI  |
|------+------+-----------+-----+------+---------+------|
|    1 | 1277 |      1119 |  66 |    2 |    2543 | 11K  |
|    2 |   79 |       234 |  71 |    2 |     471 | 37K  |
|    3 |   47 |      1065 |  33 |    6 |    1245 | 297K |
|    4 |   25 |     20680 |  40 |   74 |   20901 | 2.9M |
|    5 |  205 |   2123328 |  38 | 1905 | 2125560 | 29M  |
|    6 |   37 |       n/a | n/a |  n/a |     n/a | n/a  |

#+begin_src gnuplot :var data=xmi-weaving-model-neoemf-trace :file neoemf-integration/bench-xmi-weaving-model-neoemf-trace.png
set title Create XMI weaving model (NeoEMF Trace)'
set key outside

set xlabel 'model size (10^n)'
set logscale y
set ylabel 'time (ms)'

plot data u 1:2 w lp title 'load', \
     data u 1:3 w lp title 'javaClass', \
     data u 1:4 w lp title 'ECL', \
     data u 1:5 w lp title 'Save', \
     data u 1:6 w lp title 'Full'
#+end_src

#+RESULTS:
[[file:neoemf-integration/bench-xmi-weaving-model-neoemf-trace.png]]

** Creating a view from a NeoEMF weaving mdoel              :neoemf:emfviews:
Just tweaking the loading of the resource seems to be enough.

But it looks like my GraphDB weaving model has no virtual associations!  Opening
the weaving model... they have the same content as the XMIs.  What's going on
here?

Ahhh I get it.  I generated the model code for the NeoEMF weaving model, but I
forgot to implement the derived links!

That's better.

Now I get an NPE.  ConcreteConcept.getModel is null.  It is unset in the GraphDB
resource.  Wait no, that's weird.  First time I look it up, it's empty, second
time, it has the correct value!  I can see that behavior using the NeoEMF
browser.

Calling ~getModel~ multiple times in code always returns null apparently.  So
does a loop.

Maybe bidirectional containment references have issues?  Bah, I can just use
~eContainer~ on the object instead of ~getModel~ then.

And it works!  The transformation is run on the view, the weaving model and the
trace model are NeoEMF resources.

* [2018-04-11 mer.]
** Running transformations too large for XMI                :neoemf:emfviews:
The 10^6 model is the first to OOM on the 512MiB JVM.  Maybe more interesting is
to look at the memory usage over time for the 10^6 XMI (so we know the max
memory it uses).

Looks like the max size of the used heap is around 3.5GB.

Profiling the run for 30min, and it seems most of the time is taken by EMFTVM
executing the transformation.

[[file:neoemf-integration/profile-atl-transfo.png]]

Now, the issue is that I can't even run the transformation with a NeoEMF weaving
model for the 10^6 model, since I don't have it.  I'm not confident it will even
finish in a night, since creating the XMI weaving model with a NeoEMF trace for
10^6 did not finish in over 15 hours.

Maybe profiling the creation of the weaving model might give insight as to why
this is taking so long.

** Importing the XMI weaving model into GraphDB             :neoemf:emfviews:
Since building the NeoEMF weaving model takes a long time, maybe importing from
the XMI is faster?

I tried to use the example importer code [[https://github.com/SOM-Research/NeoEMF/blob/master/examples/fr.inria.atlanmod.neoemf.demo/src/fr/inria/atlanmod/neoemf/demo/importer/EfficientBlueprintsImporter.java][here]], but am running into exceptions.
It looks like only the original WeavingModel implementation is used.  For
loading the XMI, it makes sense.  But for saving the NeoEMF resource, it should
use the NeoEMF implementation.  Ah, but here's the issue!  When moving the
contents from one resource to another, we should switch implementations!

I have no idea how this is supposed to work.  In fact, I don't even know how it
worked in the first place when we built a Trace NeoEMF resource from an XMI one.
Other than we generated an implementation with the exact same namespace URI, so
maybe there's some magic trick going on behind the scenes?

* [2018-04-12 jeu.]
** Let constructing the NeoEMF weaving model run overnight  :neoemf:emfviews:
Still no dice.  The 10^6 does not look like it will terminate in due time.

One thing I notice in the memory monitoring is that even though the max heap
space is indeed 20GiB, the maximum size the heap will use is still 8GiB.  And we
can see that the JVM will never go above.  Maybe ~-Xmx~ is not the right flag?

Passing ~-Xms20g~ as well makes sure that the heap uses its max allocated space
right from the start.  The program may not use all of it however.

One more interesting setting is to change the GC, as used by NeoEMF benchmarks:

: -XX:+UseConcMarkSweepGC

The GC will run in another thread and collect garbage.  The difference in memory
usage is staggering.  Here is the default GC:

[[file:neoemf-integration/default-gc-neoemf-weaving-model.png]]

The shark fins go up to 6GiB here.  Many objects are created and collected.  We
can clearly see the GC pauses.

Here is the same program (creating NeoEMF weaving models) with the concurrent
mark and sweep:

[[file:neoemf-integration/conc-gc-neoemf-weaving-model.png]]

Maximum heap used is around 300MiB.  But the CPU usage has bumped by 20%, and
we can see the GC is continuously being run.  Trade-offs.

** Importing XMI weaving models into GraphDB                :neoemf:emfviews:
Okay so the missing bit of information from yesterday was that when importing
XMI into GraphDB, we are using only one EPackage: the NeoEMF one.  So you have
to tweak the XMI to change the namespace URI in there.

But, it doesn't help me much actually, because I /don't have/ a weaving model
for the 10^6 NeoEMF trace model either.  It took too long.

** Running ATL transfo on 10^5 NeoEMF weaving model          :neoemf:emfviews:
Looks like it could take a while.  I'll run it overnight.

I profiled a few minutes to see where it spent time.  Same thing as creating the
weaving model: we spend all our time in iterating over the contents of the
resource.

I tried with the concurrent mark and sweep GC.  The profiling result is slightly
different.  Here is with the default GC:

[[file:neoemf-integration/profile-atl-transfo-neoemf-weaving-model-default-gc.png]]

Here is with ConcMarkSweepGC:

[[file:neoemf-integration/profile-atl-transfo-neoemf-weaving-model-conc-gc.png]]

- ~RelationshipImpl.hasProperty~ jumps to 53% of total time, from 17%.
- ~lookupRelationship~ was dominated by self time, but is now dominated by
  getRelationshipForProxy.

The WeakLruCache here might be a clue: maybe the concurrent GC collects objects
too often, and the cost here is that the implementation is constantly adding
things back to an internal cache.

So, probably the time of execution is increased by a good amount by using the
concurrent GC.

Anyway, all of this is done in View.doLoad, because we eagerly populate the view
using the weaving model links.  We could defer that, but it's going to be
tricky.

In this specific case, we could defer this computation until either end of the
virtual feature is hit (until its size or its contents are requested).  We can
probably generalize that to any virtual association.  But we would still have to
go through /each/ virtual association in order to create reminders for deferring
the computation.  Since iterating on the weaving model is the bottleneck right
now, such deferring would not improve anything.

The least we could safely do would be to defer looking into the weaving model
until we create the view contents.

* [2018-04-13 ven.]
** Running the ATL transformation up to 10^5                 :neoemf:emfviews:
*** Java weaving model / NeoEMF trace
#+NAME: run-atl-neoemf-trace
| Size | Load | Transform |    Full |
|------+------+-----------+---------|
|    1 |  896 |       917 |    2072 |
|    2 |  329 |       617 |    1143 |
|    3 |  533 |      1820 |    2890 |
|    4 | 1346 |     58789 |   60334 |
|    5 | 6786 |   5616109 | 5623177 |

#+begin_src gnuplot :var data=run-atl-neoemf-trace :file neoemf-integration/bench-run-atl-neoemf-trace.png
set title 'Run ATL transformation (NeoEMF Trace)'
set key outside

set xtics 1 to 5
set xlabel 'model size (10^n)'
set logscale y
set ylabel 'time (ms)'

plot data u 1:2 w lp title 'Load', \
     data u 1:3 w lp title 'Transform', \
     data u 1:4 w lp title 'Full'
#+end_src

#+RESULTS:
[[file:neoemf-integration/bench-run-atl-neoemf-trace.png]]

*** NeoEMF weaving model / NeoEMF trace
#+NAME: run-atl-neoemf-weaving-and-trace
| Size |    Load | Transform |    Full |
|------+---------+-----------+---------|
|    1 |     228 |       698 |    1148 |
|    2 |     238 |       963 |    1458 |
|    3 |     575 |      1468 |    2230 |
|    4 |   23394 |     65338 |   88937 |
|    5 | 2157699 |   4944843 | 7102815 |

#+begin_src gnuplot :var data=run-atl-neoemf-weaving-and-trace :file neoemf-integration/bench-run-atl-neoemf-weaving-and-trace.png
set title 'Run ATL transformation (NeoEMF weaving model & trace)'
set key outside

set xtics 1 to 5
set xlabel 'model size (10^n)'
set logscale y
set ylabel 'time (ms)'

plot data u 1:2 w lp title 'Load', \
     data u 1:3 w lp title 'Transform', \
     data u 1:4 w lp title 'Full'
#+end_src

#+RESULTS:
[[file:neoemf-integration/bench-run-atl-neoemf-weaving-and-trace.png]]

** Running with softCache                                   :neoemf:emfviews:
When building the NeoEMF weaving model with a NeoEMF trace.

Here where the numbers with weakCache/softCache, for the 10^5 trace model:

| Cache | javaClass |    Full |
|-------+-----------+---------|
| weak  |   3049003 | 3076031 |
| soft  |   1661562 | 1698940 |

Nice improvement.  Still slow though.

* [2018-04-17 mar.]
** Trying out NeoEMF master                                 :neoemf:emfviews:
(i.e., atlanmod/NeoEMF)

To see if the API is that different for our purposes, and to see if there are
performance improvements.

Just cloning and building from master is enough to install it.  It appears I
don't need to register the BlueprintsURI.SCHEME anymore, which is good.

It appears I don't need to regenerate model code, as it looks the same (at least
for the trace model).

However, the databases are incompatible between versions.  Need to regenerate
them all.

Ah, it looks like creating the trace resource takes much longer than NeoEMF
1.0.2.  36s for 10^4 elements, against 4s.

Need to test building the weaving model and running the transfo as well.

Here are the full numbers:

| Size |    Full | GraphDB size |
|------+---------+--------------|
|    1 |    6677 | 416KB        |
|    2 |    1497 | 548KB        |
|    3 |    2415 | 2MB          |
|    4 |   35925 | 16MB         |
|    5 | 3190291 | 168MB        |

So it takes twice as long, and take twice as less on disk.

* [2018-04-18 mer.]
** NeoEMF master weaving model benchmarks                   :neoemf:emfviews:
*** XMI weaving model with NeoEMF trace

| Size | load | populate | save |   full |
|------+------+----------+------+--------|
|    1 | 1149 |      166 |   18 |   4658 |
|    2 |  151 |      123 |   10 |    481 |
|    3 |  124 |      506 |   41 |    866 |
|    4 |   96 |    11181 |  134 |  11593 |
|    5 |  113 |   945415 | 2422 | 948163 |

*** NeoEMF weaving model with NeoEMF trace

| Size | load | populate |    save |     full |
|------+------+----------+---------+----------|
|    1 |  101 |      149 |    1082 |     1632 |
|    2 |   95 |      228 |    1198 |     1722 |
|    3 |  148 |     2106 |    2277 |     4718 |
|    4 |   98 |   102354 |   43393 |   146230 |
|    5 |  179 | 10056899 | 3853064 | 13912304 |

*** Conclusion
Takes much longer than SOM NeoEMF.  There might be magical options that behave
nicely for this example but... I'm skeptical.

** NeoEMF master ATL transfo benchmarks                     :neoemf:emfviews:
*** With XMI weaving model

| Size | load | transform |    full |
|------+------+-----------+---------|
|    1 | 4294 |      1664 |    6727 |
|    2 |  472 |       795 |    1621 |
|    3 |  388 |      1998 |    2711 |
|    4 |  448 |     36373 |   37096 |
|    5 | 1069 |   3334126 | 3335569 |

*** With NeoEMF weaving model

| Size | load | transform |    full |
|------+------+-----------+---------|
|    1 |  522 |       515 |    1296 |
|    2 |  515 |       468 |    1252 |
|    3 |  291 |      1551 |    2093 |
|    4 |  298 |     45316 |   45879 |
|    5 |  323 |   4069452 | 4070129 |

** NeoEMF master is a mixed bag                             :neoemf:emfviews:
It's slower to create models, slower to create the DB weaving model, but faster
for the XMI weaving model.  And it's also faster when running the
transformation.

Overall though, there are no order-of-magnitudes improvements.  It's still in
the same ballpark as SOM-NeoEMF.

So, for the paper, it does not matter if we stick to NeoEMF 1.0.2: everything is
valid for NeoEMF master.

For implementing with EMF Views on the long run, of course we need to use NeoEMF
master.  The good thing is that we don't need as much ceremony to setup NeoEMF
master, so the integration with EMF Views is less intrusive.

* [2018-04-19 jeu.]
** Mirror dropbox into git                                      :git:dropbox:
I don't trust dropbox conflict handling (or rather, I find it inferior to git
merging).  Also, it's easier to check a git diff than using whatever Dropbox
has.

Luckily, I can just create a git repository inside the shared dropbox folder to
save changes.  Then, for syncing I use a script that calls ~rclone~ like so:

#+BEGIN_SRC sh
case $1 in
    pull)
        SRC=dropbox:$DIR
        DST=$DIR
        ;;
    push)
        DST=$DIR
        SRC=dropbox:$DIR
        ;;
    *)
        echo './drop.sh pull|push'
        exit 0
        ;;
esac

rclone --exclude-from $DIR/.gitignore --exclude '.git/' --exclude '.gitignore'\
       copy $SRC $DST
#+END_SRC

Locally I work with a git, and remotely the files are pushed to dropbox.  I can
also add a proper git remote for additional safekeeping for instance.

** Doing interesting things with calc                                 :emacs:
I wanted to convert a column in LaTeX from milliseconds to minutes.  Well,
there's an Emacs command for that.

First, select the region using rectangle mark mode ~C-x SPC~, then ~C-x * r~ to
paste that as a matrix into calc.

#+BEGIN_EXAMPLE
      1632
      1722
      4718
    146230
  13912304
#+END_EXAMPLE

In calc you get:

#+BEGIN_EXAMPLE
1:  [ [   1632   ]
      [   1722   ]
      [   4718   ]
      [  146230  ]
      [ 13912304 ] ]
#+END_EXAMPLE

Then ~u c ms min~ to convert the whole matrix (here I'm using fixed floating
point format with ~d f 3~):

#+BEGIN_EXAMPLE
1:  [ [  0.027  ]
      [  0.029  ]
      [  0.079  ]
      [  2.437  ]
      [ 231.872 ] ]
#+END_EXAMPLE

From there you can grab the column in the matrix using ~C-x SPC~ again, (or ~C-x
r M-w~), then replace the original column using classic rectangle commands.

Also, I obtained the LaTeX tables from org using ~org-latex-export-as-latex~
with the table as region, which generated a standalone LaTeX file in a temp
buffer, and I extracted the table from that.

* [2018-04-20 ven.]
** Meeting with G. and H.                                   :neoemf:emfviews:
A language for access to distant resources.  Do we need one?  What is that?  Can
we access an EMF resource that is on another machine using EMF Views?  What is
the point of [[https://som-research.uoc.edu/tools/emf-rest/][EMF REST]]?  We do not know.

A more interesting pursuit is specializing OCL requests depending on the backend
managing the resource.  Here are some leads:

- Modify the OCL interpreter by extending classes that implement each operation,
  in order to specialize them depending on the resource.

- Capture and specialize the calls to part of the EMF API on EMF Views directly.
  E.g., identifying an allInstances call and forwarding that to
  PersistentResource.getAllInstances

- Look at Hawk's EOL interpreter for inspiration

- Use Mogwai to convert an OCL request into Gremlin, and implement
  [[https://github.com/atlanmod/Mogwai/blob/master/fr.inria.atlanmod.mogwai.datastore/src/fr/inria/atlanmod/mogwai/datastore/ModelDatastore.java][ModelDataStore]] to target different backends.

Ideally, we want to run an optimally efficient OCL query by taking advantages of
backend-specific APIs.  That might not be feasible for all OCL, but it would be
interesting to determine how much we can optimize, and how much we cannot.

A couple of examples to get started:
- Type.allInstances().  This is very slow when using a NeoEMF resource via the
  EMF API.  It can be much faster when using PersistentResource.allInstances.

- select->collect->select.  Naive OCL interpretation will apply the collect
  function on every element of the first select, even though some of them might
  be filtered out by the second select.  In other words, we want collections to
  lazily create a request plan for databases.

One potentially tricky part is handling OCL queries that touch multiple
backends:
- following a virtual reference from a NeoEMF model to an XMI one.
- loading multiple resources of the same metamodel.  Log in XMI and Log in
  NeoEMF for instances.  Log.allInstances() should then collect from both
  resources, but using different backends.

Another point of discussion is how further lazyfication of EMF Views.  We
currently iterate through the whole weaving model when the contents are
requested.  I'm pretty sure that delaying that further will break some EMF
invariants if we are not careful.

** Running an allInstances OCL query                    :neoemf:emfviews:ocl:
[[file:neoemf-integration/benchmarks.org::*Running%20a%20simple%20OCL%20query][Results]].  Unloading an XMI resource is insanely slow.  Running that query on the
XMI does not even sweat OCL at 10^6: 500ms.

Now the NeoEMF resource.  Hmm, for some reason the models fail to load now?
Have I done something?  I seem to need to recreate them.  /sigh/.

* [2018-04-23 lun.]
** I don't need unload                                      :neoemf:emfviews:
NeoEMF resources need to be closed, not unloaded.  According to the doc, unload
turns all references in the resource into proxy.  It's useful if one wants to
reload the resource from the underlying file.  But it's so slow, I don't know
why one would ever want to do an unload() -> load() instead of loading the
resource anew and drop the previous one.

** Benching OCL queries                                 :neoemf:emfviews:ocl:
Reading the documentation of OCL, I see that there are two implementations: a
"classic" one that was hacked to work with UML, and a "modern" one (called
Pivot) which is supposedly an improvement on the former.  The Pivot has been the
preferred API since Eclipse Mars.

The API I'm calling right now is the classic one.  I ought to try the Pivot to
see if there are any differences.

Created a view that wraps only the NeoEMF trace resource.  OCL fails to find any
instance with a view, and returns an OCLStandardLib instead of a HashSet.
Debugging...

So allInstances has its own special case in
EvaluationVisitor.visitOperationCallExp, which uses the extent map:

: return getExtentMap().get(sourceVal);

An "extent" is a instance of a class in OCL parlance, so the extent map keeps
track of all instances of a classifier in a model.  When the extent map does not
have an entry for a classifier, it looks in the whole resource to find them:

#+BEGIN_SRC java
for (Iterator<Notifier> iter = EcoreUtil.getAllContents(roots); iter.hasNext();) {
  @SuppressWarnings("unchecked")
  E next = (E) iter.next();

  if (isInstance(cls, next)) {
   result.add(next);
  }
}
#+END_SRC

Ok this fails in ~iter.next()~ because of LazyEContentsList that throws
UnsupportedOp.  I'll never understand why people hide original exceptions and
throw another one instead.  Anyway.

Returning iterator() does the trick.

Now it fails because it calls isInstance, and has an EClassifier from the
concrete package, not the viewpoint.  It's surely the context of the OCL query
that determines this, and we need to give it classifiers from the viewpoint.
Doing that:

:  oclHelper.setContext(resource.getContents().get(0).eClass());

and OCL correctly associates the ~Log~ part of the query to the VirtualEClass
Log.

Trying to change the extent map to use the getAllInstances method of
PersistentResource to see the performance benefit.  You cannot just change the
extent map of the /query/, as it has wildcard captures, and I can't find a way
to type it.  But I can use ocl.setExtentMap, before creating the query.

Hmm but the problem here is that when I run the OCL query on a view, I have no
way of knowing that the view is using NeoEMF resources behind the scenes.  So I
can't optimize the getAllInstances call on the client side.

Conversely, EMF Views does not know whether a basicIterator call is used to
effectively answer to an ~allInstances~ from OCL.  In fact, EMF Views does not
even know OCL is trying to access it, so there's no opportunity for
optimization.  There's an API mismatch.

Trying out the getAllInstances on NeoEMF directly, it's ~400ms instead of
~17000ms, so totally worth it.  But populating the extent map feels like
cheating...

In this case it's bad, because my OCL query is simply:

: Log.allInstances

So calling that on the resource gives me the answer.  But what if we need to
iterate on instances of different classes?  Populating the extent map
eagerly by looking at the OCL AST is not a great solution either.

*** Using Pivot OCL
Let's try the Pivot OCL.  The API is a bit nicer for my purposes.  For the
NeoEMF trace, it's still ~16s, so no improvement on that front.  Doesn't work
out of the box with views however.

:  org.eclipse.ocl.pivot.utilities.SemanticException: The '<unknown>::<unknown>' constraint is invalid: 'Log.allInstances()->size()'

Looks like it cannot find the Log class this time?

The sources to Pivot are not included in the OCL SDK.  Gotta fetch the git and
checkout a revision from around my version:

: git checkout (git rev-list -n 1 --before="2017-06-07" master)

Ok now it works.  I had to implement VirtualEClass.eResource and
VirtualEPackage.getEAnnotations.

I see that for class extents they delegate to a ModelManager, maybe there's a
way to configurate that for views?  Hmm it's trickier than expected, because I
cannot simply fallback on the PivotModelManager unless I create one
myself... and I need to understand what to pass it.

However, I could use the same strategy as the ModelManager in the classic OCL
implementation: just provide an extent map that specializes the call for
persistent resources.

* [2018-04-24 mar.]
** Overriding the extent map for classic OCL            :neoemf:emfviews:ocl:
#+BEGIN_SRC java
ocl.setExtentMap(new LazyExtentMap<EClass, EObject>(context) {
  @Override
  public Set<EObject> get(Object key) {
    if (resource instanceof PersistentResource) {
      return new HashSet(((PersistentResource) resource).getAllInstances((EClass) key));
    } else {
      return super.get(key);
    }
  }

  @Override
  protected boolean isInstance(EClass cls, EObject element) {
    return cls.isInstance(element);
  }
});
#+END_SRC

This works nicely.  For the 10^6 NeoEMF resource, I can get the results in a
reasonable 23s.

To make that work on views however, I would need to extend the views API with a
getAllInstances as PersistentResource does.  This is fine for allInstances, but
what about other optimizations?  For instance, lazifying
~select->collect->select~?  It seems there is no other course of action than
modifying OCL or providing an alternative interpreter (like Mogwaï does the
translation into Gremlin).

* [2018-04-25 mer.]
** Specializing allInstances for views                  :emfviews:neoemf:ocl:
This is possible by customizing the extent map, as I did for NeoEMF.

For views however, we have to do slightly more work.  Take the request:

: Classifier.allInstances()

That will end in a ~getExtentMap().get(Classifier)~ call from OCL.  To provide
all instances of that classifier, we need to know in which resource it is
located.

- If the classifier is filtered out at the viewpoint level, then it's an error

- If it's a synthetic concept, the instances are in the view
- Otherwise it comes from one or more resources
  - Call allInstances for each resource (using optimized API if available)
  - Union the results

If instances are filtered at the model level, then we should apply filters here
once again.  But we haven't implemented model filters yet.

I've implemented that.  Now we can run the allInstances OCL query in 754ms for
the 10^4 trace, instead of 21659ms previously, and we can run it on the 10^6 model
in 16s.

It's not all roses though.  Here we will build and load the result of the
allInstances operation into memory, /even though/ we only care about its size in
the OCL query.  So there's no need to actually load the whole thing in memory.

As I understand it, Mogwaï does things differently.  It computes a request plan
for the database and completely bypasses in-memory computation.  The whole OCL
request is done on the DB, and its result can even be saved in DB as well.

Doing that for heterogeneous views will be more involved.

** Example of problematic OCL queries                   :emfviews:neoemf:ocl:
- Log.allInstances()
  When a view has multiple trace resources

- Log.allInstances()->select(l | l.javaClass)
  javaClass is a synthetic reference to an element from an XMI model

- Log.allInstances()->select(l | Class.allInstances()->exists(c -> c =
  l.javaClass))
  Very slow if naively executed

* [2018-04-26 jeu.]
** OCL on chained view is broken                               :emfviews:ocl:
:(

The ~Log.allInstances()~ query works on the view that wraps the trace model
resource.  But for the view that aggregate multiple resources, it fails to
find the classifier associated with ~Log~.

Debugging...

EcoreEnvironment.lookupClassifier([Log]) returns null

For the single-resource view, it returns the correct VirtualEClass.

Ah, it looks for classifiers in the context package, which is inferred from the
context object.  For the single-resource view, there's only one package and it
works.  For the many-resources view, the context package is the VirtualEPackage
around ReqIF, since the context object is the first element that comes in the
view.

That's tricky to fix.  I could put all contributing packages in a view
subpackage, but that will probably break other stuff.  The easiest fix is to
pass a context object from the correct package in the view.

Aaaah, but others problem appear down the road.  If I build a query that uses
types from different packages in the view, then OCL is unable to locate one of
them, regardless of the context object I pass.  This reliance on one package is
a real blocker.

* [2018-04-27 ven.]
** OCL accepts namespaces                                      :emfviews:ocl:
So I can write ~reqif10::SpecObject~ and ~java::ClassDeclaration~ in the same
query, and it will pick up the correct classifiers.

Ah, but for the java one, it picks up a regular EClass, not the virtual one.
Since the context package is the reqif10 one, it does not find a match, and
simply looks up packages in the registry.

I can try creating a root virtual package.  But I have to make sure that
hierarchy is correctly established (virtual packages have the root as
superpackage, and vice-versa, and concrete packages are not affected).

It seems to work!  Luckily, OCL does the right thing and goes back to
sub-packages after going up the root to find classifiers.  So it works without
being an ugly hack on our side.  It broke a few tests which relied on the
previous viewpoint structure though.

The other query doesn't work, mysteriously.  I get a parse error for the
~package~ attribute.  Could it be... a reserved word?  Looks like ~zorglub~
parses fine, even though it's /not/ an attribute from the metamodel.

Yep, ~package~ is reserved in the Complete OCL language (which is not the one
used for queries as I understand it).  We can quote feature names using:

: _'package'

Curious syntax, but it works.

Now I have issues of non-deterministic results.  I'm picking the first Log, and
looking its requirements.  But using:

: trace::Log.allInstances()->asSequence()->first().message

gives me different results for different models.  So I should pick one specific
Log instance first.

Okay doing that, I can finally run both queries.  Seems to work on the XMI and
NeoEMF views.

10^6 takes forever for NeoEMF though.  All CPUs at 100% while creating the view
contents.  It's curious, as loading the view for the load view benchmark was not
an issue.  Ah wait, I didn't have any numbers for loading the 10^6 NeoEMF view.
Might want to run and profile that first.

* [2018-04-30 lun.]
** Running views/OCL query benchmarks                   :emfviews:ocl:neoemf:
7min for getting all elements in a DB view, against 33s for an XMI view.  Not
great.

40min for the ~allInstances~ OCL query on a DB view, not great either.

I noticed we could use the ~getAllInstances~ of PersistentResource in
View.getContents to speed that up a bit.  Down to 15min now.

We are still spending time fetching referenced objects from virtual links.  Not
sure how to speed up that, especially on short notice for the paper.

Instead, I'd like to lazify most of this, looking at the viewpoint weaving model
in order to know which features can potentially require looking in the view
weaving model.  This in order to avoid paying the upfront costs of going through
the whole view weaving model when loading the contents.

Another surprise in the benchmark is that I'm already seeing a 100x slowdown
from XMI to XMI view, while there is only a 10x slowdown going from NeoEMF to
NeoeMF view.

So I improved the view weaving model to lazily populate features the first time
they are queried.  We go from the 15min above to 20s.  Exciting!

Even the reqToTraces query takes /only/ 49s.

This invalidates most of the benches involving views, but these are great
numbers, at last.

* [2018-05-16 mer.]
** Creating views without files                                    :emfviews:
Currently, View and Viewpoint are resources.  Resource is part of the
persistence API of EMF.  A resource is associated with a URI, and its more
important methods are ~save~ and ~load~.

But a View and a Viewpoint could be created directly from memory, without a
backing eview/eviewpoint file.  The question is: do I extract a class and
compose it in a ViewResource, or do I allow the View to be created without a
backing file?

The scenarios we should support are:

1. Load view/viewpoint from a file, as before.  The use case is creating
   views in Eclipse.
2. Load view/viewpoint by providing the contributing metamodels, models, and
   weaving models programmatically.  No files are involved.
3. Load view/viewpoint from files, and the weaving model and virtual objects
   should reside on file or in a database.  This is to support very large views
   that would not fit in memory otherwise.

I think 3. is overblown, but would be nice to have.  I think the idea is to
retain as few things as possible in memory, but I'm afraid doing things in
memory is too ingrained in the current code.  Besides, I'd rather focus on
correctness right now than scalability.

One advantage of decoupling the persistence from the view/viewpoint logic is to
avoid dealing with URIs and loading the resource in our code.  Rather, we let
clients of the API load the resources as they see fit.  This means that
integrating with persistence frameworks like NeoEMF is a breeze.

The open question is how do I reconcile 1. and 2.  Let's say I create a
Viewpoint directly, without a backing resource.  What if I want to save that
viewpoint?  Can I add it to any resource?  No, because it isn't an EObject.
Could it be an EObject?  All virtual elements are EObjects after all.  At the
Viewpoint level, I could see us creating an EPackage.  But the Viewpoint
/creates/ another EPackage, and this one is dynamic.  Which one do we want to
serialize when we add it to a resource?

Serializing the virtual EPackage is of little interest, since virtual elements
are immutable, all the information is already contained in the weaving model.

One could argue that virtual elements are the /expanded form/ of the weaving
model, and that future changes in the way views work might break this
expansion.  For that reason, it /could/ be interesting to serialize expanded
viewpoints directly.  But at this stage of development, it would mean additional
maintenance burden to ensure backwards compatibility.  Not worth it.

So the serializing story we have now is good enough.

Still, if I want to save a Viewpoint created entirely in memory, isn't the
simplest API to just do:

: vp.setURI("foo.eviewpoint")
: vp.save();

Rather than creating a resource for it?  E.g.:

: r = new ViewpointResource("foo.eviewpoint")
: r.setViewpoint(vp)
: r.save()

Well, it's not much more complicated.  And it decouples the two neatly.  Let's
do this.

Okay, it works.  It's neat.  I took the opportunity to do error reporting using
the resource diagnostics rather than exceptions.  The UX is better when loading
viewpoints now, since you can have an invalid eviewpoint file that still allows
you to create a viewpoint, and the error is visibly reported.

Now I need to do the same thing for View.

* [2018-05-28 lun.]
** Various improvements                                            :emfviews:
I did the same thing for views.  We can now create views without eview files.
All in memory.  I added tests for that, etc.

I also removed the idempotence of virtualizers.  Turns out it was the wrong way
to solve the problem I had.  It allowed me to not care too much about
virtualizing already virtual objects, but it also prevented the creation of
viewpoints on viewpoints, and views on views...

In fact, removing the idempotence and adding guards to check for double
virtualizing helped me catch subtle bugs.

Although, now that I've removed this safeguard, I'm worried we might too easily
do double virtualizations again in the future.  The good thing is that it
shouldn't be incorrect, just unnecessary and inefficient.

I've added tests for views on views and viewpoints on viewpoints.  Sexp2EMF was
helpful there.  All of this can work without touching the filesystem.

Now, I'm trying to please the Ecore validator to ensure metamodel Ecore
contraints are satisfied when creating viewpoints.  The idea is that a viewpoint
/should/ validate these constraints.  Filters throw a wrench in that, and I'm
not yet sure how if we should allow invalid viewpoints or not.  But at least,
with the validation working I'll be able to tell how bad filters mess things up.

** Pleasing the Ecore validator                                :emfviews:emf:
There's not much.  Missing nsPrefix, and having containing features return
EStructuralFeature.Setting instances.

Virtual packages need an EFactory instance as well.  For now, I'm returning an
instance that forbids creation of view objects.  It wouldn't be impossible to
support that, but then if we allow creating view objects, we would have to think
about serializing them, and /modifying/ them, as well as existing objects.  That
might be tricky to get right with opposites, virtual opposites, etc.

The only other thing I'm encountering for Ecore validation is that virtual
classes do not have ePackage set properly.  That surprised me.

Writing a test for that... it passes.  But wait, the Ecore validator is using
the reflective API.  And yes, looking up the ~ePackage~ feature returns null.

Why is that?  ~eGet(VirtualEClass, ePackage)~ will end up in
EStructuralFeature$InternalSettingDelegateSingleContainerResolving.dynamicGet:

#+BEGIN_SRC java
return owner.eContainmentFeature() == inverseFeature ?
   (isResolveProxies() && resolve ?
     owner.eContainer()
     : owner.eInternalContainer())
   : null;
#+END_SRC

Where ~owner~ is the VirtualEClass, and inverseFeature is the ~eClassifiers~
feature (inverse of of ePackage).  And here's the issue: eContainmentFeature is
virtualized, and inverseFeature is not, even though they refer to the same
feature.  So the result is null.

It could be fixed by virtualizing the right-hand side, or not virtualizing the
left.  I did not manage to hook into the creation of the InternalSettingThingy
using the debugger, so I do not know exactly when that value is set, and whether
it could be virtualized.

But actually, it might make more sense to /not/ virtualize the left-hand side.
The references are part of the Ecore metamodel proper, not a virtualization of
it.  In fact, I'm wondering why we have to override the default implementation,
which should look up the feature based on the internal container...

Okay, the internalContainer is null.  That might be the reason.

These were added by 20142606b75deb00c16eaeba1f4dfdeb5fd24b16, with message:

: Fix NPE when browsing Viewpoint with the Sample Ecore Editor

So, the Sample Ecore Editor needs that to return non-null.  But the
implementation might not be correct.

Not virtualizing the left-hand side seems correct to me.  There might be a
deeper fix looking at why internalContainer is null, but unless that causes
another issue, I'm happy to leave it at that.

Now ~instanceTypeName is null~.  What should it be instead?

#+BEGIN_QUOTE
The instance type name may be null only for a class or an enum must be well
formed when not null, and must not specify type arguments if the classifier
specifies type parameters.
#+END_QUOTE

So.. if you've got a DataType that's /not/ an enum, then it should not be null?
Hmm, adding a conditional breakpoint, this case never prevents itself.

However!  We virtualize EEnums into VirtualEDataType, which is /not/ an instance
of EEnum, so the validator believes we have plain EDataTypes.

The solution is to create a VirtualEEnum class.

Now I've got some diagnostic in the resource without a message...  Ok, that was
caused by a Viewpoint throwing an exception.  Apparently, ~ex.getMessage()~ can
be null.  Using ~ex.toString()~ is more reliable.

Now it wants to cast VirtualEEnum into
BasicExtendedMetaData$EClassifierExtendedMetaData$Holder.

* [2018-05-30 mer.]
** Extended metadata holders and other niceties                :emf:emfviews:
This is required by the default value literal check.  It checks the default
value of a feature against its type.

I can easily delegate that to the concrete classifier, but now the validation is
failing, claiming that all default value literals are of the wrong type.

Ah!  That's because I half-implemented VirtualEClassifier.isInstance.  I did not
handle primitive types.  Delegating to the concreteClassifier.isInstance calls
is enough.

Now only one test is failing.

: The feature 'eClassifier' of 'EGenericTypeImpl@7afb1741' contains a dangling reference 'EClassImpl@47447ccf{#//C}''

* [2018-06-01 ven.]
** Writing help for EMF Views                              :emfviews:eclipse:
So I want to write a user/dev manual for EMF Views.  Ideally, this manual would
be hosted on the website and available via the offline Eclipse help.  Since both
use HTML, that's feasible.

I would author the document using some markup (haven't settled yet), and
generate HTML.  That would be enough for pushing to the website.  For the
Eclipse help, there's additional ceremony (as always...): you need to produce a
plugin with a toc.xml table of contents file.

In theory, the TOC can be extracted from the HTML (or the authoring markup).  I
found [[https://www.bsi-software.com/us-en/eclipse-scout-blog/article/generating-eclipse-help-from-existing-html-documentation.html][this]].  I probably don't want to do that with Java.  On the other hand, I
probably want to build the documentation on Travis and deploy from there.

One unknown is how to deploy on gh-pages from Travis.  There is [[https://docs.travis-ci.com/user/deployment/pages/][this]].  So Travis
can make a commit with the built HTML pages, and push that to gh-pages.  This
should work fine if Travis is the only one pushing to gh-pages.

If, on the other hand, I'm also pushing to gh-pages since it will host pages
other than the manual...

Well, if the commit is made on HEAD, then it shouldn't be an issue.  It seems
that using the ~keep-history~ setting should do that.  Too bad the official way
of deploying requires the Github access token, which is probably too powerful
(since there's no way to constrain it to a particular repo).  So I cannot use it
directly, and must resort to a more [[https://gist.github.com/domenic/ec8b0fc8ab45f39403dd][manual]] deployment.

*** Security concerns
The Travis docs suggest generating a Github token and providing it to the Travis
build (encrypted, of course).  [[https://gist.github.com/domenic/ec8b0fc8ab45f39403dd][Here]] it is recommended to generate a new SSH key
when committing from Travis.  Here is why:

#+BEGIN_QUOTE
an earlier version of this guide recommended generating a GitHub personal access
token and encrypting that. Although this is simpler, it is not a good idea in
general, since it means any of your repository's collaborators would be able to
edit the Travis build script to email them your access token, thus giving them
access to all your repositories. The repository-specific deploy key approach is
safer.
#+END_QUOTE

And now I'm thinking that the same issue affects our flycheck/docker-tools
build.  I've put my credentials in there (not hub/flycheck, because there are
none!), and anyone with push access can get the decrypted value by modifying the
travis build script.

The alternative seems to be to [[https://docs.docker.com/docker-hub/builds/#repository-links][let Docker Hub build]] the images itself, but then
I have to link my Hub account with Github.  Who do I trust more?

* [2018-06-04 lun.]
** Writing help for EMF Views with org                             :emfviews:
So, Org was among my top choices.   Markdown is too poor for writing decent
help (not customizable links, no publish feature, etc.).

Sphinx was a close second.  Mostly, I do not like its markup.  Otherwise, the
tooling is good.  Potential customization.  Ability to build in batch.

In fact, sphinx is probably simpler to build in batch and on CI than Org.  But
I know elisp well enough to make that happen.

Okay so org-publish works nicely for now.  Will test creating an eclipse help
plugin from its contents tomorrow.

* [2018-06-05 mar.]
** Writing Eclipse help with org                       :emfviews:eclipse:org:
So yeah, it works.  Just ~org-publish~ into the ~html~ folder of a created
Eclipse plugin.  I manually added a toc.xml file which points to the two manuals
I'm writing.

I tried to generate a full table of contents from the exported HTML but... org
makes thatnot as easy as I'd hoped.  I looked at ~org-html-toc~, and it gets a
list of headlines, but I need a tree.

* [2018-06-12 mar.]
** More writing of help                                :emfviews:eclipse:org:
These things take time.

Yesterday I toyed with the styling a bit.  Went back and forth on the styling of
code snippets.  Using a box is tricky, because code can easily overflow it if a
different font is used (and I don't want to set fonts using webfonts).  So I
have to make sure that we are not near the max width of the box.

Writting a VPDL snippet.  It occurs to me that I want syntax highlighting.  Org
mode is nicely able to use whatever syntax highlighting Emacs has, and using the
htmlize package, Org can just output spans with classes, without doing any
styling.

That's convenient, since I can then define my own style using these classes, if
I want to.

What's less convenient is having to install htmlize.  But, luckily, Cask has
been made for this.  I can make a minimal Cask file:

#+BEGIN_SRC Cask
(source gnu)
(source melpa)

(development
 (depends-on "htmlize"))
#+END_SRC

And after ~cask install~, ~cask exec emacs~ will have htmlize loaded.  This way,
I make sure that I can build the doc on CI, since I know I can pull the
flycheck/emacs-cask Docker image and get going.  ~cask exec emacs~ is slightly
slower than ~emacs~ though, that's unfortunate.

What's also not convenient is that I don't have syntax highlighting for VPDL!  I
think I could hack it using org export filters, adding the classes directly to
the produced HTML output.  The right solution of course is to write my own
syntax highlighting for VPDL.  As much as I'd love to write more elisp, I
actually enjoy writing this manual, so I'd rather keep pushing.

I found a shortcut: the VPDL keywords I need are actually the same as SQL, so I
can just use sql-mode for the syntax highlighting.

Linking to an example block seems busted in the HTML export.  I've got a NAME
tag, and it's used as the exported ~id~ attribute for the ~pre~ HTML element,
but the link pointing to uses a funky generated id from org.  The link does work
in the org mode buffer.  Seems like a bug.  I can see that there are have been a
couple of minor versions released for Org 9.1 already, but I can't seem to get
Cask to install it as a dependency.

Linking to a SRC block works, so that's good enough for me.

*** Testing out interactive OCL for the help section                    :ocl:
Hey!  Turns out OCL has a great manual.  I will surely take it as an example to
structure EMF Views (more modest) manual.

In the manual, I found out about the interactive OCL console, which can be used
on elements under selection in the Sample Ecore Editor or MoDisco Browser.
Unfortunately it doesn't seem to work for views!  Even though running a query on
views programmatically works fine.

On concrete models it works, on views it doesn't.

* [2018-06-13 mer.]
** Trying to simplify the interface of views                       :emfviews:
to be created with models given as ~List<EObject>~ instead of ~Resource~.

It seemed to be the right thing, like Viewpoint only takes EPackage directly,
and not metamodel resources.

But, there's a snag: we rely on Resource.getEObject to locate objects in models.
I naively thought that EcoreUtil.getEObject was the same... but no.

It makes sense: I have noticed that BPMN has a funky ID scheme.  If they
override getEObject in their own resource implementation, they are free to
implement a custom scheme.

There's another issue preventing me from ditching resources: ECL acts on
resources as well, not just list of objects.  Although that one I can workaround
by providing resources to ECL, and list of objects to View in ViewResource.

But the first issue... I don't think I can fix easily.  In fact, it might be
wrong to provide something else than resources.

But I tried to do it anyway because I was having troubles creating a view on
EcorePackage.  See, EcorePackage is circular, so that was convenient for writing
the tutorial on creating views programmatically.  But turns out, I cannot add
EcorePackage to a new resource, since adding something to a resource changes its
container!

Which means that I cannot in fact create a view on EcorePackage as a model!

It's a weird corner case, but it's rubbing me in the wrong way.

I'll do the sane thing then, and just be a bit more verbose in the tutorial and
do a view on something else.

* [2018-06-26 mar.]
** Online deployment of manual: success!                :travis:emfviews:org:
After a dozen or so retries of building, because YAML syntax is terribly
confusing, I got it right.  [[https://gist.github.com/domenic/ec8b0fc8ab45f39403dd][This]] was a great help.

Now I need to revamp the webpage, then I can finish the TODO bits of the manual.

* [2018-06-27 mer.]
** Tweaks to manual                                            :emfviews:css:
Ugh, the org export was generating random identifiers for links, which meant
that the nice build setup I had on Travis was /always pushing/.  Even when the
manual source had not changed, the HTML output would be different because of
these random identifiers.

The best solution would be to get rid of them and use slugs, which are better
for humans anyway.  But, a low-cost solution is to simply set the random seed to
a constant value when publishing.  The doc says that random numbers might end up
being different from machine to machine, but hopefully on Travis it will be
stable enough.

Ah, as a bonus, it appears the Eclipse help browser will in fact fetch the HTML
pages of the help plugin from the disk without having to restart the runtime
Eclipse or having to use a debugging instance.  But it does /not/ reload the
toc.xml file.  With stable identifiers, I don't have to reload Eclipse when
tweaking the manual unless I add new nodes and want the toc to reflect that.

I managed to put the table of contents to the left of the text when the screen
is wide enough.  That's easy enough with ~float: left~ in a media query.  The
tricky part is that I wanted the toc to be fixed, since you can then more easily
jump to/from contents, and don't have to scroll back to the top to get the
contents.

So using a ~position: fixed~ instead of the float works.  But the downside is
that if the screen is too small, then you can scroll the main contents but not
the toc!  I tried to make a scrollbar appear in this case, just for the toc
block, but that's all kinds of broken, with the scrollbar not scrolling the full
height of the contents...

In the end I used media query for when the screen is too small, we revert to the
floating behavior.  It's a hack, since the media query is dependent on the height
of the toc block, so we should rather use JS for that to work in all
cases...  But we can also set it to a safe value, and if the toc becomes too
tall, I can just use float it all the time.

Last thing is that the inline toc is redundant in the Eclipse help, since there
is already a toc frame in the Eclipse help browser.  Alas, we cannot ~display:
none~ the toc using CSS, because there's no way to detect that we are part of a
frame using CSS only.  So we could use JS, or we could just not generate the TOC
when exporting the Eclipse manual.

* [2018-07-02 lun.]
** Getting back on NeoEMF integration benchmarks            :neoemf:emfviews:
Specifically, we want to know why loading a NeoEMF-backed view OOMs, because it
shouldn't!

Ran some tests with options:

: -Xmx512m -XX:+UseConcMarkSweepGC

- Loading the NeoEMF trace model does not OOM, neither does loading the XMI
  trace model.
- Loading the XMI view OOMs for 10^6.
- Loading the NeoEMF view OOMs for 10^5:
  #+BEGIN_EXAMPLE
  Exception in thread "GC-Monitor" java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3332)
	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124)
	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:700)
	at java.lang.StringBuilder.append(StringBuilder.java:214)
	at org.neo4j.kernel.impl.cache.MeasureDoNothing.run(MeasureDoNothing.java:84)
  #+END_EXAMPLE

Actually, the process keeps running after that.  I doubt it will finish though,
as I get more and more OOM exceptions from other threads:

#+BEGIN_EXAMPLE
java.lang.OutOfMemoryError: Java heap space
Exception in thread "ForkJoinPool.commonPool-worker-0" java.lang.OutOfMemoryError: Java heap space
	at java.util.concurrent.ForkJoinPool$WorkQueue.growArray(ForkJoinPool.java:886)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1687)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Exception in thread "ForkJoinPool.commonPool-worker-0" java.lang.OutOfMemoryError: Java heap space
	at java.util.concurrent.ForkJoinPool$WorkQueue.growArray(ForkJoinPool.java:886)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1687)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Exception in thread "ForkJoinPool.commonPool-worker-0" java.lang.OutOfMemoryError: Java heap space
	at java.util.concurrent.ForkJoinPool$WorkQueue.growArray(ForkJoinPool.java:886)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1687)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Exception in thread "ForkJoinPool.commonPool-worker-3" java.lang.OutOfMemoryError: Java heap space
	at java.util.concurrent.ForkJoinPool$WorkQueue.growArray(ForkJoinPool.java:886)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1687)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157)
Exception in thread "ForkJoinPool.commonPool-worker-1" java.lang.OutOfMemoryError: Java heap space
Exception in thread "ForkJoinPool.commonPool-worker-2" java.lang.OutOfMemoryError: Java heap space
Exception in thread "RMI TCP Connection(idle)" java.lang.OutOfMemoryError: Java heap space
Exception in thread "RMI TCP Connection(idle)" Exception in thread "RMI TCP Connection(idle)" java.lang.OutOfMemoryError: Java heap space
java.lang.OutOfMemoryError: Java heap space
Exception in thread "RMI TCP Connection(idle)" java.lang.OutOfMemoryError: Java heap space
Exception in thread "RMI TCP Connection(idle)" java.lang.OutOfMemoryError: Java heap space

Exception: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread "RMI TCP Connection(idle)"
Exception in thread "RMI TCP Connection(idle)" java.lang.OutOfMemoryError: Java heap space
Exception in thread "RMI TCP Connection(idle)" java.lang.OutOfMemoryError: Java heap space
Exception in thread "main" java.lang.OutOfMemoryError: Java heap space
Exception in thread "RMI TCP Connection(idle)" java.lang.OutOfMemoryError: Java heap space
Exception in thread "GC-Monitor" java.lang.OutOfMemoryError: Java heap space
Exception in thread "GC-Monitor" java.lang.OutOfMemoryError: Java heap space
Exception in thread "RMI TCP Connection(idle)" java.lang.OutOfMemoryError: Java heap space
Exception in thread "RMI TCP Connection(idle)" java.lang.OutOfMemoryError: Java heap space
Exception in thread "RMI TCP Connection(idle)" Exception in thread "RMI TCP Connection(idle)" java.lang.OutOfMemoryError: Java heap space
java.lang.OutOfMemoryError: Java heap space
Exception in thread "RMI TCP Connection(idle)" java.lang.OutOfMemoryError: Java heap space
Exception in thread "RMI TCP Connection(idle)" java.lang.OutOfMemoryError: Java heap space
Exception in thread "RMI TCP Connection(idle)" java.lang.OutOfMemoryError: Java heap space
Exception in thread "RMI TCP Connection(idle)" java.lang.OutOfMemoryError: Java heap space
Exception in thread "RMI TCP Connection(idle)" java.lang.OutOfMemoryError: Java heap space
#+END_EXAMPLE

Running with VisualVM to get a heap dump:

[[file:doc/neoemf-oom-heap-top.png]]

Looks like there is some overlap, since this sums up to well over 512M.  The
LockManager is contained by the WritableTransationState, and the first HashMap
is part of the WritableTransactionState$PrimitiveElement.

So, first question: I'm not sure what a writable transaction is in neo4j, but I
am definitely only reading the DB in this test.

Second thing is that View is hoarding references in the concreteToVirtual map.
It's only 72M, but it surely doesn't help.  But can we do better?  This mapping
is crucial.  I guess the only real alternative is to put all the dynamic
information held by the view in a DB.

Looking at the dump, I see that our caching in LazyEContentsList takes a lot of
memory, and I never actually measured the impact on timing.

Removing all of that, the View now takes only 36Mb, so a 50% reduction.  The
bench still OOMs though, but it takes a bit longer before running out memory.

* [2018-07-03 mar.]
** Trying to fix the OOM                                    :neoemf:emfviews:
According to G., autocommit should help.  I've tried with the 100k (default),
50k and 10k, and it doesn't seem to change: there's still a
WritableTransationState object that retains ~160MB.

A bit stuck there, since I really don't feel like codediving into neo4j.

** Trying to run the EMFTVM Ant task on Travis                   :travis:atl:
Now that ATL 4.0 has released, there is an Ant task for building EMFTVM.  This
should let me get rid of the ad-hoc ATL maven plugin.

Tried to use that, but unfortunately this is a very bad timing, since the mvn
build is failing due to [[https://github.com/eclipse/xtext/issues/1231][Xtext being all kinds of broken]] after the Photon
release.

Stuck there too I guess.

* [2018-07-04 mer.]
** Investigating the OOM further                            :neoemf:emfviews:
Trying with autocommit(1) and weakcache, still OOM:

[[file:doc/neoemf-oom-heap-top2.png]]

Reading the [[https://neo4j.com/docs/operations-manual/current/performance/gc-tuning/][Neo4j manual]], they recommend at least 1GB most of the time, so maybe
512MB is just too not enough?  Still, loading and navigating the NeoEMF trace
model alone works with 512MB.

(Also they recommend using the G1GC, not ConcurrentMarkSweep, so I switched to
that but it made no difference.)

So maybe navigating the whole resource is what is consuming memory.  And indeed,
if I just look at the first element of the resource, I can load the NeoEMF view
in 512MB, whereas the XMI view OOMs.

However, I tried to run the simplest OCL query, and I the NeoEMF view still
OOMs.  So, we can load the view in 512MB, but we cannot do anything useful with
it?  That's not very satisfying.

** Pleasing the Ecore diagnostician for all tests            :emfviews:ecore:
Only one test was still failing: a dangling reference eClassifier in the
eGenericType of a virtual association.

Looking at the Ecore class diagram, indeed EGenericType holds a reference to an
EClassifier, and this should point to virtualized elements instead for
coherency.

So, adding a VirtualEGenericType class, and doing the usual dance of delegating
to the concrete element, eventually virtualizing, and overriding
dynamicGet... now all tests pass!

I really wish I had some way of abstracting this pattern, since it would
tremendously cut down on the noise.

So now I can write more tests for filters, and see if the resulting viewpoints
are still valid.

* [2018-07-06 ven.]
** Deploying EMF Views to an update site                           :emfviews:
To facilitate installation of EMF Views, I need an update site.

Options:

1. Push to gh-pages.  We don't want to do that because that would bloat the
   repository.
2. Push to Github releases.  Can host large files, but cannot host an update
   site (JARs + HTML).  Besides, cannot work for snapshots, as we need a tag for
   a release.
3. Push to a virtual machine on atlanmodexp.  We need SSH (or ftp?) access to
   push from Travis, and the VM needs to be (reliably) available from the
   outside.  Also we need to make we have enough disk space.
4. Push to OVH/S3.  Presumably reliable, but with running costs.

I'll try to see if #3 can fit the bill.

*** Struggling with Proxmox                                         :proxmox:
The graphical interface for managing the VMs and containers is very laggy.  It
seems to drop the connection a lot, wich doesn't help.

Trying to create an OpenVZ container, the template dropdown menu is empty, even
though there are templates on the machine (checked via the graphical interface
and the console).

Same thing for VMs: there are ISOs, but the template dropdown is busted.
Doesn't work under Chromium either.

I guess I can try creation in the console then.

This is [[https://wiki.openvz.org/User_Guide/Operations_on_Containers][the doc]] on creating OpenVZ from the CLI.

: vzctl create 109 --ostemplate debian-8.0-standard_8.0-1_amd64 --config 109

After creating a /etc/vz/conf/ve-109.conf-sample file to serve as template.  The
Debian template image was already present in /var/lib/vz/template/cache.

It works!  Now to boot and log into that container.

: vzctl set 109 --userpasswd ***:***

Okay I'm in!  Now to setup a webserver to serve on 8800, and to bind that port
to the outside.

Or let's test with netcat.

Ahaha.  I wanted to change the routing table to point the open 8800 port to my
new container.  I restarted the network interface with ~ifdown eth0~ and... yep.
Lost all contact with the machine.  Didn't think that through.

Luckily, we have another management interface for the cluster, where I could
reboot the machine.  Didn't have to ask for a physical access to the server
room.

After that,

: ifdown eth0 && ifup eth0

is better because it connects me back.

Hmm I've been trying to forward the port from the container to the host
machine... no dice.  The host can talk to the guest alright, but no access from
the exterior.

The Tomcat server on another container /did/ work on port 8800 though.  How did
they manage that?  Especially as I changed the iptables redirection rules in
/etc/network/interfaces.

Let's reboot the host once more and see if that changes anything.

No, same thing.  The Tomcat server can magically answer to 8800 but my container
can't.  Let's dig into that.  I don't have an account on that container.  Let's
change the root password.

Doesn't look like they are doing anything fancy.

R. came to help.  First thing to check is what rules are actually used by
iptables:

: iptables --list -t nat

(using the nat table, since that's the one set by the commands in
/etc/network/interfaces).  And it shows that 8800 is still redirecting to the
Tomcat server.

Okay nice.  So we can changes the rules manually, and I should be able to
redirect to my container right?

: iptables -t nat -A PREROUTING -p tcp -m tcp --dport 8800 -j DNAT --to-dest ...:8800

But I still have the rule redirecting to the Tomcat server container in
there... we can delete it using ~-D~ instead of ~-A~.  And after
that... success!  I can connect to ~nc~ from the exterior.  I can also run an
HTTP server.

But will the iptables rules stick after a reboot?

No, they are discarded after rebooting.  Grmbl.  What if I comment every iptable
rule in there, do they magically re-appear after reboot?

Yes they do.  Editing this file has basically no effect on the machine's
configuration.

The Proxmox doc mentions that changes to the interfaces file are not made
directly by the GUI either, but are put into an interfaces.new file, which is
then applied to the interfaces file.  Let's try that then.

So this does indeed copy the interfaces.new to the interfaces file after
reboot.  But the iptables content is still the same.

Sent a mail to someone who may have solutions to this problem.

If not, the backup plan is to apply iptables rules somewhere else, in an init.d
script or using iptables-persistent.

* [2018-07-09 lun.]
** Preparing the MODELS camera-ready                        :emfviews:models:
Review points to discuss:

1. Hedging.

   #+BEGIN_QUOTE
   One issue is the extremely cautious and tentative tone of the document.  It
   is hard to find a sentence without subjunctive phrasing (in particular \sect3.2
   and 3.3).

   [..]

   I strongly suggest to rephrase the paper and aim for a more confident way of
   formulating their statements.
   #+END_QUOTE

2. Weaving model is not explained well enough.

   #+BEGIN_QUOTE
   Unfortunately, the paper simply assumes the presence of a "weaving model"
   that suddenly appears from out of nowhere.

   If possible, please elaborate more on how the weaving model is created, what
   the prerequisites in the individual models are and how the "join criteria"
   are identified.
   #+END_QUOTE

3. Bold claim of transforming views.

   #+BEGIN_QUOTE
   In \sect4.1 you claim that the resulting model view can be "transformed". This is
   a bold statement. Can I add a new model element into a view that is generated
   from multiple sources?
   #+END_QUOTE

4. Benchmark makes the solution looks bad.

   #+BEGIN_QUOTE
   the provided benchmark makes your solution look a lot worse than
   it actually is.

   The text does not nearly stress [the fact that RAM will always be 10x times
   faster] enough, leaving the reader with the impression that in-memory
   operation is the way to go, rather than the elaborate solution provided in
   the paper.
   #+END_QUOTE

5. Benchmark "winners" are hard to get at a glance.

   #+BEGIN_QUOTE
   The benchmark section would benefit from line graphs (or bar charts) instead
   of plain tables.
   #+END_QUOTE

6. Relation to Data Warehouses

   #+BEGIN_QUOTE
   The requirements as well as the implementation techniques are very similar to
   what Data Warehouses do in the generic database community.  I would expect at
   least a short comparison what is present in that domain and how it relates to
   the presented approach.
   #+END_QUOTE

7. Send to Artifact Evaluation?

** Found where the iptables were coming from               :emfviews:proxmox:
There was a rogue iptables-restore in /etc/rc.local...  That was not documented
anywhere!  Bad form.  But now at least I can configure that directly, and update
the procedure on the wiki.

So it looks like we have only one public-facing port on that machine, so if I
want to expose a web server and also be able to push files on it, I will need to
handle the pushing through the web server.

* [2018-07-10 mar.]
** Making a Docker image for running benchmarks      :emfviews:neoemf:models:
We'll try to push for the artifact evaluation.  Since to run the benchmarks you
need a specific version of EMF Views, a specific version of NeoEMF, a specific
version of CDO... I think making a Docker image is much simpler.

I managed to make something a bit dirty, by fetching all JAR dependencies (from
the list nicely given by Eclipse when it runs the process) and putting them in
the Docker image.  And it works!  It's only around 77Mb of dependencies, and the
image is around 200Mb with JDK8 and some of the models.

Now I can run the different programs from inside Docker, without having to
install any of the Eclipse stuff.  Neato.

Currently running the Creator task with a mounted volume, so I don't have to run
it everytime I want to re-run the benchmarks.

I was surprised that without mounting volumes, the container was still able to
write the models... /somewhere/.  It makes sense, that the process should be
able to write to disk (for logging, etc.).  I have no idea where it is writing
that, but I guess all is going away when we remove the container.

Did some tests with J.  From what we gathered, a container will have all its
filesystem mounted somewhere under /var/lib/docker/overlay2, to contain all the
differences between its image and whatever happened at runtime.

You can write to this location in the host, and the changes will be immediately
visible in the container, and vice-versa.

Additionally, if the container is stopped but not removed, you can start it
again with ~docker start~.  Its filesystem state is still saved.  From what we
can tell, when you remove the container with ~docker rm~ there's no way to start
it again (through the Docker interface).  It's not clear whether the overlay
filesystem lingers until you call ~docker prune~ or something.

For my use case, it means I don't actually need to mount the local disk as a
volume in Docker.  I could more simply let the container write its generated
model in its filesystem, and one could still run the benchmarks without having
to regenerate the models using ~docker exec~.

But!  If I do that, I'm not sure /where/ exactly that overlay resides.  Could be
on disk, but from the docs it looks like it will use tmpfs, which might be
mounted in RAM instead.  So you would definitely /not/ be measuring the same
thing.

Best to stick with mounting volumes then.

[2018-07-17 mar.]
** Cleaning up atlanmodexp with T.                              :atlanmodexp:
Started doing backups of the containers, and sent emails to previous members who
may have some data on this machine.

We'll probably try to do a full upgrade of the server, which requires physical
access.  We found where the server was hiding, but we'll need permission from
the IT dep.

** Fixing some oversight with the artifact evaluation                :models:
The Git LFS thing failed to push the two Java models on the server.  Probably a
misconfiguration of the hooks.  Anyway, the fix is to call ~git lfs push
--object-id~, as pointed out [[https://stackoverflow.com/a/35266717][here]].

Another reviewer got an OOM on our benchmarks.  Should have made the JVM args
customizable from the start.  Fixed that, and upped the default limit to 8G.
Should be enough?

Also, it looks like running the benchmarks in Docker adds some overhead.

** Looking up references for view maintenance                      :emfviews:
Presented the [[https://cozy.uwplse.org/][Cozy]] paper this morning, and at the end of the related work, they
talk about view maintenance.

That's the term used in the DB community for updating views after their
contributing tables are updated.

Found one survey on view maintenance, but it's from 1998.  But it's the term to
use in that community.

I probably want to spell out our requirements in EMF Views and start
experimenting to see where the naive approach breaks.

* [2018-07-18 mer.]
** View maintenance in EMF Views                                   :emfviews:
The way we rely on built weaving models to create views is incompatible with
view maintenance.

If we assume contributing models can be updated, then we the naive solution is
to rebuild the full weaving model for each update.  That is certainly intractable
for larger models.

A more realistic solution would be to infer what parts of the weaving model need
to be changed, and what part of the view needs to be altered.  I guess the
approach comes at a larger complexity cost.

The obviously correct solution is just to not build the weaving models in
extension, but rather in intension.  Instead of spelling out all the elements
that are part of a virtual association, we keep the weaving model contains only
the intensional definition, and whenever the association is accessed, we rebuild
its contents.

That gives us a baseline.  From there, we can think about caching parts of the
view, and do some more clever analysis of the intensional definitions in order
to boost performance.

** Filters and inheritance                                         :emfviews:
So I finally finished writing that test.  It's the case where we have:

: A <- B <- C

(where ~<-~ is the "is supertype of" relation").  We filter B, leaving us with
only A and C.  By transitivity of inheritance, we had A <- C, but in the
viewpoint that's not the case.

Note that the Ecore diagnostician has nothing to complain about, because as far
as it is concerned, C has no relationship with A.

The issue is that in the implementation, when looking up the super types, if any
of them is filtered, we stop looking up.  And that's because we rely on
getESuperTypes for getEAllSuperTypes.

That's now fixed.

* [2018-07-24 mar.]
** Making OCL interactive console work with EMF Views          :emfviews:ocl:
On a concrete model, if I click on an object in the MoDisco browser, the OCL
console says:

: Xtext OCL for 'Publication ATL in Depth' : Publication

If I do the same in a view:

: Xtext OCL for 'Publication ATL in Depth' : OclInvalid

Looks like OCL has troubles understanding the viewpoint.

Grepping into the source trying to find where the incompatibility might come
from.

For some reason, Eclipse does not allow me to open classes of the OCL Pivot
implementation, even though I have the source plugin installed.

Have to clone OCL, load the plugins in Eclipse to override the installed one,
then I can actually debug.

Come to think of it, this issue stems from the fact that OCL will look at the
currently selected object, and try to adapt it into an EObject.  Probably that's
not implemented correctly for virtual objects.

We have to register ourself into EMF adapters or something.

Okay found something in:

: org.eclipse.ocl.examples.xtext.console/src/org/eclipse/ocl/examples/xtext/console/OCLConsole.java

The setSelection methods are where the "Xtext OCL for..." text is set.

Looks like it's not necessary to be an IAdaptable of EObject, since the code
looks for instances of EObject, and Virtual objects are fine in this respect.

The code fails to find a metaclass.

When looking at a concrete model, I get:

- staticType = ClassImpl (Publication::Publication)
- contextType = ClassImpl (Publication::Publication)

For a model:

- staticType = InvalidTypeImpl (OclInvalid)
- contextType = InvalidTypeImpl (OclInvalid)

staticType will end up in ECore2AS.getCreated, where this code:

#+BEGIN_SRC java
Element element = newCreateMap.get(eObject);
if (element == null) {
	return null;
}
#+END_SRC

returns null for views, but a ClassImpl for concrete models.

So now to find when this newCreateMap is populated.

Looks like this happens in Ecore2AS.update, where this code behaves differently
between the view and the concrete model:

#+BEGIN_SRC java
for (EObject eObject : ecoreContents) {
  	EClass eClass = eObject.eClass();
  	if (eClass.getEPackage() != EcorePackage.eINSTANCE) {
  		error("Non Ecore " + eClass.getName() + " for Ecore2AS.update");
  	}
  	else {
  		Object pivotElement = declarationPass.doInPackageSwitch(eObject);
  		if (pivotElement instanceof org.eclipse.ocl.pivot.Package) {
  			newPackages.add((org.eclipse.ocl.pivot.Package) pivotElement);
  		}
  		else {
  			error("Bad ecore content");
  		}
  	}
  }
#+END_SRC

ecoreContents is the content of the metamodel.  And the code explicitly checks
that the eClass of the metamodel objects is part of the EcorePackage.

In views, it's not the case: Ecore is virtualized, and the equality checks
fails.

I think I encountered the issue precedently with the meta-meta-level loop not
working.  I did encounter the circularity [[*Trying to simplify the interface of views][previously]], but this case is slightly
different.

So it looks like the correct thing is to always have viewpoints point to the raw
EcorePackage as metamodel.  And hopefully it's enough for the OCL console to
work.

* [2018-07-25 mer.]
** More OCL debugging                                          :emfviews:ocl:
Okay, found the issue.  It was actually caused by eResource() returning null for
a VirtualEClass.  This broke since the split of Viewpoint into
ViewpointResource.

So now we've got Viewpoint and ViewpointResource, and both are coupled because
Viewpoint needs a ViewpointResource, and ViewpointResource needs a Viewpoint...
Same thing for View and ViewResource.

It's probably best to merge the two again.

But!  OCL is still not working.  Now I do not get the OclInvalid type anymore in
the console, and auto-completion works, but trying to get an attribute fails:

#+BEGIN_EXAMPLE
Failed to evaluate 'viewpoint::Publication::Publication::title' for 'org.atlanmod.emfviews.elements.VirtualEObject@6a5fc8b (eClass: VirtualEClass of org.eclipse.emf.ecore.impl.EClassImpl@7342a935 (name: Publication) (instanceClassName: null) (abstract: false, interface: false))' and 'self.title'
java.lang.ClassCastException: java.lang.String cannot be cast to java.util.Collection
	at org.eclipse.ocl.pivot.internal.library.executor.AbstractIdResolver.boxedValueOf(AbstractIdResolver.java:451)
	at org.eclipse.ocl.pivot.internal.library.ExplicitNavigationProperty.evaluate(ExplicitNavigationProperty.java:60)
	at org.eclipse.ocl.pivot.internal.evaluation.AbstractExecutor.internalExecuteNavigationCallExp(AbstractExecutor.java:358)
	at org.eclipse.ocl.pivot.internal.evaluation.BasicEvaluationVisitor.visitPropertyCallExp(BasicEvaluationVisitor.java:666)
	at org.eclipse.ocl.pivot.internal.PropertyCallExpImpl.accept(PropertyCallExpImpl.java:417)
	at org.eclipse.ocl.pivot.internal.evaluation.BasicEvaluationVisitor.safeVisit(BasicEvaluationVisitor.java:148)
	at org.eclipse.ocl.pivot.internal.evaluation.BasicEvaluationVisitor.visitExpressionInOCL(BasicEvaluationVisitor.java:302)
	at org.eclipse.ocl.examples.xtext.console.OCLConsolePage$EvaluationRunnable.run(OCLConsolePage.java:226)
	at org.eclipse.jface.operation.ModalContext$ModalContextThread.run(ModalContext.java:119)
#+END_EXAMPLE

Found the cause.  OCL had attributed an OclInvalid type again as return type for
self.title.  That was because ~title~ has type String, which is a data type.
OCL then has a map from Ecore primitive data types to OCL types, which is
initialized as such:

#+BEGIN_SRC java
public void initializeEcore2ASMap() {
	ecore2asMap.put(EcorePackage.Literals.EBOOLEAN, standardLibrary.getBooleanType());
	ecore2asMap.put(EcorePackage.Literals.EBIG_INTEGER, standardLibrary.getIntegerType());
	ecore2asMap.put(EcorePackage.Literals.EBIG_DECIMAL, standardLibrary.getRealType());
	ecore2asMap.put(EcorePackage.Literals.ESTRING, standardLibrary.getStringType());
}
#+END_SRC

It will only match raw EcorePackage literals, not virtualized ones.  But in
VirtualEGenericType, we virtualized the classifiers, thus no corresponding data
type could be found.

** Running OCL programmatically                                :emfviews:ocl:
While writing that section of the manual, I hit a pain point of EMF Views: the
VirtualLinksDelegator can only work using Eclipse extension points.  But what if
we are not running inside Eclipse, but as a plain Java program?

It crashes with NullPointerException.

It looks like it's not trivial to reuse the extension point registry outside of
Eclipse... So instead, I added a way to register delegates directly in map, so
clients can just use set that and not run inside Eclipse.

* [2018-08-01 mer.]
** Updated to latest Eclipse                                        :eclipse:
Process was relatively smooth.

I didn't even copy anything from the old Eclipse, since all settings are kept in
the workspace.

I got a cruft-free Eclipse from this (slightly hidden) [[http://download.eclipse.org/eclipse/downloads/drops4/R-4.8-201806110500/][page]].

Then installed what I needed:

- JDT
- EMF
- ATL
- Epsilon
- MoDisco
- Classic OCL & Unified OCL

For OCL, the console is hidden in the full package.  I had to untick "Group by
category" in the install wizard.

Also, I updated Java to 10, and it's a smooth experience for now.

* [2018-08-08 mer.]
** Reading H's thesis chapter on EMF Views                         :emfviews:
p. 75
No legend for '# * @ ##' footnote symbols

Doesn't 'Multiple Models' arity include 'Single Model'?

Runtime Consistency/View/ is it an exclusive 'OR'?  Table shows ticks for both
branches of OR.  If it's not exclusive, what's the difference with not putting
anything?

p. 77
3rd par. s/puting/putting/

p. 96
For MEL, could state explicitly that we can specialize/generalize *new concepts*
(due tox this being allowed by the Weaving model)

First par. of listing 4 (MEL example) is not coherent with listing.

p. 103
((.ecl)) -> (.ecl)

4th par.  We do not generate eview files from VPDL.  We probably should, but
we'd need to specify the contributing models

4th par.  s/the view behave/the view behaves/

Overall
Maybe make it clear that much of the chapter contents are taken verbatim from
papers?

p. 114
last par.  s/sigal/signal/

* [2018-08-09 jeu.]
** Thinking about view maintenance                       :emfviews:formalism:
View maintenance works in EMF Views due to virtual objects being just proxies to
concrete objects.

So updating a model attribute or reference is directly visible in the view
without additional work.

However, synthetic view elements are computed only once, at view creation.
That's because the weaving model is extensional in nature, even though the VPDL
gives you an intensional interface through ECL.

If we were to define a purely intensional weaving model, then the view could be
always in sync.  The first inefficient approach would be to recompute the value
of a feature using the intensional definition.  A more efficient way would be to
cache and invalidate on update to contributing elements.

In order to describe the approach, I would prefer an abstract model of EMF
rather than the full-blown thing.  I wonder how people usually deal with that,
for ATL or other

- [[https://www.isa-afp.org/entries/Featherweight_OCL.html][Featherweight OCL]] is a machine-checked semantics of OCL, expanding upon the
  semi-formal "Annex A" of the OCL standard.  It's in Isabelle.

  Maybe too heavy-handed for what I had in mind.  I think I can get away without
  a full mechanization of OCL.

- [[http://dblp.org/rec/bibtex/journals/sosym/Westfechtel14][Merging of EMF models - Formal foundations]] is quite interesting in its own
  right.  Merging EMF models is complex, and this looks like it tackles most
  corner cases in a formal way.

  \sect4 contains a small mathematical model of Ecore metamodels and Ecore models.
  There is also some formalization of /change/ to models, due to the setting of
  tackling model merging.

  A good candidate to take inspiration from.

- [[http://dblp.org/rec/bibtex/journals/sosym/BiermannET12][Formal foundation of consistent {EMF} model transformations by algebraic graph
  transformation]] formalizes EMF models as graphs, for the purpose of using
  algebraic graph transformation theory to ensure consistent EMF
  transformations.

  Neat on the surface.  The EMF graph formalization looks straightforward.
  Looking for consistent transformations could help making sure MEL extensions
  make sense.

* [2018-08-10 ven.]
** Attempt at a formal model for EMF Views               :emfviews:formalism:
Following Westfechtel.

Ecore model is a tuple (C,D,F,P), resp. sets of classes, data types, features
and properties.  F is a partition of A and R, resp. attributes and references.

Packages and factories are ignored.

A model instantiated from an Ecore model (C,D,F,P) is a tuple (O,class,FV): a
set of objects O, a function ~class: O \to C~  from objects to their class, and a
set of feature value functions.

A viewtype is a tuple (MM, C_V, D_V, F_V, P_V): M is a set of Ecore models, C_V, D_V, and
F_V are additional classes, data types and features.

A viewtype can be interpreted as an Ecore model (C,D,F,P) as such:

- C = C_v \cup \sum_{e \in MM} C_e
- D = D_v \cup \sum_{e \in MM} D_e
- F = F_v \cup \sum_{e \in M}M F_e
- All functions of P canonically apply to their model e \in MM:
  - dom: F \to C = dom_e if f \in F_e
                 dom_v otherwise
  - etc.

(Alternatively, a viewtype is a disjoint union of Ecore models)

With a viewtype V = (MM, C_V, D_V, F_V, P_V), a view is a set of models.

*** A step back
What are we trying to prove here?  The tricky parts of views are: filters (which
might leave the model inconsistent) and modifying existing links or
cardinalities (which might break invariants).  Another motivation was making
sense of view maintenance and view update.  For these, we need to model change
in models as well.  Westfechtel definitions are read-only.

For filters, we have several options:

- A list of filtered elements (as in the weaving model)
- A function filtered : C \cup D \cup F \to which tells us if an Ecore
  element is filtered out
- Or say that a viewtype is also a tuple (C,D,F,P), and the filtered elements
  are just not included in its sets.

An interesting property is whether transitive inheritance is kept, even in the
presence of filters:

: A <- B <- C

superTypes(C) = {B}, but superTypes*(C) = {A,B}.

However, if we filter B in the viewtype, superTypes(C) = {} because B does not
exist anymore, but A is still conceptually a superType of C, even though the
transitive closure must be empty (since C has no direct parent).

So we have a contradiction, or at least a very unintuitive behavior.  But
really, something I've overlooked before is that C should not exist in the
viewtype if we filter B out.  That solves the conundrum.

Because if C continues to exist, it might reference B in its features (might
have a feature of type B for instance).  What should we do with such a feature?
Change its type to C?  Remove it?

Also, if C exists, does it get the features inherited from B?

So, the simplest option is for C to be filtered as well.  That means:

- To a given Ecore we can associate a /weaving model/.  To start with a weaving
  model is just a function ~filtered C \cup D \cup F \to boolean~ which indicates if an
  element from the Ecore model is to be filtered.

- From a weaving model we can create a /viewtype/.  A viewtype is an Ecore
  model (C_V, D_V, F_V, P_V), (with F_V is partitioned in A_V and R_V) where:

  - C_V = { c \in C | (\not filtered(c)) \wedge (\forall s \in superTypes*(c), \not filtered(s)) }
  - D_V = { d \in D | \not filtered(d) }

For data types we don't have inheritance.  However, they are used as types for
attributes.  If we filter the data type out, the attribute is invalid.  We
should filter the attribute as well to restore consistency.  Same argument for
references and classes.  If we filter the containing class, the feature is out.
So:

- A_V = { a \in A | \not filtered(a) \wedge \not filtered(domain(a)) \wedge \not filtered(range(a)) }
- R_V = { r \in R | (\not filtered(r)) \wedge \not filtered(domain(r)) \wedge \not filtered(range(r)) }

The functions of P_V are restricted to the subsets visible in the viewtype:

- \forall c \in C_V superTypes(c) = superTypes_C(c) \cap C_V
- \forall f \in F_v domain(f) = domain_C(f) \cap C_V
- \forall f \in F_v range(f) = range_C(f) \cap (C_V \cup D_V)
- \forall f \in F_V many(f) = many_C(f)  (idem ordered, unique)
- \forall r \in R_V containment(r) = containment_C(r)
- \forall r \in R_V opposite(r) = opposite_C(r) if r \in domain(opposite_C) \wedge opposite_c(r) \in R_V
                         undefined otherwise

That's a start.  That only covers filters, not even aggregation of Ecore models,
and not yet extension.

* [2018-08-13 lun.]
** Improving the rules                                   :emfviews:formalism:
The rules:

- A_V = { a \in A | \not filtered(a) \wedge \not filtered(domain(a)) \wedge \not filtered(range(a)) }
- R_V = { r \in R | (\not filtered(r)) \wedge \not filtered(domain(r)) \wedge \not filtered(range(r))

are wrong.  Take an Ecore mode with a class B with a single attribute b of type
B, and a class A supertype of B.

In the weaving model, filtered(x) = true  if x = A
                                    false otherwise

In the viewtype, C_v = {} because A and all its subtypes are filtered out.  Since
B is filtered out, we expect A_v to be empty, but domain(b) is undefined, so it
is nto filtered out, and thus present.

The set of visible classes C_v is not the same as the set of classes that are not
filtered out, due to filters propagating to subtypes.  So a subtype can be
filtered out implicitly.  Then, we want to include only the features of visible
types, so we have to use the 'visibility' definition rather than the 'non
filtered' one.

So:

- A_V = { a \in A | \not filtered(a) \wedge domain(a) \in C_V \wedge range(a) \in D_V }
- R_V = { r \in R | \not filtered(r) \wedge domain(r) \in C_V \wedge range(r) \in C_V }

Note that domain and range are the ones from the Ecore model (obviously, since
domain_V is defined on F_V, not F).

The opposite definition can be simplified:

- \forall r \in R_V opposite(r) = opposite_C(r) if opposite_C(r) \in R_V
                         undefined    otherwise

*** Whitelisting
So far the filters are taken as blacklists: filter things /out/ of the base
Ecore model.  Let's see if whitelisting changes anything.  At the outset, we
need to reverse the meaning of filtered:

- C_V = { c \in C | filtered(c) ... }

But what happens in A <- B <- C if B is filtered?  Clearly, we do not care about
C.  But what about A?  If A is filtered out, then it doesn't really matter.  If
we do not take their features into account, there is no reason to include A.

So:

- C_V = { c \in C | filtered(c) }

However, if we include a feature, then we have to include its domain and range
to be consistent.  But that means extending the C_V set after the fact.  So the
C_V definition should instead be:

- C_v = { c \in C | filtered(c) \vee (\exists f \in F_V, c = domain(f)) \vee (\exists r \in R_V, c = range(r)) }

If we include an attribute, its data type must be included as well:

- D_V = { d \in D | filtered(d) \vee (\exists a \in A_V, range(a) = c) }

and after that:

- F_V = { f \in F | filtered(f) }

So the interesting observation is that the rules for whitelisting are not
trivially symmetric to blacklisting.

*** Access control without consistency
Why do we care about consistency anyway?  In EMF Views, we can in fact create
viewpoints that are inconsistent Ecore models.  There's a value in that for
access control purposes.  Then you don't care about propagating visibility of
classes or features, it's just:

- C_V = { c \in C | \not filtered(c) }

idem for D_V and F_V.

You might run into problems though (missing datatypes means attributes have no
sensible value).  And it can be tedious in whitelisting mode.  That's why
propagating visibility is useful.

*** Aggregating several Ecore models in viewtypes
If we allow viewtypes to take several Ecore models as inputs, then it's a
trivial extension.  The base Ecore models are disjoint, and there are no
interactions between them.  We just have to extend the definitions to take all
models into account.  So:

- C_V = { c \in C | (\not filtered(c)) \wedge (\forall s \in superTypes*(c), \not filtered(s)) }

becomes:

- C_V = \sum_m { c \in C_m | (\not filtered(c)) \wedge (\forall s \in superTypes_m*(c), \not filtered(s)) }

where m is taken over the set of input Ecore models.

Note that in practice, EMF elements are identified by their name, which must be
unique in a given package.  Here, this constraint may be invalidated in a
viewtype if it holds in base models: m_1 and m_2 both have a class named 'A', the
viewtype now has two classes named A.  To respect the constraint, the sensible
thing to do is to prefix the classes by their package name.

*** Extending Ecore models in viewtypes
Adding new types is easy, with their own features, is like taking one more input
model.  Trivial.

Adding features to base model types is similarly simple.

Problems arise when we want to /change/ existing elements instead of just adding
new ones.  We can fake a change to an existing element by combining a filter and
an add.  Let's say we want to change the type of the feature A.a from B to C.

We could, in the weaving model, write:

: filter A.a
: new A.a : C

and this would be consistent.  As far as the mathematical model is concerned,
the filtered A.a and the new A.a are different objects.  They happen to have the
same name, but names only matter in the EMF implementation.

If the filtered A.a had an opposite, the opposite link is now null.

What if tried to change a class?  Change A to B, where B has all the features of
A minus one?

: filter A
: new B

Ah, but then, we have to basically recreate the features of A in B (and
eventually updating the types from A to B).  So there is potentially a problem
of expressivity of the weaving model (it's more verbose than it should), but not
a problem of consistency.

There might be invalidated constraints though, but these will appear at the EMF
level of implementation.

So it looks like this model is mostly useful for formalizing how filters work.

* [2018-08-14 mar.]
** About filtering and inheritance                       :emfviews:formalism:
First of all, in blacklisting, it's not obvious that we should remove C when B
is filtered.

A <- B <- C

True, superTypes*(C) = {A,B}, but superTypes_V*(C) = {} even though A still
exists in the view.  But that's only the case if we use a naive definition for
superTypes_V*.  If we define it as:

- \forall c \in C_V, superTypes_V*(c) = superTypes_C*(c) \cap C_V

Then superTypes_C*(C) = {A,B}, and superTypes_V*(C) = {A,B} \cap {A,C} = {A}.

That means we can define C_V simply as:

- C_V = { c \in C | \not filtered(c) }

So that leaves the potential problem of inherited features from B in C.  But if
B is filtered out, then its features are also filtered out (by definition of A_V
and R_V).  Meaning that:

A <- B <- C

A.a : int
B.b : int
C.c : int

Then, in the view, C only has attributes {a, c}.

- C_V = {A,C}
- A_V = { a \in A | \not filtered(a) \wedge domain(a) \in C_V \wedge range(a) \in D_V }
- A_V = { a, c }

And any link pointing to B is dropped:

C.r : B

- R_V = { r \in R | \not filtered(r) \wedge domain(r) \in C_V \wedge range(r) \in C_V }
- R_V = {}

*** Revisiting whitelisting
The dual to the A <- B <- C example in whitelisting is that we include both A,
C, leaving B out.  So:

- C_V = { A, B }

And we have the same question: should superTypes_V*(C) = {A} or {}?

We can define superTypes_V* the same as for blacklisting:

- \forall c \in C_V, superTypes_V*(c) = superTypes_C*(c) \cap C_V

and we have the more intuitive answer {A}.

*** Multiple inheritance and filtering
Is multiple inheritance a problem for filtering?

A <- B <- D
A <- C <- D

superTypes*(D) = {A,B,C}
spureTypes*(C) = {A}
spureTypes*(B) = {A}
superTYpes*(A) = {}

filter(B)

C_V = {A,C,D}
superTypes*(D) = {A,B,C} \cap {A,C,D} = {A,C}

Can't think of a problematic case.

*** Another formalization of EMF models
Pointed out by JC: [[https://dblp.uni-trier.de/rec/bibtex/journals/isse/EgeaR10][Formal executable semantics for conformance in the {MDE}
framework]].

The formalization is straightforward, using sets for classes, features and the
supertype relationship.  They do not state conformance properties (one container
per feature, etc.) as Westfechtel does, because these conformance properties can
be expressed as base OCL constraints.  They then provide an executable semantics
for full conformance (including additional, metamodel-specific OCL constraints).

* [2018-08-20 lun.]
** Formalization of views proper                         :emfviews:formalism:
A model instantiated from an Ecore model (C,D,F,P) is a tuple (O,class,FV): a
set of objects O, a function /class/: O \to C from objects to their class, and a
set of feature value functions.

At first, let us consider views with trivial weaving models (i.e., no filtering
or new attributes).

A /view/ (O_V, class_V, FV_V) is created from a viewtype (C_V, D_V, F_V, P_V) and a
base model (O, class, FV).

For conformance, the view can only contains objects whose class exists in the
viewtype.

- O_V = { o \in O | class(o) \in C_V }

The class_V function is simply restricted on O_V \to C_V.

- class_V : O_V \to C_V, \forall{}o \in O_V class_V(o) = class(o)

The set FV_V is restricted to features that exist on the viewtype.

- FV_V = { fv \in FV | feature(fv) \in F_V }

where /feature/ : FV \to F is the bijective function that associates a feature
value function to its owning feature.

* [2018-08-21 mar.]
** Formalization of views, contd.                        :emfviews:formalism:
If we look at Westfechtel properties for models, it's easy to see they hold for
views as well.

Referenced objects must be part of the model:

- \forall{}o_1 \in O, o_2 \in fv(o_1): o_2 \in O

The equivalent property for the view is:

- \forall{}o_1 \in O_V, o_2 \in fv_V(o_1): o_2 \in O_V

But this means we have to change the feature value functions.  Since, for
instance if we have a class with a reference feature a, and a class B, and a
model with two objects o_1 and o_2, such that:

- class(o_1) = A
- class(o_2) = B
- fv^a(o_1) = o_2, where fv^a \in FV and feature(fv^a) = a

then o_2 \in O by referential integrity.

However, if the viewtype is defined by excluding B, in the view we have O_v = {
o_1 } (o_2 is excluded since its class is not part of the viewtype), and then:

- fv_v^a(o_1) = o_2 \notin O_V

so referential integrity is violated.

We have to redefine the feature value functions to check for that:

- For each feature f \in F_V, there is a feature value function fv_v, defined as:
  fv_v(o) = fv(o) \cap O_V if many(f)
           undefined  otherwise

(Instead of undefined, we could have a sentinel value for references pointing
nowhere.)

Opposites should be consistent:

- For two features value functions fv_1 and fv_2 such that feature(fv_1) =
  opposite(feature(fv_2)),

  \forall{}o_1, o_2 \in O : o_2 \in fv_1(o_1) \leftrightarrow o_1 \in fv_2(o_2)

That one should not be impacted, as long as the opposite_v function is sound in
the viewtype.

* [2018-08-22 mer.]
** Formalization of views, third                         :emfviews:formalism:
Each object has at most one container.  fv^1 and fv^2 are feature value functions
for containment references:

- \forall_{}o, o_1, o_2 \in O, o \in fv^1(o_1) \wedge o \in fv^2(o_2) \rArr o_1 = o_2 \wedge
  feature(fv^1) = feature(fv^2)

That one should hold on the views as long as it holds on the base model.

Similarly, since we are only taking things out of the base model, there is no
way to add a containment cycle.

*** Choices in formalization
To restrict the functions from Ecore models to viewtypes, we have at least two
obvious solutions.

1. domain_V : F_V \to C_V, \forall{}f \in F_V domain_V(f) = domain(f)
2. domain_V = { (f, c) \in F_V \times C_V | domain(f) = c }

In the first, domain_V is total, but in the second we do not know.  However, in
the first form we have to show that \forall{}f \in F_V, domain(f) \in C_V.  In the second form
this is given, and we have to show instead that domain_V is total: \forall{}f \in F_V, \exists{}c \in
C_V s.t. domain_V(f) = c.

The argument will be about the same for both proofs (by construction of F_V and
C_V), so it doesn't really matter.

However, for a partial function like opposite_V, then the second form is better.

There is a minor argument whether c \in superTypes(c).  To me, it's most
convenient to answer yes.  Inheritance is reflexive and transitive.  And it
makes definitions simpler.

There is an oversight in Westfechtel: it's unclear what single-valued references
which are not set have as value.  For multi-valued features, it can be the empty
set, or the empty list.  But for single-valued features, the feature value
function /is/ defined, but we still need something to stand as value.  That
means the range of the feature value function includes that special value.  \perp is
fine.

* [2018-08-24 ven.]
** Formalizing change to models and weaving models       :emfviews:formalism:
Westfechtel's definitions are immutable.  If we want to give an account of view
maintenance, we have to add functions that change values of features.

Also, we could probably tweak the definition of weaving models to be operations
that the viewtype accepts dynamically, rather than a list of rules from which
the viewtype reads only once at creation time.  That would let us alter a
viewtype and a view by applying a filter dynamically.

* [2018-08-27 lun.]
** Metamodel extension                                   :emfviews:formalism:
Currently we allow adding new concepts, attributes and references to the
viewtype.

This may break some properties of the viewtype.

The viewtype weaving model has a set of new concepts C_WM, a set of new attributes
A_WM, and new references R_WM.

The viewtype is then constructed as:

- C_V = { c \in C | ... } \cup C_WM
- A_V = { a \in A | ... } \cup A_WM
- R_V = { r \in R | ... } \cup R_WM

We also need to supply supertype information for new concepts.  But we can have
base classes or new classes as supertypes, so supertypes_WM : C_WM \to 2^{(C \cup NC)}.

From there, we can build supertypes_V:

- supertypes_V(c) = supertypes(c) \cap C_V   if c \in C
                   supertypes_WM(c) \cap C_V otherwise (c \in C_WM)

There's an interesting situation where we add a new class B that has A as
supertypes, and A is also excluded by the weaving model.  It's a bit
nonsensical, but what should we do with it?  Intersection with C_V makes sure
these do not appear.

Now I'm wondering, do I even need to spell out that intersection with C_V in the
definition?  If the codomain of supertypes is 2^{C_V}, it seems pretty clear that
we can have only elements of C_V.  On the other hand, it's less explicit.  Hmm,
maybe it's more natural to leave it out, but I feel it's better to have it
explicit.

At this point it becomes cumbersome to carry three different definitions of
supertypes.  But it also maps best to what we have in the implementation.

Similarly, all the functions of the set P have a correspondance in P_WM: domain,
range, many, opposite...

Opposite has the interesting case where we add a new reference as an opposite to
an existing association.

After that, we have the weaving model for views.  If we add a new concept to the
viewtype, then we need a way to add new instances to the view through the view
weaving model.  This can be done by extension, as in a set O_WM of new objects.
Or by intension, as a function of O to the set of all objects that can be
instantiated from the classes in C_V.

* [2018-08-28 mar.]
** Other formalisms                                      :emfviews:formalism:
Z. pointed me to a couple of relevant Ecore formalisms:

- View-based model-driven software development with ModelJoin, Burger et al.
- Reconstructing Complex Metamodel Evolution, Vermolen et. al

First one is particularly relevant because it's about another view mechanism.
They do not seem to prove any meaningful property about their views however;
their definitions only serve to give a semantics to ModelJoin.

** Meeting with H                                        :emfviews:formalism:
We figured a few things out.

1. In a <- b <- c with excluded(b) = true, we can state that supertypes_V(c) = {
   a } instead of redefining supertypes_V^{+}.  It's equivalent, but it makes things
   less awkward (the transitive closures are canonically defined).

2. We should make an option in the weaving model for cascading subtypes
   filtering, which is the first behavior I retained for solving the subtypes
   problem.  Both options seem valid in hindsight, maybe cascading is more
   intuitive.

3. Referential integrity is free, because R_V does not contain any reference
   whose range is filtered out.  However, need to check if it's still true in
   more complex cases where we filter a parent class, and have a reference which
   targets the child type.

4. Proposition 7 is defined at the model level, but it seems it could be more
   directly defined at the metamodel level?

5. For view maintenance and updates, we can just say that we have OCL requests
   that compute a set of view objects, or attributes, and whenever we update a
   contributing model, then we run these requests again to obtain a modified
   view.  No need to actually know what is inside these requests.

   No need either to state that we have a dependency graph between these
   requests and model features, which helps us determine the minimal set of
   requests we need to re-execute.  That only helps performance, and has no
   impact on the semantics.

* [2018-08-29 mer.]
** Containment cycles                                    :emfviews:formalism:
If we define contains on the metamodel as:

- contains = { (f_1, f_2) \in F^2 | containment(f_1) \wedge range(f_1) = domain(f_2) }

We can state that the metamodel has no containment cycles if:

- \forall{}f \in F : (f,f) \notin contains^{+}

This one is a stronger property.  No containment cycles at the model level
implies no contaiment cycles at the metamodel level.  However, in the case of an
empty model, we could have a containment cycle at the metamodel level.

So forbidding cycles at the metamodel level seems better.

** Revamping inheritance                                 :emfviews:formalism:
The supertypes_V definition is slightly more complex.  But it seems to make more
sense.  Seems to work with multiple inheritance.  Only weird case I can think of
is the diamond:

A <- B <- D
A <- C <- D

When you filter B, you end up with:

A <- C <- D
A <- D

and clearly A <- D is redundant.  But is that an issue?  Not really.  If it
becomes one, we can add that case to the definition of supertypes_V.

Another thing is that supertypes shouldn't be reflexive, as that makes the
supertypes_V definition more cumbersome.

One thing lacking from Westfechtel is a clear connection between a class and its
features.  We can define a relation:

- features(c \in C) = { f \in F | domain(f) = c }

but usually we will want all the features of a class, including inherited ones:

- allfeatures(c \in C) = { f \in F | domain(f) = c
                                \vee (\exist{}s \in supertypes^{+}(c) : domain(f) = s) }


For a feature f \in F, we can access its value with get_f : O \to val.  The domain can
be restricted to the set of objects that respond to the feature f, that is, the
feature must belong to the features of the instantiating class or one of its
supertype:

- { o \in O | f \in allFeatures(class(o)) }

The codomain depends on the feature.  A many-valued feature will range over sets
of values of range(f).  A single-valued feature will be either v \in range(f) or
\perp, indicating the absence of value.

We can change the value of a feature for an object o using set_f : O \times val \to O.

- get_f(set_f(o, v)) = v

To populate new associations, the weaving model contains requests.  Each new
association has a request r \in Req.  The value of the new association in the view
come from r.  Whenever the base model is updated, we can reconstruct the view
using these requests:

- O_V = { o \in O | class(o) \in C_V } \cup { r(O) | \forall{}r \in Req }

* [2018-09-05 mer.]
** Reading the ModelJoin paper                 :emfviews:formalism:modeljoin:
They have a straightforward (although symbol-heavy) set theory-based formalism.
They don't try to prove any property though: they just describe how the
modeljoin operators work, by giving the set the operators construct, and the
properties the operators hold.

* [2018-09-12 mer.]
** Preparing a cool demo with EGL templates           :emfviews:egl:megamart:
We wanted to show off a nice interactive HTML page constructed from an EGL
template extracted from a traceability view.

But of course, EGL template don't work anymore with the current version.

Specifically, the template runs, but it finds no instances of Log.  I had the
issue [[*Fixing EGL templates with EMF Views][previously]], and I guess it's the same issue still: EGL loads a view and a
viewpoint (V_1) separately, directly from file, and loading the view creates a
separate instance of viewpoint (V_2).  Since both instances are different, their
virtual elements are differ as well, and thus in the view, there are no
instances of V_{2}.Log, only instances of V_{1}.Log.

My solution at the time was to use a viewpoint registry to keep track of loaded
viewpoints.  But this turned out to be [[*Viewpoint registry may not be a good solution][a bad solution]], so I reverted the
registry.  And probably this broke EGL, but we didn't have any tests for that.

(Also, templates presumably can work if created programmatically.  The headache
is mostly a result of the interface assuming a standard Ecore metamodel as file
or URI.)

* [2018-09-13 jeu.]
** Repairing EGL templates                                     :emfviews:egl:
Grmbl.  Spent a lot of time trying to make that work.  Had to resort to using
the ViewpointRegistry trick again, since the Java API for calling EGL still
requires us to give it a metamodel and a model separately.

Maybe there's a better way.

Managed to make it work by going to the design-runtime-demo branch.  And also I
needed to tick the "Read on load" box in the EGL launcher.  Otherwise, the model
was never loaded.

Maybe that suffices to make it work in master as well?

Yes!  Pfft.  Stupid error.  Anyway, now onto the fun stuff.

Got something nice working, so we can quickly see which requirements are tied to
a trace and vice-versa.  Just needs some polish.

* [2018-10-02 mar.]
** Hackathon                                                       :megamart:
*** WP4 Challengy CSY_1 Log interpretation
They want to join runtime logs to specification.  They have:

- A B-language model
- A B ecore metamodel
- Some generated C code

We picked that one.

*** Post-mortem
We wanted to do something very similar to the petstore demo that links runtime
logs to design requirements.

They have B code for specifying a core safe subset of their software.  The B
generates C code, which has also unsafe parts.  And they have large runtime
traces (~500MB CSV files) that they want to connect to the B code (because they
are mostly interested in the safe core).

Traces look like this:

: <timestamp> <input/Output variable name> <variable value>

One thing they want is to find out what happened in the code when the variable
"fallback" occurs with a non-zero value.  Ideally, they want to find out what
branches were taken in the code when that value appears.

What we wanted to do for the day was to first load their B code and the traces
into EMF-usable models.

For the trace, we just had to define a new Ecore metamodel, then transform the
CSV into the XMI file.  That was done by R and J.

My part was to load the B model.  They already provided XML files of the B AST
they were able to generate.  And an XSD file for the XML.  It was supposed to be
easy, but it wasn't.

We generated an Ecore from the XSD, and tried to load an XMI file (generated
from the XML -- only the header changes) with that Ecore.  It didn't work.
There is mismatch between the XMI tags and the classifiers in the Ecore
metamodel.

In the XML/XMI files, we have tag names like Machine, Abstraction,
Operations... but in the Ecore file we have classifiers called MachineType,
AbstractionType, OperationsType...

Thus, when trying to load the XMI file, Ecore complains that it cannot
instantiate the "Machine" class.

We tried /several/ things suggested online to no avail.  We tried generating the
code, regenerating the Ecore from XSD, looked for loading options, ...

Dropping the "Type" suffix from classifiers looked like the only sound
workaround.  I did that by generating the Ecore package from XSD
programmatically and iterating over the classifiers.  But there was a
complication.  In the XSD there are presumably homonyms, which get disambiguated
in the Ecore package by adding increasing numbers: ValueType, ValueType1,
ValueType2...

And, somehow, we have enumerations and classes with the same name in the Ecore
package.  So when loading the XMI with the doctored Ecore package, I'm now
getting the error:

: org.eclipse.emf.ecore.impl.EEnumImpl cannot be cast to org.eclipse.emf.ecore.EClass

*** Future work with CSY
All in all, the XML from B was already a bit underwhelming, because it wasn't a
real graph of the B project like what we can get from MoDisco.

It was simply an AST of B files as XML (without comments).  A call site in
module A wasn't linked in any way to the definition site in module B.  So even
if we had been able to load the XML files into EMF, we would have had to
reconstruct this information by through additional analysis on the single ASTs.

The proper way to go forward is to create an actual graph of the B code
(preserving comments while we are at it) as an XMI directly.  Maybe using
MoDisco as a base technology, where we plug-in a custom discoverer for B code
that emits a new metamodel.

Then we have a good basis to answer the problem.

To gather useful information though, we probably will have to enhance the B code
to emit additional debugging information, e.g., record which branches were taken
at runtime, and put that into the traces.

From my understanding though, they really wanted to be able to do post-mortem
analysis of existing traces.  For these traces, we wouldn't have the additional
information available, so that's a harder problem.  My intuition is that we
should be able to get /some/ information by doing static analysis of the B code
that generated the traces, since we know input variables, we could determine for
instance that a branch had to have been taken, or have not been taken.  If we
have all the inputs, it's in theory feasible (at worst, we just run the code
again).  But we still have to look at the code and inputs in detail to know if
that would be doable.

*** Potential collaboration with UAQ
A potential collaboration with L'aquila is to use JTL traces as weaving models
for EMF Views.  JTL lets you create transformations that links a source model to
a target model.  They have an ATL-like language to create traceability links
between source and target elements.  In the end, they have a traceability model,
where all these links exist.  Basically, this is very similar to a view weaving
model for us.  We could maybe use these traceability links as weaving model
instead of ECL.

* [2018-10-03 mer.]
** XSD to Ecore issues                                         :megamart:xsd:
The EEnum to EClass issue was caused by having a 'Machine' enum in the Ecore but
only a 'MachineType1' class.  Renaming the MachineType1 to Machine and the enum
to MachineEnum fixed it.

Now, the Machine class cannot find the 'Abstraction', 'Imports' and 'Operations'
/features/.

When looking at a correct XMI, we have:

#+BEGIN_EXAMPLE
<Trace>
  <logs message="">
#+END_EXAMPLE

So the 'logs' feature is used directly as a tag in XMI.  Looking at the XMI
generated from XML:

#+BEGIN_EXAMPLE
<Machine>
  <Abstraction>Treatment</Abstraction>
#+END_EXAMPLE

But the 'Machine' class in the Ecore has only an 'abstractions' feature, so it
should rather be:

#+BEGIN_EXAMPLE
<Machine>
  <abstraction value="Treatment"/>
#+END_EXAMPLE

So the XMI itself is malformed, with respect to what EMF accepts.

Looking at [[ All these names can be control with ecore:name annotations in the
schema (or you could change then in your Ecore manually).][this Eclipse thread]], the XSD to Ecore conversion is the way it is because XSD
has different constraints than Ecore.

Crucially, we can doctor the XSD by adding 'ecore:name' annotations to instruct
the conversion process which name to take for a specific element.

But this doesn't solve the issue that EMF is unable to load the XMI correctly,
because the XML -> XMI conversion is busted.

So the issue now is more: how to load an XML with a specific Ecore?

Aaah, but apparently, when you generate the code, there /is/ a resource factory
implementation for your specific package!  So I may be able to use that when
loading the XML directly?

[Waiting for after my demo, since yesterday evening when preparing the demo
MoDisco was refusing to load with the Bxml plugin activated, for some mysterious
reasons]

Success!

** Properly loading an XML from XSD as an EMF model               :xsd:ecore:
The recipe is then:

1. Create a genmodel from the XSD

   New project \to EMF Generator Model

   Then you have to select 'XML Schema' as 'Model Importer'.  I didn't have XML
   schema as an option until I installed most XSD-related plugins (especially,
   XSD to Ecore mapping).

2. Generate the model code

   Open the genmodel \to right-click on the root element \to Generate model code

3. Add the 'suppress document root' option in the generated ResourceFactoryImpl

   The XML file should load without it, but I'm interested in getting the same
   kind of runtime object I would get out of loading an XMI.  This option does
   it.  You could add it as an option when loading the resource, but for me it
   makes more sense to have it as default.

   In the generated ResourceFactoryImpl in the model.util package:

   #+BEGIN_SRC java
   result.getDefaultLoadOptions().put(XMLResource.OPTION_SUPPRESS_DOCUMENT_ROOT, Boolean.TRUE);
   #+END_SRC

4. Load the XML file with the generated ResourceFactoryImpl

   Arguably the only trick in the approach is to use the generated
   ResourceFactoryImpl in order to load the XML correctly.  The code is fairly
   straightforward:

   #+BEGIN_SRC java
   public static void main(String[] args) {
     BxmlPackageImpl.eINSTANCE.eClass();

     Resource.Factory.Registry.INSTANCE
       .getExtensionToFactoryMap()
       .put("bxml", new BxmlResourceFactoryImpl());

     Resource r = new ResourceSetImpl().getResource("path-to.bxml", true);
   }
   #+END_SRC

   And now the resource contains an instance of the top-level element.

** Some files do not conform to the XSD                        :xsd:megamart:
In truth, I also had to tweak the Bxml file, which presumably was not conforming
to the XSD?

The typref feature of ~Boolean_Literal~ was not known:

: <Boolean_Literal value='TRUE' typref='2043437594'/>

and the TypeInfos tag shouldn't contain anything:

: <TypeInfos>
:   <Type id='2043437594'>
:     <Id value='BOOL'/>
:   </Type>
: </TypeInfos>

xmllint agrees:

#+BEGIN_EXAMPLE
$ xmllint Treatment_i.bxml --schema bxml.xsd
...
Treatment_i.bxml:67: element Boolean_Literal: Schemas validity error : Element 'Boolean_Literal', attribute 'typref': The attribute 'typref' is not allowed.
Treatment_i.bxml:128: element Type: Schemas validity error : Element 'Type': This element is not expected. Expected is ( TypeInfo ).
Treatment_i.bxml fails to validate
#+END_EXAMPLE

* [2018-10-04 jeu.]
** Looking to properly discover B                                     :csy:b:
[[https://sourceforge.net/p/abtools/code/HEAD/tree/src/ABTOOLS/GRAMMAR/B.g][Found]] some ANTLR grammar file for B.  Not so small.

There's also [[https://sourceforge.net/p/bcomp/code/HEAD/tree/trunk/B_COMPILER/][this source code]] for the B compiler from CSY.  Not updated since
2015, and there's no Yacc file to be found.

Grabbed the source code and compiled it, because I noticed an interesting
option:

: print("\t-[s|l] <file>  : [save|load] Betree [into|from] file\n") ;

The Betree is the tree constructed after parsing and even the semantic and
dependency analyses.  I wanted to know what this dump looked like, because we
could maybe use it to export to XML/XMI.

But joke's on me, because that option actually does nothing in the code.  It's
not implemented!  Probably that option was dropped at some point, but among the
revisions available at the sourceforge repo, the code for this option is nowhere
to be found.

Ahah!  There is actually decent documentation for the Betree object and its API
under in ~doc/Specification_BeTree.pdf~.  In we can find that saving a Betree to
file is done by the ~compiler_save_betree~ function, which is actually commented
out in ~c_api.cpp~!

[[https://sourceforge.net/p/bcomp/code/111/][r111]] is the revision which commented out this code.  It introduced compatibility
for 64bit AtelierB.  Maybe the save/load of betree was not compatible with 64bit
and that part was dropped?

I will try to uncomment the code.  If it crashes, I can use r110 instead.

* [2018-10-09 mar.]
** Trying to fix exceptions in Sample Ecore Editor                 :emfviews:
Changin VirtualEClassifier.setExtendedMetaData to a no-op triggers another
exception: ClassCast from EObjectContainmentEList$Resolving to EObject.

: EObject otherEObject = (EObject)((EObject)value).eGet(eOtherEnd);

Somehow, we are getting an EObjectContainmentEList from the eGet, even though
the feature should not be a containment one in this branch.  Thus, the EObject
cast fails.

Maybe EObjectContainmentEList can be given as a result for non-containment
features?  Or something fishy is going on.

This happens reliably with the Petstore view, when clicking on any element of
the UML package, but not with other packages.

Also, this happens because the editor is trying to populate a menu with commands
for creating objects, which is currently not useful for views.

* [2018-10-10 mer.]
** A proper fix for EGL templates?                             :egl:emfviews:
I need to find a way to make a proper fix, since workarounds are not gonna cut
it, and EGL templates are actually useful for our demo purposes.

The problem currently is that EGL expects a metamodel and model given
separately, but the view (model) creates its own viewpoint (metamodel).  The
viewpoint, when created, will be registered in the package registry.  So maybe
we can just pass an URI to the view, and it will load the viewpoint from the
registry there?

Argh, what was I thinking.  The package registry contains packages, not
viewpoints.  Ok so we need to save the viewpoints in a registry.  But we can
make it opt-in.  The viewpoint file can have an option saveInRegistry, and the
view can refer to the viewpoint from its URI instead of file.

Okay, that worked!  For some reason, loading the view now takes forever though.
Need to investigate.

Hmm, now it seems to work fine.  Curious.

* [2018-10-15 lun.]
** Fix failing travis builds                            :travis:lfs:emfviews:
My deployment ended with:

#+BEGIN_EXAMPLE
ERROR: Authentication error: Authentication required: You must have push access to verify locks
error: failed to push some refs to 'git@github.com:atlanmod/emfviews.git'
Script failed with status 1
#+END_EXAMPLE

Was my deployment key suddently invalid?  No, it turns out this is a [[https://github.com/git-lfs/git-lfs/issues/2284][Git LFS
error]].  I used Git LFS to include the Java model for the Petstore example since
it was a few megabytes.  But apparently this broke the deployment script.

One workaround is to remove the pre-push hook for LFS, which is harmless here
since we don't touch the LFS during the deployment.

** Looking to upgrade the atlanmodexp server                    :atlanmodexp:
There is a way to load an image via IDRAC, but you need to use the virtual
console to attach it.  The virtual console is a Java applet.  I installed the
icedtea plugin and it launches, but after clicking through a few dialog boxes,
it finally fails with "Connection failed".

Crawled a bit, found [[https://gist.github.com/xbb/4fd651c2493ad9284dbcb827dc8886d6][steps]] to connect to the virtual console bypassing the
webstart non-sense.  Still "Connect failed".  Tried to change the java.security
file as suggested (to accept the disabled SSLv3 algo), no success.  Tried the
both credentials of idrac as user/password and the ones supplied in the
viewer.jlnp file.  No success.

Maybe updating idrac will do the trick?

It was a bit annoying to find the right file, as the DELL support site is a
mess.  The surefire way is to use the service tag/express code.  Then look for
iDRAC.  Ultimately, I found [[https://www.dell.com/support/home/us/en/04/drivers/driversdetails?driverId=MXKG2&osCode=WS8R2&productCode=poweredge-r710][this page]], which only had windows exe.  But in fact,
the ~iDRAC6_2.91_A00_FW_IMG.exe~ is an executable archive which yields a
firwmare image that can be accepted by Idrac.  Using Wine was enough.

Trying to update through the iDRAC interface failed though.  Trying again to
"wait 5 minutes and click update", as suggested [[https://www.dell.com/community/Systems-Management-General/iDRAC6-firmware-upgrade-failed-during-process/td-p/3855103][here]].  No dice.

Maybe I need to update the Lifecycle Controller first?  Hmm, [[https://www.dell.com/support/article/us/en/19/sln292343/poweredge-server-lifecycle-controller-update?lang=en][looks like]] we need
hardware access for that.

So I give up.  We need hardware access to do anything here.

* [2018-10-24 mer.]
** Reading about news MODELS'18 papers on view              :emfviews:papers:
First is Incremental View Model Synchronization Using Partial Models.

Strange focus on composability of view transformations and on parallel
transformations.  Not really well motivated.  Approach is incremental though,
but built on VIATRA, so reusing its incremental graph query engine.

Heavy use of mathematical notation that is often ill-defined or not at all.  Not
sure it makes the text clearer.

Section 2.2 is a useful classification of transformation engines (batch,
incremental, consistent, validating).  There's an accompanying comparison of
existing approaches.

Second paper is Distributed Graph Queries for Runtime Monitoring of
Cyber-Physical Systems.

The relevant part for us is this:

#+BEGIN_QUOTE
Runtime monitors are synthesized by /transforming high-level query specifi-
cations into deployable, platform dependent source code for each computation/
unit used as part of a monitoring service.  The synthesis includes a query opti-
mization step and a code generation step to produce platform-dependent C++
source code ready to be compiled into an executable for the platform.
#+END_QUOTE

Enticing.  But immediately followed by disappointment:

#+BEGIN_QUOTE
Due to space restrictions, this component of our framework is not detailed in
this paper.
#+END_QUOTE

There's another bit:

#+BEGIN_QUOTE
A graph query is evaluated according to a search plan [43], which is a list of
predicates ordered in a way that matches of predicates can be found efficiently.
During query evaluation, free variables of the predicates are bound to a value
following the search plan. The evaluation terminates when all matches in the
model are found. An in-depth discussion of query optimization is out of scope
for this paper, but Sect. 5 will provide an initial investigation.
#+END_QUOTE

But Section 5 does not in fact contains any relevant info.  However, the search
plan paper sounds more promising.

So the next paper is then An algorithm for generating model-sensitive search
plans for pattern matching on EMF models.  But maybe that one is best to read
/after/ we have an idea and prototype on how we want to tackle the similar
problem for OCL queries.

* [2018-10-26 ven.]
** Building a Bxml view                                    :csy:emfviews:emf:
I had to regenerate the Ecore metamodel from the new XSD.  After that, no issues
with loading bxml files or creating a dummy view with it.

However, trying to populate a virtual association between traces and identifiers
doesn't work.  The traces do have the new virtual feature, but the identifiers
don't.  Even with an always-true matching model predicate, the new virtual
associations remain empty.

There's something funky in these bxml models.  Clicking on identifier instances
in MoDisco often triggers (but not always!), cast exceptions in
VirtualEObject.dynamicGet:

#+BEGIN_SRC java
Object value = concreteEObject.eGet(concreteFeature);
if (feature.isMany()) {
  @SuppressWarnings("unchecked")
  EList<EObject> list = (EList<EObject>) value;
  return new VirtualEList(list, virtualizer);
#+END_SRC

See, I thought that a many-valued features would always be an EList... but here
it's not.  It's a BasicFeatureMap.  I knew the day would come where we would
have to handle feature maps.  Now I need to brush up.

* [2018-10-29 lun.]
** Feature maps                                        :csy:emfviews:emf:xsd:
Chapter 8 of the EMF bible was illuminating.  A feature map is mostly a way to
handle the situation where values need to appear in multiple features.

One of the use-case cited is for expressing the ~xsd:choice~ relationship in XSD
schemas.

Chapter 9 also helpfully describes the XSD to Ecore process, along with the
annotations we can add to the XSD in order to tweak the resulting Ecore.

So, on the one hand, EMF Views should support feature maps.  But on the other
hand, as the current bxml output is probably ill-suited for views (missing or
garbled info ...), we might as well export to a proper XML.  Or better yet,
directly to XMI with a simple corresponding Ecore metamodel!

** Dumping B syntax trees from the B compiler                         :b:csy:
With r201, uncommenting the code and simply calling ~compiler_save_betree~ on
the analysed btree is not enough:

: bcomp: B_COMPILER/d_misc.cpp:419: void T_object::dump_disk(T_disk_manager*): Assertion `end - start == metaclass->get_size()' failed.

What if.. I disable the assert?

Haha!  It succeeds.  Now I have a binary file.  But I'm not sure which is
simpler: reading that binary file and restoring the Betree info from it (in
order to dump it as XMI), or modifying the B compiler to directly dump the info
as XMI after the semantic analysis...  Presumably the latter has fewer steps.
And I might benefit from the existing API for navigating Betrees... or I might
fight it.  We'll see.

Also, the binary file might not be quite correct.  The magic number I get is
0fab ccba, whereas it should have the form x999 88yy.  So that binary dump is
probably busted.

The HTML dump is also useful.  However, it's concerning that most values inside
the dump are:

#+BEGIN_EXAMPLE
ILLEGAL ADDRESS 0x0x55c082c51e18
ILLEGAL ADDRESS 0x0x55c082c51f38
ILLEGAL ADDRESS 0x0x55c082c52148
ILLEGAL ADDRESS 0x0x55c082c5f628
...
#+END_EXAMPLE

or NULL.

Another helpful resource is [[https://tools.clearsy.com/tools/b-compiler/b-compiler-tutorial/][this tuto]].  At least navigating operation names
seems to work.  So I might be able to work out an XMI export.

* [2018-10-30 mar.]
** Further Bcomp investigations                                       :b:csy:
What a mess.

It's all C++ files, but actually it's plain C in OO-style mixed with some C++.
It's a mess of tabs and indentation and odious brace style.  There are debug
#defines everywhere, and commented-out code.  And the API is strangely
verbose...

: res->get_root()->get_abstraction_name()->get_name()->get_value()

I can understand the first two... but ~get_abstraction_name()->get_name()~,
really?

There's plenty of dead code as well.  The save/load options were not the only
options left in the code but not plugged into anything.  The 'R' option is as
well.

But more importantly, I do not understand how the semantic analyses are supposed
to work.  The syntactic analysis is fine.  The semantic one is supposed to look
at other files to construct an actual graph.  So in the file ~Treatment_i.imp~,
we have:

#+BEGIN_SRC
IMPLEMENTATION Treatment_i
REFINES Treatment
#+END_SRC

which refers to ~Treatment~.  The bcomp will look for a the files called
Treatment.ref, Treatment.mch and Treatment.sys, in that order.  Since it doesn't
find Treatment.ref, it fails and exits.

According to the doc, a .ref file is a refinement of an abstract machine.  The
implementation could refine a refinement, I guess.  But the compiler should
certainly keep looking.

Thanks for strace, it looks like the bcomp is looking.. in the current
directory!  And there's an ~-I~ option for including a directory in the lookup.
That solves this issue.

Looking at the ~Treatment_i.imp~ file, the bcomp has to open and parse 5 other
(.mch) files.  Now I'm expecting that these 5 files are accessible as betrees
objects.  Doesn't look like it?

If I iterate over betrees:

#+BEGIN_SRC c
for (T_betree *cur = betree; cur; cur = cur->get_next_betree()) {
   printf("%s\n", betree->get_root()->get_machine_name()->get_name()->get_value());
}
#+END_SRC

I get:

#+BEGIN_EXAMPLE
Treatment_i
Treatment_i
Treatment_i
Treatment_i
Treatment_i
Treatment_i
#+END_EXAMPLE

Maybe... ~machine_name~ is actually shared?  If I look at what operations each
machine has instead:

#+BEGIN_SRC c
T_machine *mch = cur->get_root();
T_op* op = mch->get_operations();
while (op != NULL) {
  string name = op->get_op_name()->get_value();
  cout << "Operation name : " << name << endl;
  op = (T_op*)op->get_next();
}
#+END_SRC

#+BEGIN_EXAMPLE
machine name: Treatment_i
Operation name : aopp_authorization
machine name: Treatment_i
Operation name : aopp_authorization
machine name: Treatment_i
Operation name : is_train_in_par
Operation name : is_train_stopped
machine name: Treatment_i
Operation name : are_doors_opening
machine name: Treatment_i
Operation name : get_inputs
machine name: Treatment_i
Operation name : print
Operation name : urgent_print
Operation name : flush
#+END_EXAMPLE

So they are not all the same!  But the have the same machine name, which is
confusing to me.

At this point I'm wondering if going the Xtext route to produce an Ecore from a
reduced grammar would not be more productive.

* [2018-11-06 mar.]
** Meeting with E
Some directions for interesting research with applications to emulators:

1. Use generic tools (like omniscient debuggers) on emulators
2. Build a complete emulator system using MDE tools (a model of CPU instructions,
   a model of GPU pipeline, a model of audio synthesis, ...)

There's also a strong component of generating performant code.

Maybe tackling real machines like GB or MD is too much work, but we can look at
CHIP8 or simple derivatives.

Another direction is in testing emulators: large-scale testing on a corpus of
ROMS, or testing comparing the behavior of many different emulators.

* [2018-11-08 jeu.]
** Xtext grammar for B                                          :csy:b:xtext:
Is not going quickly.  Making a grammar is hard!  I had trouble parsing binary
expressions, because ANTLR doesn't like left recursions.  Luckily there's [[http://www.eclipse.org/Xtext/documentation/301_grammarlanguage.html][a
section in the Xtext documentation]] about using assigned actions to eliminate
left recursions.

Still, even with this trick, the languages parses but we got a deep tree.  The
solution was to re-read the aforementioned documenation carefully, and find out
that writing this:

: TermExpr ({Expr.left=current} '&' ...)

instead of:

: left=TermExpr ({Expr.left=current} '&' ...)

made all the difference.  With the latter, we always construct an instance of
the rule, adding the TermExpr instance as a feature.  With the former,
since ~TermExpr~ is an unassigned rule we don't construct anything at this
stage, and ultimately build a TermExpr directly.

In my case I had multile levels, to handle operator precedence:

#+BEGIN_SRC xtext
Expr:
  left=ImplyExpr ({AndExpr.left=current} '&' right=ImplyExpr)?

ImplyExpr:
  left=EqualExpr ({ImplyExpr.left=current} '=>' right=EqualExpr)?

EqualExpr:
  left=TermExpr ({EqualExpr.left=current} '=' right=TermExpr)?
#+END_SRC

So, this would construct an Expr containing an ImplyExpr, containing an
EqualExpr, eventually containing a TermExpr, even though the expression was
terminal (a constant, let's say).

Dropping the ~left=~ as suggested by the doc gives me a single TermExpr for a
constant, an ImplyExpr for a '=>', and so on.  Which is good!  But not perfect.

See, the Imply and Equal expressions are clearly binary, but the AndExpr is not:
it's n-ary.  So if you have:

: a & b & c & d

You end up with an unbalanced binary tree:

#+BEGIN_EXAMPLE
  And
 /   \
a    And
    /   \
   b    And
       /   \
      c     d
#+END_EXAMPLE

Which is unecessarily verbose.  At first I tried:

: Expr
:  ImplyExpr ({AndExpr.left=current} '&' rights+=ImplyExpr)*

The multiplicy and the ~+=~ assignement ought to do it?  But no, still got the
degenerate tree.

Then I realised that the Kleene star probably applied to the assigned rule call,
and wrote that instead:

: ImplyExpr ({AndExpr.exprs+=current} ('&' exprs+=ImplyExpr)+)?

So the right part is optional, because we can have a standalone ImplyExpr, but
if we go into it then we do the tree rewriting to turn into an AndExpr, but then
we just push the rest of the ImplyExpr in the same feature.  So now I have:

#+BEGIN_EXAMPLE
       And
     / | | \
    a  b c  d
#+END_EXAMPLE

which is much more palatable.

Help.  I'm swarmed by non-LL decision errors, and I can't seem to find the
cause.  I mean, I can comment stuff that looks recursive and then ANTLR stops
complaining, but it doesn't /look/ recursive to me.  One problematic rule is
Condition:

#+BEGIN_SRC xtext
Condition:
  CondEq ({CondAnd.exprs+=current} ('&' exprs+=CondEq)+)?
;

CondEq returns Condition:
  CondLessThan ({CondEq.left=current} '=' right=CondLessThan)?
;

CondLessThan returns Condition:
  CondMinus ({CondLessThan.left=current} '<' right=CondMinus)?
;

CondMinus returns Condition:
  CondTerm ({CondMinus.left=current} '-' right=CondTerm)?
;

CondTerm returns Condition:
  '(' Condition ')'
  | {Ref} var=[Variable]
  | {IntLiteral} value=INT
  | {BoolLiteral} value=BoolLiteralEnum
#+END_SRC

Specifically, if I remove the first alternative of CondTerm, then no more non-LL
decision error.  However, it's not recursive!  If I'm taking the first
alternative, I'm clearly advancing the token stream.  What gives?

Ah!  [[https://blogs.itemis.com/en/debugging-xtext-grammars-what-to-do-when-your-language-is-ambiguous][This post]] suggests using ANTLRWorks which can help debugging the grammar.
Will try that tomorrow.

* [2018-11-09 ven.]
** Xtext B grammar progress                                     :xtext:b:csy:
After defeating the left-recursion hydra, I am now parsing all the B files!

It's not a total victory yet, as I now have to fix the scoping in order to
construct a proper graph of the project.

Xtext references are not very useful for a general purpose language like B,
because there are very little ways to distinguish the uses of the same token.
At first I had variables, constants, sets and operations, but I had to drop
constants and sets, since these could appear where variables could.. and the
grammar is context-free.

Still, the references work for Machines and Implementations, so I don't have to
handle that.

Next time: re-reading the linking/scoping part of the manual, and fixing that
for the grammar.

* [2018-11-13 mar.]
** Xtext scoping                                                :xtext:b:csy:
The [[http://www.eclipse.org/Xtext/documentation/303_runtime_concepts.html#scoping][Xtext documentation on scoping]] is a bit light.  The local scoping part is
fine enough, but the global scoping is hairy.

I looked at [[https://github.com/tetrabox/minijava/blob/master/plugins/org.tetrabox.minijava.xtext/src/org/tetrabox/minijava/xtext/scoping/MiniJavaScopeProvider.xtend][E's minijava]] and [[https://www.eclipsecon.org/france2017/session/deep-dive-xtext-scoping-local-and-global-scopes-explained][this presentation]] before realizing that, hey, I
don't need global scoping!  Overriding the default "local" scope provider is
enough, since I /can/ look at other resources from it.  That means I can get all
variables/operations defined in imported machines without having to mess with
what Xtext calls "global scoping".

Now just one issue to fix with some expressions that still do not parse (return
expressions that would cause left-recursion if naively added).

* [2018-11-14 mer.]
** Building a view for B + traces                            :csy:b:emfviews:
Some things are lacking in the model, to investigate next time:

1. We have to specify one B file in the view, but we would rather have the whole
   project as input model.  Do we specify multiple files?  (Doesn't work) Do we
   specify a folder?  Can we resolve all proxies in the B file in ECL?

2. As a workaround we tried to build a view aggregating all B files, with an
   empty weaving model.  This works, but then when we try to use that view as a
   model in another view, we cannot find the virtual association "identifiers"!

3. There seems to be duplicate entries for the "aopp" variable in the view.  A
   bug due to forcing insertion of values in the opposite virtual reference?

* [2018-11-23 ven.]
** Whether to use Gitlab for hosting EMF Views builds       :emfviews:gitlab:
Our self-hosted Gitlab is busted: the SSL certificate is out of date and the IT
departement is probably not goint to change it.  Besides, our version is super
old.  It probably hasn't been updated since it was first created.

On the other hand, the version hosted by the university is up to date.  I'm
seeing [[https://gitlab.univ-nantes.fr/help][version 11.4.7]], and [[https://gitlab.com/gitlab-org/gitlab-ce/blob/master/CHANGELOG.md][the latest]] (which came out yesterday) is 11.5.0.
Gitlab releases every month, so this is looking good.

Gitlab is capable of [[https://docs.gitlab.com/ee/user/project/packages/maven_repository.html][hosting a Maven repository]], but it's for paying users (even
for self-hosting!).  I was a bit confused at first, but the community version
only has the [[https://about.gitlab.com/pricing/self-managed/feature-comparison/]["Core" features]], which includes CI/CD but not the Maven repo.

So we could probably use this Gitlab to host all our JARs.  But we could do it
on Github as well.  It doesn't make sense to use Gitlab just for one part,
unless we want to reduce our dependency on Github.

While I'm at it, I'll check if we can use CI/CD on the university Gitlab.  From
what I understand, the built-in CI/CD requires that runners be installed.  I'm
guessing we can always fallback on using external services like Travis, or [[https://github.com/travis-ci/travis-ci/issues/5931][maybe
not]].

Well I guess I'll try with a dumb project and see how that goes.  Okay looks
like it works!

[[file:doc/gitlab-ci-test.png]]

If I understand the Runners configuration correctly, there appears to be exactly
one Docker shared runner.  But on further testing, it appears that runner is
able to spawn multiple containers per project.  I was able to run three jobs
concurrently: two in the same project, and one in another project.

However, for pages hosting:

#+BEGIN_QUOTE
Support for domains and certificates is disabled. Ask your system's
administrator to enable it.
#+END_QUOTE

So it means we won't be moving our public-facing project with pages to this
Gitlab, unless we ask the admins if this is a possibility.

* [2018-11-26 lun.]
** Putting the formalism in the journal paper           :emfviews:formalism:
Trying to use dots above symbols instead of the V suffixes.  Maybe we need a
different way to signal stuff that's part of the viewtype, as dots are easily
missed, and look a bit weird on longer names like /range/.

I updated the text to include multiple Ecore models in the definitions.  It's
tedious having to lug these extra i suffixes around, especially for the
functions of P: domain_i, range_i, etc.  It might be simpler to lump them all and
say we have functions for a set of models which act as you expect.

I thought I solved the problem we had with inheritance by simply using a
transitive relation.  If we remove elements from the sets participating in the
relation, the transitivity still holds.  But actually if we have a < b < c, and
note << the viewtype inheritance relationship, and exclude b, then we cannot
have a << c by transitivity.

So we should probably define << as a restriction on the transitive closure of <.

* [2018-11-29 jeu.]
** Updating the EMF Views update site manually         :emfviews:atlanmodexp:
Had to do that to help with the CSY use case.  Luckily the command was in the
history:

: mvn -Dmwe2-skip-generate=true package
: scp -rP 10922 update/target/repository/* atlanmodexp.info.emn.fr:/var/www/html/updatesite/snapshot/

Hopefully we'll get CD up soon enough.

* [2018-12-05 mer.]
** Troubles with EMF Views build from scratch                :emfviews:maven:
Forgot to add the sexp2emf dependency.  On the other hand, it should only be
required for testing.  Maven has the ability to 'scope' dependencies, but here
we are using Tycho, which pulls dependencies from Eclipe's MANIFEST files, which
do not have this extra metadata.

On the same topic, building the manual requires Emacs... which is an extra
involved step for people just wanting to try out the plugins.

Maven allows you to disable specific sub-modules:

: mvn -pl !doc

But the emfviews feature requires the doc plugin in any case, so you would have
to exclude all of them.  Instead, I created an 'update-site' profile which
builds the update site and doc plugin.

It can even use that profile by default if the toc.xml exists (presumably because
the doc has been built).  Nifty!  But a bit dangerous if we move folders around
at some point.  Better be explicit than magic.

** Looking at Git for hosting our JARs                           :git:neoemf:
I can see what the problem is with NeoEMF.  I'm trying to clone the repo, and
it's at 3GiB.  Pretty bad.

I'm pretty sure we can just scratch the history of the gh-pages branch to avoid
ballooning the size.  I think a combination of --orphan to create a branch
without history, adding all files to it, and force-pushing to a remote branch
should do it.

Let's test it out.

Yeah, seems to work.  Just create an orphan branch:

: git checkout --orphan new-gh-pages

and force-push it to the remote branch:

: git --force push origin new-gh-pages:pages

Tada!  History lost.  We could deploy using that.  We could have two rotating
branches as well, to be on the safe side in case the deploy script fails.

* [2018-12-06 jeu.]
** Investigating a failing maven build                       :emfviews:maven:
So:

1. Cloning emfviews and doing ~mvn verify~ fails to run the tests with null
   pointer exceptions.

2. Travis has no issue building and running the tests.

3. Calling ~mvn verify~ on my local copy works fine.

4. [[https://github.com/atlanmod/emfviews/issues/4#issuecomment-444560147][This]] is yet another error.

Trying out with a more recent Eclipse release.. 4.8, 4.9.  First of all, had to
track down the correct p2 repositories to use.

If you go [[http://download.eclipse.org/eclipse/downloads/drops4/R-4.9-201809060745/][here]], you will see links to a p2 repository, like this:

: http://download.eclipse.org/eclipse/updates/4.9/

But trying to use that one in our pom.xml will trigger unmet dependencies,
something about

: org.epsilon.emc.emf requires org.eclipse.emf 0.0.0 but could not be found

No EMF?  What?

Turns out, the p2 repository is not a main release one, so you don't get the
basic stuff in it (even though there seems to be EMF plugins in there...).

Anyway, to get past this error, I used the [[https://wiki.eclipse.org/Simultaneous_Release][simultaneous release p2 repositories]]
instead.

Now we get /another/ dependency error.  Actually we get a bunch of them, and
most of them seem to complain at some point about missing ~javax.annotation~.

Well, [[https://bugs.eclipse.org/bugs/show_bug.cgi?id=539038][I'm not the only one]].  Something to do with java 9 and 10 modules
splitting stuff out of the core JDK.  Apparently one workaround is to manually
require javax.annotation in test modules.  Great.

It does work.  In the sense that I don't have a missing dependency anymore.  Now
instead I get back to state #1:

#+BEGIN_EXAMPLE
addSuperclass(org.atlanmod.emfviews.tests.TestVirtualObjects)  Time elapsed: 0.036 sec  <<< ERROR!
java.lang.NullPointerException
	at org.atlanmod.emfviews.tests.TestVirtualObjects.createConcreteModel(TestVirtualObjects.java:75)
#+END_EXAMPLE

Same issue under 4.8 and 4.9.  So upgrading the eclipse release makes no
change.  Using java8 instead of java10 makes no change.

Now, these NPE all seem to come from the EcoreFactory.eINSTANCE call returning
null.  Why does that happen, and why can't I replicate it only in a fresh clone?

Ok so actually, the NPE comes not from EcoreFactory.eINSTANCE being null, but
from EcoreFactory.eINSTANCE.getEPackage() returning null.

A ~mvn -Dmwe2-skip-generate=true clean verify~ on my development version of
emfviews /succeeds/ in running the tests.  No NPEs.  What is different here?

OKAY fantastic.  I was testing my fresh clone in /tmp, and on a hunch I tested
another fresh clone in /home instead.  And tada!  The emfviews.test pass.

Lesson learned: do not run mvn in /tmp.

LO AND BEHOLD.  A ~mvn clean verify~ works perfectly on a fresh clone (outside
/tmp).

I can also build the manual:

: pushd doc
: cask install
: make
: popd
: mvn clean verify -P update-site

* [2018-12-13 jeu.]
** Streamlining the EMF Views installation                 :eclipse:emfviews:
So, even though we have a nice update site, we also have extra dependencies that
may require the user to install additional stuff before they are able to install
EMF Views.

If they grab an Eclipse Modeling Tools distribution, they get Xtext and ATL, so
the only thing missing is Epsilon.  And that's only one update site to add.

However!  The experience is still rather terrible.  Try to add EMF Views, wait
for Eclipse to compute alternate solutions... then if you squint when reading
the error message you will find that it's chocking on an epsilon dependency.  If
you then go back to the README, you will find that Epsilon is actually
required... and if you install it everything goes smoothly from there on.

But it's stupid!  Why doesn't Eclipse let you specify another update site to
contact during installation?  Well, actually it does.  There are in fact
/multiple ways/ to specify that, but none of them work.

In a feature.xml, you can specify "Sites to visit".  This does nothing.

In a category.xml for an update site, you can specify an "Additional
repository".  This is [[https://bugs.eclipse.org/bugs/show_bug.cgi?id=426531][exactly what we want]].  But, first gotcha, [[https://bugs.eclipse.org/bugs/show_bug.cgi?id=453708][it doesn't work
with Tycho]].  So if you use Tycho to build the update site, that setting does
nothing.

At the end of the day, that setting is supposed to [[https://stackoverflow.com/a/8279204][add a couple of lines to an
XML file]], and that could be done with an external script.  Alternatively, just
to test it, you can use the Export command of Eclipse to build the update site
without Tycho, and there the content.xml file has the covoted lines:

#+BEGIN_SRC xml
<references size='2'>
  <repository uri='http://download.eclipse.org/epsilon/updates/' url='http://download.eclipse.org/epsilon/updates/' type='0' options='0'/>
  <repository uri='http://download.eclipse.org/epsilon/updates/' url='http://download.eclipse.org/epsilon/updates/' type='1' options='0'/>
</references>
#+END_SRC

Do not ask me why the URL needs to appear /four times/ in the XML.  Probably
because it wasn't verbose enough otherwise.  Also, there's no documentation for
this.

But, testing that out in a fresh Eclipse does not make a difference.  Well, it
does make one little: after /failing/ to install emfviews due to not finding
epsilon, I now have an extra update site:

[[file:doc/eclipse-drives-me-mad.png]]

The Epsilon one.  So these two lines had an effect.  But look!  The update site
has been disabled automatically, so it cannot be used to fetch the dependency.

I tried to, blindly, change the 'options=0' into 'options=1'.  No dice.

At this point, I throw my hands in the air, curse the software gods, and give
up.

* [2019-01-21 lun.]
** Thinking about splitting requests                    :emfviews:neoemf:ocl:
*** Computing in RAM or leveraging DB indexing?
List of names in DB, need to match them against another list of names in RAM.
Assuming you can't load the DB in RAM, to do the matching you would pull the DB
names one by one, and match them against the ones in RAM.

(Although for really large datasets, using using a probablistic data structure
like a bloom filter may be best)

But maybe the other way would be more efficient?  Dumping the RAM names in DB,
do the matching using the indexed structure of the DB, and return the result to
RAM?

*** Lazy query execution
Other typical problem: do some complex joins, to only get the first line of the
result: map.join.map.map.first().  Here the ineffiency comes from computing the
query eagerly.  Nothing groundbreaking, but adding laziness may skip hitting the
DB entirely, depending on the query.

On a related note, I read about [[http://adapton.org/][ADAPTON]], a language for incremental computation
that ticks many boxes.

*** Matching subqueries with model sources
Before even considering optimization, we can ask: can we statically determine
which parts of a query can be run on a given backend?

trace::Log.allInstances()
  ->any(l | l.message.startsWith('CaptchaValidateFilter'))
.javaClass._'package'.component.requirements->size()

Assuming all Log instances come from the same source, ~any~ will only reduce the
collection so stay in the same source.  ~javaClass~ brings us into a different
source.  We know that because, in the viewpoint, the Log class has a javaClass
attribute with type modisco.java::ClassDeclaration, and the view knows that the
modisco model comes from an XMI file.

It seems we can do that in general, and at worst we would get an upper bound of
on the set of sources a query might touch.

That information could then be used to make a request plan, eventually splitting
or parallelizing queries in backends.

* [2019-01-23 mer.]
** Algorithm for identifying sources                    :emfviews:neoemf:ocl:
Inputs:
- An OCL query expression
- Context object for the OCL query (optional)
- Metamodel, with the ability to map features to model sources

Output:
- S, the set of model sources "touched" by the query

Algo:
1. If we have a context object, add its source to S.
2. Get an AST for the OCL expression.
3. Interpret the AST.  For each node:
   - If it's a Type (~Foo::A~), add the source to S.
   - If it's a navigation to a reference (~a.bar~), add the source of its target
     type to S.
4. Return S.

Straightforward.  The only useful thing you can do with that is that if S only
has one source, you may dispatch the query to that source directly.  E.g., if
the source is in NeoEMF, you may be able to use Mogwaï to convert the query to
Gremlin, and reap the benefits.

Better would be to identify sub-sequences of the query that are tied to a single
source.  E.g.:

reqif10::SpecObject.allInstances()
  ->any(r | r.values->selectByType(reqif10::AttributeValueString)->exists(v | v.theValue.startsWith('Controller')))
    .components
  ->collect(c | c.javaPackages)
  ->collect(p | p.ownedElements)
  ->selectByType(java::ClassDeclaration)
  ->collect(c | c.traces)
  ->size()

Fisrt two lines are for the ReqIF source:

reqif10::SpecObject.allInstances()
  ->any(r | r.values->selectByType(reqif10::AttributeValueString)->exists(v | v.theValue.startsWith('Controller')))

And these two are for the Java source:

->collect(p | p.ownedElements)
  ->selectByType(java::ClassDeclaration)

To get that kind of information, we could tag each node of the AST with its
sources.  Then we could collect sequences of homogeneous sources.

We can get that information mostly by looking at type of flowing data:

reqif10::SpecObject.allInstances()     : Set[reqif::SpecObject]
->any(...)                             : Set[reqif::SpecObject]
->collect(c | c.javaPackages)          : Set[java::Package]
->collect(p | p.ownedElements)         : Set[java::AbstractTypeDeclaration]
->selectByType(java::ClassDeclaration) : Set[java::ClassDeclaration]
->collect(c | c.traces)                : Set[trace::Log]
->size()                               : int

The predicate inside ~any~ is interesting:

r                                             : reqif::SpecObject
.values                                       : Set[reqif::AttributeValue]
->selectByType(reqif10::AttributeValueString) : Set[reqif::AttributeValueString]
->exists(...)                                 : boolean

Inside ~exists~:

v                         : reqif::AttributeValueString
.theValue                 : string
.startsWith('Controller') : boolean

Here we are staying inside reqif, but the predicate could have queried another
model source.  E.g.:

trace::Log::allInstances()
  ->any(t | t.total == java::ClassDeclaration.allInstances()->select()->size())
  .message

where the predicate queries the java source.  Disregarding the fact that here
the result of the right-hand side is likely constant for the duration of the
query, and could be cached, it's an example where the external sequence stays in
Log, but actually we are intermingling calls to the Java model source.

* [2019-01-24 jeu.]
** Building an OCL parser                                      :ocl:analysis:
Trying out [[http://tree-sitter.github.io/tree-sitter/creating-parsers][tree-sitter]].  Lost 30min because the instructions installed the
0.14-beta3 version, which does not include tree-sitter proper, hence failing to
compile the generated grammar.

The grammar language is a bit rough, but using the latest 0.13 version works.

Error messages are rough as well.

Okay, after a lot of tweaking around, I got a prototype going.  It's
tree-sitter + codemirror inside electron.  Since tree-sitter generates a native
node module, I cannot just use it on a webpage.  I wanted to use codemirror as
editor, so that left not many options.  Electron was quite painless to setup.

* [2019-01-25 ven.]
** Further improvements on the OCL toy                         :ocl:analysis:
So, using Tree-sitter and hooking into the change event of codemirror, I'm able
to reparse only the changed bits of an expression.  In /theory/.  In practice,
I've got bugs trying to convert the change data into something tree-sitter is
expecting.  The documentation is lacking in what exactly is expected.

Well, anyway that's overkill for the size of the expressions I'm dealing with.

I've added highlighting of syntax errors using the codemirror lint plugin.
Tree-sitter does not provide good errors out of the box, but I guess you could
walk the AST and generate good errors by looking at the type of the node and
what is inside.

Tree-sitter is able to generate a graphviz dot file for the CST.  Nifty.  But
I'm going to generate an AST with D3 anyway.

* [2019-01-30 mer.]
** Good progress on the type checker                           :ocl:analysis:
It now seems to type check all the examples I drew up.  It's not handling helper
functions yet, but it's sufficient to type check the queries from the MODELS
paper.

It's a bit verbose.  Calling it on:

: trace::Log.allInstances()->size()

yields:

#+BEGIN_EXAMPLE
(query : number
  (prim_expr : number
    (path_expr : number
      (path : Classifier trace::Log
        (identifier : Package trace)
        (identifier : Classifier trace::Log))
      (call : Set trace::Log
        (identifier : Classifier S . () -> Set S))
      (call : number
        (identifier : Set S . () -> number)))))
#+END_EXAMPLE

We could omit some redundant nodes like ~prim_expr~ and ~path_expr~.  Actually,
that's easy to do.  I just did it.

So now we have:

#+BEGIN_EXAMPLE
(query : number
  (path : Classifier trace::Log
    (identifier : Package trace)
    (identifier : Classifier trace::Log))
  (call : Set trace::Log
    (identifier : Classifier S . () -> Set S))
  (call : number
    (identifier : Set S . () -> number)))
#+END_EXAMPLE

The type-checker could probably be simplified, as it's a bit ad-hoc currently.
There are two ways to go about it.  The first one is to view the type-checker as
an abstract interpreter of the query, so when you go down the tree, you do what
an OCL interpreter would do, but instead you check the input types, and return
output types.

What's interesting is for situations like this:

: Log.allInstances()->any(l | l.name.startsWith('foo'))

When interpreting ~any~, you get a ~Set Log~ as input (the receiver to the
method call), and a lambda as input.  You evaluate the lambda (i.e., get its
type), and get ~Log -> bool~.  The pseudo-code for ~any~ will thus be something
like:

#+BEGIN_SRC js
function any(self, pred) {
  // 1. Check self is a Set of T (emit error if not)
  // 2. Given pred.arg has the type T, evaluate pred.body
  // 3. Check pred.body has type bool (emit error if not)
  // 4. Return type T
}
#+END_SRC

Of course, T is a concrete type, ~Log~ in this case.

I actually started this way, but then it was obvious that all operation calls
(any, select, collect...) were the same.  It became tedious to duplicate the
checks for arguments type and predicate types.

Instead, the second way to implement it is to define a small language to
describe how these operations behave:

#+BEGIN_EXAMPLE
any    : Set T . (T -> bool) -> T
exists : Set T . (T -> bool) -> bool
collect: Set T . (T -> S)    -> S
#+END_EXAMPLE

And implement the method call generically, just by interpreting this type
annotation.  The tricky part was handling the type variables T and S correctly,
so that applying ~any~ to a ~Set Log~ would return the type ~Log~, and not just
~T~.

The way I ended up doing it is by first checking that types are compatible, that
is, if they are strictly equal (bool == bool) or if one of them is a variable.

Then, a second step is to 'flow' a concrete type into a type potentially
containing variables.  This is done in two steps.  First, gather the bindings
for type variables.  If we look at the receivers we have a pair of the actual
receiver type and the expected receiver type (the part before the dot in the
type signature of any):

: (~Set Log~, ~Set T~)

Trying to unify these types yields the type bindings:

: [ T: Log ]

Once we have the bindings, we can flow them into the larger type of ~any~:

: [ T : Log ]
: Set T . (T -> bool) -> T

replacing T by Log yields:

: Set Log . (Log -> bool) -> Log

This replacement is just like the usual replacement of free variables in
function application, except here we are substituting types.

The whole process feels a bit ad-hoc, but it works.

You have to repeat it for arguments as well, since each argument can have type
variables, they must be flowed into the expected type in order to get a fully
concrete type.  For ~collect~, it's not until you have given the type of the
lambda argument that you can know the return type (S).

The big advantage of using the type annotations is that now you just give the
type of the operations, and you don't need to have custom code emulating what
the operation should do.  It seems that this very simple type language with
variables, akin to Java generics, is enough to type all the examples I've tried.

One thing the type system doesn't handle is subtyping.  I haven't found a
necessity to use it now, but it comes up in the ~selectByType~ operation.  It's
supposed to return all instances of T from a Set of A, where A is a supertype of
T.  So its type should be:

: Set T . (Classifier S) -> S  where S <: T

And if we wanted to build a type analysis to check that queries are correct, we
should check the relationship between S and T.  But for our purposes, we don't
need to!  All that matters is to know the return type, so the type can simply
be:

: Set T . (Classifier S) -> S

and if S isn't actually a subtype of T, then the query will not even run!

-----

I also got synchronized highlighting working between the type tree and the code,
so moving the cursor over the code highlights the expression at point and in the
type tree, and vice-versa.

Pretty nifty!  Now what's left is the dependency analysis.  I'll probably go
with merging tree nodes from the leaves up, as long as they are in the same
source.

* [2019-02-01 ven.]
** Why use a static analysis?                                  :ocl:analysis:
Answer to a question I got from E, which may be need to be addressed in the
paper.

Why couldn't we hook into the OCL (or ECL/ATL) interpreter and use the info
that's provided there?

Answer: it's too late.  If we hook into the individual operations, we cannot
know if OCL already executed part of the query before an operator.  If we want
to split the query, it might be too late.

We have to look at the whole thing first before passing it down to OCL, as we
may bypass OCL completely by rewriting a subquery and passing it to NeoEMF/CDO.

Implementation-wise, we would do the static analysis at /runtime/, and we could
even re-use the type analysis of OCL (if there is one that precedes
evaluation).  But it's still static because we haven't executed the query yet
(and maybe we will transform it).

* [2019-02-04 lun.]
** Using gzip for identifying patterns in large logs                    :csy:
[[https://dl.acm.org/citation.cfm?id=1379445][Astute paper]].  We were talking about this issue CSY have, which is to find
patterns in very large execution traces.  Some patterns they already know, some
they don't.  What would be interesting is to automatically find patterns from
the raw logs, which are too large to feasibly comb by hand.

I thought of using a similar technique as the LZ algorithm to detect redundancy.
Well, that paper did just that, but not by reusing the algorithm techniques, but
by using the compression algorithm directly, and measuring the compression ratio
to detect "interesting" chunks of logs.

You may have to massage the logs a bit (the "anonymizer phase" in the paper),
but the technique is quite general, as long as you are looking for redundancy.

* [2019-02-13 mer.]
** Using queries to populate a view                                :emfviews:
With the CSY use-case, we hit two limitations of EMF Views.  First, using ECL
for matching traces to other traces in a large log is too slow.  We had a
similar issue in the MODELS paper where I had to write a Java program to output
the view weaving model using a faster algorithm.  Here I did the same thing.

It's unfortunate because this manual weaving model cannot be "called" by EMF
Views directly.  And it's not even a very large file.

The second limitation was that I wanted to add a virtual attribute whose value
would have been computed from two other attributes.  There was no way to do that
at all in EMF Views, since the view weaving model did not take properties into
account.

Now, it looks to me that instead of using a matching model with ECL, we could
provide a 'build model' with EOL/OCL/ATL.  The build model would contain queries
for each virtual attribute/relation/concept.  We would then call the
corresponding executor in order to populate the view when we load it.

As a first step, we could do all this populating upfront.  More interesting
would be to execute the query only when the virtual feature is accessed.

The viewpoint weaving model could hold the queries for each feature, and the
view could access these to populate itself.

However, the CSY example is interesting, because the attribute query needs to
refer to the traces that will populate a virtual association, and computing them
again would be wasteful (and slow!).

* [2019-02-15 ven.]
** Prototype of using OCL queries to populate a view           :emfviews:ocl:
Added a query field on the weaving model, and using OCL to execute that and
populate the view.  Unresolved issues:

- What should the context of the query be?  We have multiple input models, but
  OCL is not able to query all of them at once.  The context should thus be the
  view itself, but at this step the view is still in construction, so we have to
  be careful.  In any case, either we enforce an order on query execution, or
  things may explode in the case of recursive queries (if query A inspects the
  value of B and vice-versa).

- What do we do if a view weaving model is also provided?  This should only
  contain computed elements, so we could simply add them to the ones already
  computed by the queries.  This has the nice side-effect of keeping backwards
  compatibility.

  Though I'm also tempted to just drop view weaving models, and use the query
  mechanism instead.

- At first I went with EOL, because that would have been more interesting for
  us.  But EOL are complete programs, and I couldn't find a "query" subset of
  the language I could use.  This raised more questions: how to use the EOL
  language to communicate to the view builder the new elements?  There is no
  "return value" as in OCL.  Should we use specially-named top-level variables?
  Should we provide a phony "ViewOutput" class that the EOL program should
  instantiate and populate?

  Using EOL for populating the view has the potential to be the most efficient
  option (since with OCL you may have duplicated computation between queries,
  with no way to reuse them), but we still need to find the adequate syntactic
  option.

* [2019-02-19 mar.]
** Populating a virtual association using an OCL query         :emfviews:ocl:
I hacked a first version yesterday.  It works, but it was too slow at first.
The problem was: the query to populate the virtual association ~m24~ was
executed on every instance of Trace.  In the Java program I wrote to bootstrap
the view, I'm not running the query on all instances, rather, I'm only
considering a subset of traces that have the ~fallback~ variable.  As there are
only 4 of them, compared to >1000 instances of Trace, finding the corresponding
sensor is much faster.

This highlights the need to add guards to queries.  OCL has pre-conditions, and
we could use the same thing: queries that output the set of objects you want to
execute the populating query on.

Another way of doing it is to let the user provide an OCL "builder program", in
which the user is able to direct the flow of construction.  So instead of
giving:

: Event.allInstances()->asSequence()->select(... 'm24' ...)->last()

for the virtual association ~m24~, and potentially doing the same thing for
populating the association ~m21~, you would write a single program:

#+BEGIN_SRC ocl
events = Event.allInstances()->asSequence()
sensor(name) = events->select(... name ...)->last

m24 = sensor('m24')
m21 = sensor('m21')
#+END_SRC

With ~m24~ and ~m21~ being "special" values that will populate the view, while
~events~ and ~sensor~ are just helpers.  Of course we would need a distinctive
syntax to avoid name clashes here, but that's a syntactic detail.

There are several benefits to this approach:

1. It reduces duplication.  Here we had two virtual associations that are
   populated using the same query, with just one parameter changed between them.
   Above, we can use a helper to factorize this duplication.  More readable,
   DRY, etc.

2. It prevents redundant computation.  The OCL evaluator has the opportunity to
   cache the result of ~events~ here, and other parts of queries, since
   everything is running in the same "execution".  Which is not the case of the
   current prototype, where we have one new OCL instance for each source model
   element.

3. It leaves control to the programmer.  It's basically an inversion of control:
   you do not call my queries, I populate the view.  More flexible, as having
   one query field per virtual link is too restrictive.

Now, ideally, we may not want to execute these queries every time we open the
view, because these queries may be expensive.  A first solution would be to
allow caching of these results.  Which means we still need the view weaving
model to exist (in some form), in order to persist it.

But it would be best to make sure whatever solution would allow us to /lazily/
build the view in the future.  Working on the MODELS paper last year, I started
hacking together a lazy version of the view, which did not iterate the weaving
model and only registered 'proxies'.  Trying to get the value of a feature would
'force' it, and actually retrieve the links from the weaving model.

With queries, it seems both the current solution and the 'builder program' one
are amenable to lazification.  Instead of executing the queries outright, we
just register them to be played whenever the feature is accessed; then the
result is cached (which leads to cache invalidation, but that's a concern for
later).

* [2019-02-20 mer.]
** Prototype to populate a view with an OCL document           :ocl:emfviews:
So, my previous attempt was using queries attached to individual virtual
property/associations.  But then it appeared that it would be more flexible to
use a full OCL program to drive the building of the view.

I've built a small prototype, not hooked to EMF Views yet, that just takes an
OCL document (in the CompleteOCL language), like this:

#+BEGIN_SRC ocl
import 'platform:/resource/emfviews.test/resources/metamodels/Log.ecore'

package Log

context Trace::name : Trace

init: self.variable = 'fallback'
derive:
  let events = Event.allInstances()->sortedBy(e | e.timestamp),
      sensors = events
                  ->subOrderedSet(1, events->asSequence()->indexOf(self.oclContainer())-1)
                  ->collect(e | e.traces)
                  ->select(t | t.sensor = 'M24' and t.variable = 'Position')
  in if sensors->isEmpty()
     then null
     else sensors->last()
     endif

endpackage
#+END_SRC

and calls OCL to obtain the objects we should eventually put in a view.  The
above program works, and the good thing is that the code consuming that OCL
document does not contain model-specific hacks; it's generic.

However, there are still a few hurdles to clear:

1. The ~import~ should point to the viewpoint, but OCL expects an Ecore file
   here.  Hopefully there's a way to instruct it to load the eviewpoint and get
   an EPackage as expected.  Currently I've pointed to the contributing
   metamodel, but then I cannot use the virtual properties since they are not
   present in that metamodel.

2. I'm using the PropertyContext to specify an extension, because I needed two
   sub-expressions: a pre-condition (the guard) and the query used to obtain the
   result for the virtual property.  So here ~init~ is the guard, and ~derive~
   the query.

   But actually, it would be nice to be able to specify a default value using
   ~init~.

   Now that I think of it, we can probably move the guard in the ~derive~... But
   having the guard as a separate syntactic entity would allows some
   optimization down the road.  Currently, it's noticeably slower to obtain the
   view information than with the previous prototype which hardcoded the guard.

   For each element, to know if it's filtered out, we must: create an OCL query
   (parse it), then execute it.  It's a significant overhead to using ~eGet~
   directly for instance.  If we restrict the guard to very simple expressions
   that we are able to interpret into calls into the EMF API, bypassing OCL, we
   would probably gain.

   If I hardcode the guard with eGets, the view information is extracted in ~1
   second, while it takes ~13 seconds through OCL.

3. I thought I would be able to reuse the OCL query objects, but apparently
   calling query.evaluateEcore a second time raises an exception.  Still, it's
   only a minor optimization point.

* [2019-02-27 mer.]
** Running EOL queries on views with NeoEMF resources   :eol:neoemf:emfviews:
Was a hassle.  Because NeoEMF resources are linear, and I wanted to use a
matching model in ECL to create the view weaving model, and the Epsilon Delegate
only knew how to open models via a resource URI (even though we give it a list
of already loaded resources...).

Turns out, there's an ~InMemoryEmfModel~ implementation of ~EmfModel~ which
shortcuts this roundabount, and solves the issue of opening the NeoEMF resource
twice.

Now, trying to write an EOL query on that view, I ran into a [[*Creating HTML from a view][familiar problem]] of
Property not found.  Turns out, I forgot to virtualize objects in the result of
View.getAllInstances which was called by our EMFViews connector with epsilon.
D'oh.

Now it works.  Have to figure out how to intercept operations and split queries
now.

* [2019-03-15 ven.]
** Using arbitrary Epsilon models instead of EMF resources :emfviews:epsilon:
Epsilon has a vast range of connectors allowing you to use EOL and other
languages on EMF resources, CSV files, Bibtex files, etc.

It could be interesting to allow view loading such files.  The problem is that
right now we are tightly coupled to EMF.  Viewpoints need Ecore packages, views
expect resources...

When you look at what a View does, ultimately it just wraps EObjects and keeps
values for virtual associations in the wrapper.  With EMF, the wrapper is also
an EObject, and we use eSet to populate the virtual associations.

The point of EMF is to provide a unified interface (EObject) to manipulate model
elements.

I tried matching between an EMF model and a CSV file using Epsilon.  The match
gives me EObjects for the EMF model and ... a LinkedHashMap for CSV.  In
general, in ECL, you can get any Object.  So you don't have a usable interface
to do anything with that result.  The only solution is to use reflection.

To load CSV with EMF Views then, it would make more sense to have an EMF
interface to CSVs...

* [2019-03-18 lun.]
** Loading a view with an Epsilon model                    :emfviews:epsilon:
It worked.

First problem is loading an Epsilon model as an EMF resource.  Defined a dummy
Resource object that calls an adequate EMC driver depending on the file we are
looking at.  In general, a map from extension to factory like EMF does it would
suffice.

Then the resource needs to be able to yield its contents.  IModel has
allContents, but no notion of containment references really.  So we may have to
add a dummy root element to Epsilon models.

Then we want to actually query the contents.  The EMF API provides eGet and
eSet, but eGet takes a feature object.  We don't have a notion of metamodel on
Epsilon, so we have to pass a mock feature object (new EpsilonFeature("name")),
and since the eGet is going to end up on a VirtualEObject, we can special-case
that.  If it's an EpsilonFeature, then the concrete object is an Epsilon model,
and we can use the PropertyGetter of that model to access the feature.

Not sure how it would work more generally, but that's a good first step.

Next, to see how we can handle Simulink models.

** Call with G.                                            :emfviews:epsilon:
Need to nail the motivation of the paper.  We have two contributions in mind:

1. Extend EMF Views to load Epsilon-based models.

   Au passage, this lets you handle Epsilon-based models through the EMF API.
2. Optimize queries on views when they are made through Epsilon technologies.

For #2, why use EMF Views at all?  You can already run EOL queries on several
models.  "Virtual features" can be defined through helpers in a query.  You
already leverage optimized EMC drivers (e.g. JDBC), while by adding EMF Views in
the mix you have to re-implement the delegation to get the optimized versions
back.

Well, one argument might be that you want to benefit from what the view gives
you.  Virtual features are not more expressive than helpers, but they are
defined when the view is designed, not when it is queried.  It's a matter of
shifting the concern.

Also, views can filter elements from contributing models.  Restrictions of
concerns, access control.

The overall motivation looks like:

1. Views are useful, we want views.
2. EMF Views solves the view problem, but only for EMF models.
3. Epsilon has a different approach that lets you query non-EMF models.
4. Epsilon does not have views.
5. Now you can have views in Epsilon.

Point 4 is where the counter-argument above can be raised.  "Epsilon does not
need views".  Need to support all the 5 points.

*** Epsilon vs. EMF
EMF lets you build the infrastructure for dealing with models.  You need to
define a metamodel, but then you get the code and associated tools to work with
the models.  However, the serizliation is handled by the framework.  You are
locked in the framework basically.  Also, upfront costs of having to define a
metamodel.  And unbearable if you already have models in ad-hoc formats (unless
you solve automatic discovery of EMF models).

Epsilon works the other way around: you have models, but lying around in
different formats.  You still want to use nice tools for them.  Epsilon lets you
query them.

Question for York: why is there no interest in having a way to handle Epsilon
models through the EMF API?  There is an existing bridge to treat an EMF model
as an Epsilon one, but the reverse does not exist.  It doesn't seem complicated
though.

Epsilon also has an API that's more amenable to optimization (e.g., redefining
abstract operations on EOL).

- Q for York: Why no Epsilon -> EMF?

* [2019-03-20 mer.]
** Trying out another Epsilon model                        :epsilon:emfviews:
Wanted to try Simulink.  It's part of Matlab.  Matlab is proprietary and
academic licenses are for students, not for researchers.  I may be able to grab
one from the school.

Still, it's easier to test with another model.  They have a bibtex driver.
Let's use that.

Loading a Bibtex model works.  The API is strange though, since it accepts a
Bibtex file as a string, while the CSV model wanted a BuffereadReader...

Anyway, the property getter works.  Strangely though, I cannot access the type
of a bibtex entry (article, book, etc.).  The type property is parsed, but
seemingly only used for allOfKind.

So the only thing that changes between Bibtex and CSV is how we load the model.
Getting/setting properties is the same (via property getters).  Good sign.

I don't think there is any good reason that these models do not have a metamodel
defined somewhere.  For CSV I understand that it's loose, but EMF has solved
this problem with feature maps.  For Bibtex, if I want to know which fields are
supported and which are not, I have to look into the source code.  This will
also make adding virtual links more tricky.

* [2019-03-21 jeu.]
** Virtual links between Epsilon models                    :epsilon:emfviews:
Right now ViewResource.loadWeavingModel will execute an ECL matching file
on EMF resources.  It needs to turn these EMF resources into IModel.  I can use
InMemoryEmfModel, sure, but if these resources are actually holding Epsilon
models... that's a roundabout way of doing it.

So we need to load models not as EMF resources, but as something else, which may
be turned into an EMF resource or into an Epsilon model.  One problem what that
is you may not be able to open the same resource as an EMF model /and/ as an
Epsilon model at the same time.  We need some kind of linearity.  Not sure Java
can enforce that.

Hmm maybe the 'Either' type can dispose of one resource when we ask for
the other one.  Like:

#+BEGIN_SRC java
class Either {
  A myA;
  B myB;

  A asA() { if(myB) { myB.dispose() }; myA = new A(); return myA; }
  B asB() { if(myA) { myA.dispose() }; myB = new B(); return myB; }
}
#+END_SRC

Actually, I don't think that's a good solution.  Because even if we cache stuff,
we are re-opening resources.  That may invalidate references we held to object.
Bad stuff.

Instead, what we just need is that if we are dealing with an Epsilon model
wrapped as an EMF resource, then we need to be able to access the underlying
Epsilon model.  No reloading.

Hmm, when browsing through results of an ECL match, I need to map these results
back to the model they come from.  It seems Epsilon has nothing built-in for
that.  I can find out the signature for the left and right parameter of the
matching rule, but that only gives me the alias for the model.  It won't work if
I have multiple models from the same metamodel.

Ah!  Spoke too soon.  Found ModelRepository.getOwningModel, which unfortunately,
does what I had in mind: look exhaustively in the models and check if the object
belongs to it.  For the CSV model it means do an O(n) lookup of the rows.  For
EMF models, do an O(1) test against the EObject resource.

So it may be a good idea to wrap /all/ models given to ECL in EpsilonResource,
if performance of the O(n) lookup is an issue when building the weaving model.

Having no metamodel at all is definitely a hindrance to the way EMF Views works
right now.  Trying to work around it, but it's not going to be pretty.  The
problem is that virtual associations used to be defined at the viewpoint level.
Now they are defined by the view?  Or maybe in View.build I can define them on
the viewpoint... but I still wouldn't have any kind of metamodel to extend for
Epsilon models.

* [2019-03-22 ven.]
** Virtual links between Epsilon models (contd.)           :epsilon:emfviews:
The problem I'm currently facing is that when executing an ECL file on Epsilon
models and EMF resources, I can get plain Object or EObject.  I need EObject in
the end, since in order to be virtualized, I need an underlying EObject.

So I'm wrapping the match results in EpsilonEObject.  Crucially, these
EpsilonEObjects are wrapped in VirtualEObject by the View, which populates
virtual associations.  However, when loading the contents of the input models,
EpsilonResource will wrap the Objects in /other instances of/ EpsilonEObject,
leading to other instances of VirtualEObject, which means the populated
virtual associations are lost.

I tried to execute the ECL matching on EpsilonResource directly (delegating the
instantiation of EpsilonEObject to the resource, with an instance cache to
ensure referential transparency is preserved), but then I run into types not
found error.  Presumably there's something EpsilonResource does not implement
correctly.  It's understandable since it's just a hack.

So another solution is to bypass the EpsilonResource when executing the matching
model and use the underlying Epsilon model.  But then I need to ask the
EpsilonResource to project the match results as EpsilonEObjects (so it can keep
track of its instances), /and/ I need to keep track of the EpsilonResource tied
to an Epsilon model.

Ok, got something working.  I was able to populate a virtual association going
from a CSV model to an Ecore model.  As there is no metamodel, the viewpoint is
of no importance here.  The trick is that when populating the view, we are
accessing a feature, and since we are dealing with a VirtualEObject wrapping an
EpsilonEObject, we bypass the usual eGet mechanism:

#+BEGIN_SRC java
public Object eGet(EStructuralFeature feature) {
  if (feature instanceof EpsilonEStructuralFeature) {
    try {
      return ((EpsilonEObject) concreteEObject).get(feature.getName());
    } catch (EolIllegalPropertyException e) {
      return virtualValues().computeIfAbsent(feature.getName(), name -> ECollections.asEList(new ArrayList<>()));
    } catch (EolRuntimeException e) {
      e.printStackTrace();
      return null;
    }
  } else {
    return super.eGet(feature);
  }
}
#+END_SRC

If that feature exists on the EpsilonEObject, then we get a result.  If it
doesn't, then we assume it's a virtual feature.

It works, but then any eGet call on a virtual object backed by an epsilon object
will work, for any feature you can throw at it.  We may want to restrict that to
the view build time only.

For virtual features, we could go through the usual mechanism of declaring them
in the viewpoint weaving model.  Though the trouble is that we really don't have
that information from the Epsilon models right now...

* [2019-03-25 lun.]
** Using Ecore metamodels for Epsilon models           :epsilon:emfviews:emf:
There is no good reason Epsilon models can not have Ecore packages.  Sure, the
metamodel may not be very useful.  For CSV, the only thing you can say is that
you have a "Row" type, but you don't know how many fields it has.  You could add
a "fields" attribute of type Object, but that's about it.

The point though is to avoid the EpsilonEStructuralFeature hack: all virtual
features should be declared at the viewpoint level.

But it means that when wrapping Epsilon objects, I can map them to an EClass
from a metamodel.  In this case, rows from CsvModel should be mapped to the Row
EClass (from a trivial metamodel I created).  How to know the "type" of that
object?  Well, IModel.getTypeNameOf of course!  Except it's inconsistent:

#+BEGIN_SRC java
public String getTypeNameOf(Object instance) {
  return LinkedHashMap.class.getName();
}

protected Collection<String> getAllTypeNamesOf(Object instance) {
  return Collections.singleton("Row");
}
#+END_SRC

Pretty sure we want the first one to return "Row" as well.  So I have to
workaround it.

I did, and now I can create virtual links to and from an Epsilon model.  I have
to define a metamodel for the Epsilon model, in order to include it in the
weaving model.

I also refactored the Epsilon-specific hack of EpsilonEStructuralFeature to a
more generic "dynamic feature" on VirtualEClass.  If that flag is true, the
VirtualEClass will act as if any feature is valid.  The VirtualEObject can pick
up on that, and delegate to the concrete object to answer for that feature.

It's still a hack, since getEStructuralFeature will return features that do not
exist in getAllStructuralFeatures...  One mitigation is to add dynamic features
to a list when they are created, so getAll can pick from that list...  Another
is to just document this potentially curious behavior.

* [2019-03-27 mer.]
** Running EOL queries on views                            :epsilon:emfviews:
(views containing Epsilon models)

A simple ~VIEW!Row.all.println()~ yields: ~Sequence {}~.  Presumably the
getAllKindOf is not finding the right types.

Ah, it was a matter of providing the EPackage for the viewpoint when
constructing the IModel:

: p = r.getContents().get(0).eClass().getEPackage();
: new InMemoryEmfModel("VIEW", r, p)

Now it works.  I can also navigate virtual links.  Relief!

** Why not EpsilonViews instead?                  :epsilon:emfviews:models19:
Now, is that efficient though?  We are using the default EmfModel
implementation, which does this for the ~all~ operation:

#+BEGIN_SRC java
protected Collection<EObject> getAllOfKindFromModel(String kind) throws EolModelElementTypeNotFoundException {
  final EClass eClass = classForName(kind);
  final List<EObject> allOfKind = new ArrayList<EObject>();

  for (EObject eObject : (Collection<EObject>)allContents()) {
    if (eClass.isInstance(eObject)){
      allOfKind.add(eObject);
    }
  }

  return allOfKind;
}
#+END_SRC

Exhaustive search over the contents of the resource, and an instance check.
NeoEMF has a faster method, which we would like to use even when a NeoEMF
resource is put into a view.  That motivates the EMFViews EMC driver.

Though it /does/ sound like we are creating our own problems and solving them:
we add abstraction, it has benefits but then it gets slow, so we add another
stuff on the pile to make it faster.  We keep the benefits, but with the
downside of added process complexity.  This begs the question: can we get the
benefits without the complexity?

The benefits of the approach are:
1. ability to define views on EMF models and Epsilon models
2. ability to filter elements
3. ability to reuse the view for EMF navigation
4. ability to reuse the view for Epsilon querying

1 you already have with base Epsilon.  The view is not permanent though.

2 you don't have, although you can define an Epsilon transformation that selects
the content to put in the view.

3 you definitely don't have.  Can't navigate Epsilon models using EMF tools.

4 no, again the view is not permanent.

So to get most of the benefits without the costs, we'd need basically the same
thing as EMFViews, but working on top of Epsilon.  EpsilonViews.  Load any
model, but expose them as one IModel.  Can filter, can reuse the view.
Delegating operation calls from queries becomes straightforward.

We still can't use EMF tools on it.  Unless it also exposes an EMF API, or we
have a more generic facade from IModel to Resource.

* [2019-04-03 mer.]
** Taking look at the CTF format                                        :ctf:
[[https://diamon.org/ctf/][The CTF format]] is the one we may end up using for sharing traces in the
project.  Seems like a decent solution.  It's a binary format, the tool is
written in C with Python bindings.

I was given an example trace, and the tool babeltrace is available readily and
can be used to browse CTF traces.  So from this text file:

#+BEGIN_EXAMPLE
1 780000
45 781000
48 782000
49 783000
50 784000
#+END_EXAMPLE

We can produce create a CTF trace, which when viewed with babeltrace yields:

#+BEGIN_EXAMPLE
[01:00:00.000000000] (+?.?????????) 0 response_time: { }, { ID_field = 1, timestamp_field = 780000 }
[01:00:00.000000000] (+0.000000000) 0 response_time: { }, { ID_field = 45, timestamp_field = 781000 }
[01:00:00.000000000] (+0.000000000) 0 response_time: { }, { ID_field = 48, timestamp_field = 782000 }
[01:00:00.000000000] (+0.000000000) 0 response_time: { }, { ID_field = 49, timestamp_field = 783000 }
[01:00:00.000000000] (+0.000000000) 0 response_time: { }, { ID_field = 50, timestamp_field = 784000 }
#+END_EXAMPLE

The CTF format lets you define basically any kind of fields, but you always have
/events/, which happen at given clock times.  So it looks to me like this
particular trace was not constructed to take advantage of that, as we see a
timestamp field which increases, but all events time are zero.  Rather, the
timestamp should be used to drive the clock.

Anyway, what I want to know is how easy it would be to manipulate these traces
in Java, and integrate them with EMF/EMF Views.  My first idea was to use an
Epsilon driver for these.  Since I have a build of EMF Views that loads Epsilon
models... it means we can expose these traces as EMF models.  Tada.  (It might
be more efficient to have a more straightforward EMF wrapper for Epsilon
objects... but I digress.)

There is [[https://www.eclipse.org/tracecompass/download.html][a Java tool]] for browsing CTF traces, and it's an Eclipse project to
boot.  So there should be a package inside that does what I want.

: org.eclipse.tracecompass.ctf.parser

Bingo.

After toying around with the API, I found a way to extract the same info:

#+BEGIN_SRC java
public static void main(String[] args) throws CTFException {
  CTFTrace trace = new CTFTrace("tmpnl2pnllx");
  for (ICTFStream s : trace.getStreams()) {
    for (CTFStreamInput e : s.getStreamInputs()) {
      CTFStreamInputReader r = new CTFStreamInputReader(e);
      while (r.readNextEvent() != CTFResponse.ERROR) {
        IEventDefinition d = r.getCurrentEvent();
        if (d != null) {
          System.out.printf("%s: %s\n", d.getTimestamp(), d.getFields());
        }
      }
    }
  }
}
#+END_SRC

Stream events are /not/ loaded in memory directly, because they can be quite
large.  So that's interesting.

For interfacing with EMF, we could create an Ecore metamodel based on the CTF
metadata.  Or use Epsilon (but no metamodels).

* [2019-04-10 mer.]
** Testing EMF Views EMC delegation with NeoEMF models           :neoemf:emc:
I got the EMF Views EMC working for Epsilon models, now I want to test with
NeoEMF models.

Trying on the example we already have of a Java package in a NeoEMF resource,
and a Log model in XMI.  I had forgotten that I hacked around to not use the
~//alias~ syntax in ECL files when building the weaving model... so now the
matching model works.

However, I have weird results when trying the query ~VIEW!Event.all~.  First,
sometimes it works, sometimes it doesn't.  The culprit is:

#+BEGIN_SRC java
  public Collection<EObject> getAllOfKind(String kind) {
    for (Entry<Resource, IModel> e : models.entrySet()) {
      if (e.getValue().hasType(kind)) {
#+END_SRC

The order of ~entrySet()~ is not deterministic.  It shouldn't matter though,
unless hasType() returns true for both metamodels... which it does!  So
if we pick the log model first, it outputs all events.  If we pick up the Java
model, it tries to enumerate all events... and there are none.

Except that the Java metamodel does not have any Event class!  So what's going
on here?

AbstractEmfModel will respond to hasType by searching the type in the package
registry of the model... or the global package registry!  So if the Log
metamodel is in the global registry, the first model asked about it will say
yes, even though it doesn't have instances of that type.

Doesn't make sense to me.

There are two way to fix it: either provide a resource set to the Epsilon model,
or rewrite hasType to use the EPackages.  I'll try the first one since the
second way is basically duplicating the Epsilon effort.

Hmm okay so apparently ViewResource is already setting all metamodels on the
resource set when loading resources.  So that's probably not going to work.
Plan B then.

Hmm ah wait I remember why I delegated hasType in EMFViewsModel... the CSV EMC
for instance has the hardcoded type "Row".  There is no corresponding
metamodel.  Can we restrict the package registry used by the Epsilon models
after the have been loaded maybe?

Aahh.  It's NeoEMFModel.getPackageRegistry which is returning the global
registry.  Crap.  Have to change that.

Okay in the end we have to know which models belong to which metamodel at the
viewpoint/view level.  Adding aliases to eviewpoint and reusing them in eview.
Need to change more things now...

It works!  I broke the Viewpoint API and the tests... but at least what I was
trying to do works.  Following the virtual reference from Event to NeoEMF works.

Now it would be great if we could run arbitrary EOL queries on views from the
command line.  That would make benchmarking and testing a lot easier.

* [2019-04-12 ven.]
** Outlining CTF in EMF solutions                                   :ctf:emf:
There is already an Eclipse project for visualizing CTF traces, called
[[https://www.eclipse.org/tracecompass/download.html][TraceCompass]].  This library comes with a parser for CTF files we can use in Java
programs.

Here is some Java code that goes through all the events of the trace you gave
us:

#+BEGIN_SRC java
public static void main(String[] args) throws CTFException {
  CTFTrace trace = new CTFTrace("tmpnl2pnllx");
  for (ICTFStream s : trace.getStreams()) {
    for (CTFStreamInput e : s.getStreamInputs()) {
      CTFStreamInputReader r = new CTFStreamInputReader(e);
      while (r.readNextEvent() != CTFResponse.ERROR) {
        IEventDefinition d = r.getCurrentEvent();
        if (d != null) {
          System.out.printf("%s: %s\n", d.getTimestamp(), d.getFields());
        }
      }
    }
  }
}
#+END_SRC

So loading CTF traces in Java is already solved.  The remaining problem is to
interface these traces with EMF.  There are two characteristics of CTF which are
relevant here:

1. CTF traces can be large, even quite large.  For this reason, the Java library
   lets you browse traces only by streaming: the metadata is loaded in RAM, but
   the events are streamed as they are requested.  This entails we cannot fully
   load arbitrary traces in RAM.

2. The actual data content of the events may vary from trace to trace.  The
   content format is fully specified by the trace metadata.  This means we
   cannot precisely capture the structure of all traces in a single Ecore model.
   We would need either one Ecore model /per trace/, and this model may be
   conforming to a more generic CTF Ecore model.  Or we could use the generic
   CTF Ecore model directly to stand as metamodel for the traces.

   An example to illustrate: if our trace collects ~sensor_xyz~ events that each
   have an ~id~ and a ~value~ field, a specific Ecore model for this trace would
   be:

   #+BEGIN_EXAMPLE
         1    *            1     *
   trace -----> sensor_xyz ------> id : int
                        |
                        |  1     *
                        +--------> value : int
   #+END_EXAMPLE

   But a more generic one would instead speak of streams, events and fields:

   #+BEGIN_EXAMPLE
         1   *        1    *       1     *       1     *
   trace ----> stream -----> event ------> field ------> name : String
                                              |
                                              |  1     *
                                              +--------> value : Object
   #+END_EXAMPLE

   Working with the former model would be more straightforward, but then it
   would not work for all CTF traces.  Working with the latter allows you to
   handle any trace, at the cost of being less direct.

   We could make the former model to conform to the latter, as a sort of hybrid
   approach.  But the generic model should allow us to work with any CTF trace
   via EMF.


Implementation-wise, I see X potential solutions:

1. A straightforward transformation that takes a CTF trace and outputs an EMF
   model, conforming to a generic CTF Ecore model.  This would be an offline
   transformation, which rules out live CTF traces.

   For handling large traces, the transformation should not use the standard XMI
   serialization but rather dump the trace in a database (NeoEMF could handle
   that).

2. An Eclipse plugin that implements the EMF Resource interface with a
   CTFResource class.  The CTFResource class would use the TraceCompass CTF
   library to query the CTF file directly, while keeping care of not loading the
   trace in memory via lazy loading.

   This solution could possibly work with live traces, although I'm not certain
   EMF resources are meant to dynamically change in size.  As long as the
   metadata is fixed for a given resource, I think it could work.

   The advantage of this solution is that the transformation happens at runtime,
   without requiring an extra step in the pipeline.  The specifics of the lazy
   loading mechanism would need to be fleshed out further however.


In conclusion, the CTF seems like a sensible choice for operating between WP3
and WP4.  Although I didn't find any existing solutions to interface CTF traces
with EMF, I don't see any technical hurdles that would make this unfeasible.
There would be some work required for implementing both solutions I've outlined
above, but this is probably something that can be achieved before the end of the
project.

* [2019-04-24 mer.]
** Describing the EpsilonResource hack                     :epsilon:emfviews:
This IModel to EMF Resource adapter is a hack made to work with EMF Views
specifically.  When we load a view on a CSV file, we load it through an
EpsilonResource, which loads an Epsilon CsvModel under the hood.  When we need
to call ECL (when building the view) or EOL (when querying), EMF Views will
delegate directly to the underlying IModel.  However, when we need to treat the
IModel as an EMF Resource containing instances of EObject (for instances when
navigating the view using EMF-based tools), that's where things get hairy.

To masquerade an IModel as an EMF Resource, we start with the [[https://github.com/atlanmod/emfviews/blob/load-epsilon-models/plugins/org.atlanmod.emfviews/src/org/atlanmod/emfviews/core/EpsilonResource.java][EpsilonResource]]
class.  This overrides two relevant methods of ResourceImpl:

1. void doLoad(InputStream in)
2. EList<EObject> getContents()

In doLoad, we want to load the contents of the input stream and turn them into
EObject for getContents.  Here, we are assuming that the input stream points to
something Epsilon can load.  For now, we are using the filename extension to
know which Epsilon driver to call (".csv" uses CsvModel, ".bib" uses
BibtexModel).  Once we know which Epsilon model to use, we simply load it.

In getContents we need to produce EObjects, but an IModel.allContents holds
instances of Object.  We use the [[https://github.com/atlanmod/emfviews/blob/load-epsilon-models/plugins/org.atlanmod.emfviews/src/org/atlanmod/emfviews/core/EpsilonEObject.java][EpsilonEObject]] class to obtain the interface we
need.  Note that the mapping from Object to EpsilonEObject is cached, not for
performance reasons but because we want referential equality to hold (whenever
we getContents on the same resource, we should get the same EpsilonEObject
instances, not new ones for each call).

The EpsilonEObject is a barebones implementation of the EObject interface.  The
two interesting methods are:

1. EClass eClass()
2. Object eGet(EStructuralFeature feature)

For eGet, we delegate to the Epsilon property getter of the underlying Object
managed by the IModel.  The EObject requires an EStructuralFeature though,
whereas the property getter only needs a name.  Now, to be conforming to an
Ecore model, the structural feature should be part of an EClass, which should be
part of an EPackage: the metamodel of the model we are considering.  It turns
out that since we only need the name string out of the feature, we can use a
trivial implementation of EStructuralFeature which only responds to the getName
method.  That is the [[https://github.com/atlanmod/emfviews/blob/load-epsilon-models/plugins/org.atlanmod.emfviews/src/org/atlanmod/emfviews/core/DynamicStructuralFeature.java][DynamicStructuralFeature]] class.

For eClass, either we provide one at construction time, or we use the singleton
EpsilonEClass.  I don't think that singleton is used anymore.  What it /should/
do is give back instances of DynamicStructuralFeature for
getEStructuralFeature(String).  So it's as if any feature exists on the EClass,
but then when that feature is used to eGet in an EpsilonEObject, the property
getter might complain if it can't be found.

Now, unfortunately this dynamic approach is not enough when adding virtual
features in EMF Views: the viewpoint declaration needs actual Ecore models.  For
the CSV model, I created a trivial Ecore model with one EClass: Row, so the
EpsilonEClass is not used.

When navigating a CSV model through a view, we don't get an EpsilonEObject
directly, but a VirtualEObject (wrapping an EpsilonEObject).  This
VirtualEObject conforms to a VirtualEClass, but one that will emit
DynamicStructuralFeature instances (this is done [[https://github.com/atlanmod/emfviews/blob/load-epsilon-models/plugins/org.atlanmod.emfviews/src/org/atlanmod/emfviews/core/View.java#L225][here]]).

Currently the EpsilonResource adapter is tied to the way EMF Views make use of
it.  If we wanted to extract it as a standalone thing, we would move the logic
EpsilonEClass.  However, I'm not sure how far we can go when implementing
EObject and EClass methods.

* [2019-05-03 ven.]
** Using EOL to populate views                                 :eol:emfviews:
Coming back to that point.  The OCL experiment was nice, but in truth EOL seemed
more suited for the task.  As much as I'd like to toy with JIT'ing OCL into
straight EMF API calls in Java, the performance problem I had with the guards in
OCL just went away when I used an imperative program written in EOL to get the
traces.

Since we already use ECL to populate weaving models, this is just a more generic
variant.  Right now it's only printing the results, but I could put the results
into a weaving model as well.  I guess I would need to include the
VirtualLinks.ecore as metamodel at least.  Then the open question is: how do I
get the results back programmatically?

* [2019-05-17 ven.]
** Streamlining update site installation                   :eclipse:emfviews:
Still no dice about querying the Epsilon update site when installing.  So I
split out the Epsilon delegate in a separate feature, so at least people can
blindly click 'next' and install EMF Views.  They just won't have ECL working
out of the box unless they install Epsilon first.

* [2019-05-22 mer.]
** Megamart Prague hackathon                                       :megamart:
*** IKER challenge 1
IoT ecosystem.  Want to improve log structure and discover links between logs
and UML state machine.

*** IKER challenge 2
Automate the design of asynchronous architectures using models.
Generate DSLs to assist developer in creating messages.

*** CSY challenge 1
Log interpretation.  Follow-up of our collaboration.
Want to focus on the missing piece: automatic analysis of logs to identify
fallback situations.

*** CSY challenge 2
Goal: model the overall system to simulate it faster than realtime.

*** Volvo challenge
Needs variability design models for their engines.

*** BT challenge 1
Convert actual train logs to CTF.
Detect log anomalies and trace them back to requirements.
More log analysis?

*** BT challenge 2
Improve testing feedback with models (continuation of previous challenge).

* [2019-05-23 jeu.]
** Fixing the traceability demo                                    :emfviews:
Damned, the traceability demo doesn't work anymore for some reason.  It mostly
works, but the references between Java code and UML component models are not
populated.  Strange thing though is that running the report template finds these
links without issues...

I bisected and found the culprit was switching to InMemoryEmfModel.  Actually,
it was an interplay of InMemoryEmfModel and the way View gathered all
contributing models in the same resource set.  Doing this, EmfModel.classForName
from Epsilon will look for the class in all packages from the resource set, and
will stop at the first matching one.  UML and Java both have a Package class, so
it returns the UML one and so no matching happens.

That does not explain why the templating was working correctly though.

Oh, and the fix.  Well, we need to have a separate resource set for each
resource to accomodate the way InMemoryEmfModel works.  Which means we need a
mapping from model to metamodels.  I reused the change of putting aliases in
front of metamodels and models (which breaks all examples).  But it's cleaner
that way.

Hmm there might be another fix, which is to specify the absolute path in the ECL
file for ambiguous classifiers.  Certainly a simpler fix...  will see.

* [2019-06-03 lun.]
** Fixing the traceability demo                                    :emfviews:
So actually specifying the absolute path in the ECL file is not enough.  It
would fix the ambiguousity in the traceability demo, but loading all metamodels
in the resource set was not a good idea in the first place.  Besides, ECL was
not warning us that there was indeed an ambiguous name.  Even the Ecore
diagnostician would have let this slide.

In the end, I'm not happy about the specific syntax used to communicate the
metamodel-model association.  It does not account for multiple metamodels for
one model, for instance.  But we can always extend the syntax later.  I'm
thinking something like TOML would be palatable.

* [2019-06-14 ven.]
** Updating the benchmarks for EOL                         :eol:ocl:emfviews:
AAAArgh.  Everything is terrible.  The benchmarks code was a dirty pile of
hacks, and now we have to add hacks on top of it.

The main problem is that I ditched the weaving model loading in the
load-epsilon-models branch because we couldn't locate matching objects in the
resource.  Instead, I used more straightforward VirtualLinkMatch objects which
are POJOs.  But the benchmarks relied on weaving models created offline for perf
reasons...

There are guards and preconditions to ECL, so we may be able to squeeze more
speed out of it later down the road.  Or if that matters we can just ignore the
whole matching phase when reporting times for queries.

So I had to change the benchs to load the views using an ECL file instead.
That's where things get fun: using XMI models everything is fine, but using CDO
and NeoEMF models not so much.  I can run the OCL queries, but they return
invalid results.  Some of the virtual association are not populated, and this
stems from ECL not using the right EClass when trying to locate instances of
match candidates, because it looks into a package registry that contains too
many EPackage with the same name!

Tried to disambiguate java and java-cdo, no dice.

Okay, got it.  CDOResource returns a package registry that contains every
package loaded.  Since AbstractEmfModel.classForName stops at the first result,
it picks the one from the plain java package, not the java-cdo one.  Grrrr.

Maybe I can rename the package?  But best thing would be to clean up the
resourceset of the CDO resource.

Ok cleaning up the resourceset package registry worked.  Yet another hack to the
pile.  But I win.

* [2019-06-19 mer.]
** Custom ECL matching, again                                  :ecl:emfviews:
The benchmarks are running again, but loading the view was agonizingly slow.  Up
to 3 minutes for a NeoEMF trace with 1000 elements.  That's due to the slow
O(n^2) matching algorithm used by MatchRule in Epsilon.

Last year, I wrote a custom Java class that executed the 'javaClass' rule from
the ECL directly using Java and using a HashMap, for an O(n) algo.  This time
though, I got rid of WeavingModel and used VirtualLinkMatch instead.  The key
difference is that a weaving model file pointed to elements in a resource using
strings, whereas the VirtualLinkMatch class points to object references in
memory.

I think there was also an issue of obtaining URI for objects in Epsilon
resources that prevented the use of the offline weaving model.

Anyway.

I converted the code I had for the custom weaving model to do the same thing for
virtual link matches.  Now the NeoEMF trace with 1000 elements loads in 8
seconds, and the XMI one in 1 second.  The algo is linear, and quadratic for the
latest rule but these are tiny models so it doesn't matter.  It should hold for
the 10^6 models, but we'll see.

* [2019-07-03 mer.]
** Looking at a full benchmark run                         :emfviews:epsilon:
Numbers are quite good.  Better than last year.  I'm not sure why there is such
a disparity: 19sec instead of 82sec to iterate through the NeoEMF view, while at
the same time, the XMI view jumps from 2sec to 6sec.

Other weird stuff: the reqToTraces query is ~15ms with EOL but takes 20sec with
OCL.  It should actually be fast since we are just following links, but somehow
OCL struggles with it.

The OCL optimisation is drastic.  EOL, not so much.  I'm guessing that the EMF
Views EMC is breaking the pipeline optimisation by taking the elements out of
the pipeline and into a list.  We can check that by looking at the numbers for a
manual call to getAllOfKind.size.

Can we get around that?  For the allInstances query, we should be able to keep
the pipeline intact all along.

Trying to get a feel for the lower bound by loading the neoemf trace directly
with NeoEMFModel and counting its size, but running into issues.  Need to
recreate the model?

* [2019-07-04 jeu.]
** Speeding up EOL optimisations                    :emfviews:epsilon:neoemf:
Recreating the NeoEMF trace had no effect.  Still can't find the Log class.

To get a lower bound, I just loaded the trace as a PersistentResource, and
called getAllInstances.size on it.  Took 19s.  That's amazingly slow, since the
FastExtentMap does exactly the same thing, and manages to do it in 2s.  It's not
on the same machine but...

For EOL we also reach 2s through the EMF Views EMC with the Gremlin pipeline.

I'm going to give up on optimising further for now, and put the numbers we have
in the paper.  We'll see about going faster after that.

* [2019-07-08 lun.]
** Trying out the beefy LS2N machine                           :ls2n:ssh:vpn:
Heard about a nice DELL 4x20 cores with 1.5To of RAM hosted at the LS2N we can
use.

Had an account created.  Have to use VPN to access it.  There's a nasty and lame
proprietary client.  Luckily, it should also work with openconnect.

The only trick is to pass the juniper flag:

: sudo openconnect --juniper HOSTNAME

It actually failed me with "cannot create tun device".  Figured it was because I
updated the kernel before rebooting.  After reboot, it works fine.

After that, SSH into the machine is simple.  The machine is shared, so there are
long-running processes launched by other LS2N members.

Tried to run some C++ program I had lying around, but of course I need to
compile it statically since the machine does not have an up to date libc (I have
2.27).  GCC on the machine is also ancient (6, I have 9), and clang is absent.

* [2019-07-11 jeu.]
** Lazy loading weaving models?                                    :emfviews:
It's unnecessary to populate the virtual links when the view is created.  We
could defer that work to when the target features are accessed and save on
loading time.

I tried that.  It's not that easy, unfortunately, because of opposite features.
So I measure what we would gain by skipping the View.build phase:

| view   | load time | lazy load time |
|--------+-----------+----------------|
| XMI    |        23 |              5 |
| NeoEMF |       132 |            115 |

(time in seconds)

Not a lot.  The View.build phase is only O(n) after all.  We might have better
gains by completely deferring the full ECL matching instead of just the
populating phase.  In theory we would pay a very small constant overhead, until
the feature is accessed.  The downside is that we could end up paying a higher
total price when iterating over the full view.

In the benchmarks for the SoSym paper, we already have good query numbers, and
these would be negatively impacted by lazy loading.  Maybe not a priority,
especially given the added complexity.

* [2019-07-12 ven.]
** Speeding up EOL connectors                       :emfviews:epsilon:neoemf:
This time, loading the trace model using the NeoEMF EMC worked.  I tried a
simple ~Log.all.size().println()~ query, and it took 29s.

Funny thing... if I call PersistentResource.getAllInstances before executing the
query, the getAllInstances takes 20s, and the query now only takes 1.5s!  Even
with warmups, this result is consistent.

So there must be some caching mechanism, either in PersistentResource, or in the
DB itself.  Now, why do we get 2s for the same query on a view though?

If I try to load the trace view (that contains only the trace model, no virtual
links) through the EMF Views EMC, I get 22s to call View.getAllInstances, and
1.5s to run the EOL query.  However, I also have 93s (!) in:

: EMFViewsModel m = new EMFViewsModel(view);
: m.setName("VIEW");
: m.load();

If I skip the View.getAllInstances, then it's 101s to load and 1.8s to execute
the query.  It seems our good numbers for the queries are due to some heavy
caching done up front when loading the resource.  The NeoEMF EMC does none of
this caching, so it must happen in InMemoryEmfModel...

So the culprit is the constructor, which calls InMemoryEmfModel.init!
Specifically, the init method adds a CachedContentsAdapter, which crawls the
resource to cache everything in RAM... and it's not togglable.  Since
InMemoryEmfModel does really nothing more than EmfModel, let's inherit from the
latter instead.

Now I get 21s to execute the query, and no overhead in loading.  Good news: now
at least I understand what is going on.  Bad news: we need to re-run the EOL
queries to get accurate numbers.

It doesn't explain why the FastExtentsMap can reach 2s for the same query
though...

Calling PersistentResource.getAllInstances twice gives 20s for the first call,
then 2s.  But it doesn't look like the OCL query bench has hidden 20s away, as
everything is accounted for.

So, calling benchQuery on the trace view with ~Log.allInstances()->size()~
takes... 20s!  With the fast extents map!  This is around what I expect of
course, but it doesn't explain the numbers we have in the bench.  It could be
that the bench machine is that fast...  Without fast extents map, I get 94s.  On
the bench machine we had 20s.  The speedup is still more impressive on the bench
machine.

If I force the EMF EMC for the EOL query, I get 70s instead of 21s.  So at least
the optimization is visible.  I'm still stumped by the bench machine results,
but so far everything is coherent on my side now.

Now to check why the traceToReq is so slow.

Ha!  One thing is that matching the model to build the weaving model calls the
getAllInstances(Log), so the 20s cost of that is hidden away in the loading
model phase.  That's why we can get 2s results after.  Hmm.

* [2019-07-15 lun.]
** Speeding up EOL connectors, contd.               :emfviews:epsilon:neoemf:
So loading the NeoEMF trace resource using the noCache option does make a
difference when calling getAllInstances.  Instead of 800ms the second time, we
get 3000ms.  However, we are /still/ much faster after the first getAllInstances
call.

If no caching is happening in PersistentResource, maybe it's due to the JIT?

Anyway, it's not that important, since we will be eating that cost when matching
models in the view.  And it means we don't need to re-run all benchmarks for
EOL.

I've tracked the problem with traceToReq: the ~Log.all.select().first()~ is
fetching the whole Gremlin pipeline, only to discard everything and keep one
element.  I've added a getFirst option to the Gremlin collection wrapper, which
greatly speeds things up.

1.5s for NeoEMF, 0.5s for XMI, instead of 15s/10s on this machine.

Hmm actually, with the default selectOne operation implementation, we get the
same numbers.  So just the select().first() to selectOne change is enough.  Good
news!  It means we don't even have to special case anything about the Gremlin
pipeline in the EMF Views EMC.

* [2019-07-23 mar.]
** Freezing the old dokuwiki                                   :atlanmod:php:
And now I'm realizing that if we don't want to break URLs, we need to keep at
least a simple index.php file that redirects to static versions of the existing
pages.  Damn.

And I need intranet access to meddle with the sources.  Double damn.

** Extending VPDL to include an ECL-like grammar                   :vpdl:ecl:
Long overdue.  Doesn't have to be full-ECL, we can just define a convenient
subset.  The upside is that we can then output ECL, or we could generate
something else down the line.

Have to wrestle a bit with Xtext grammars again though.

Ok, done.  Updated the example and it's still work!

We'll probably need to extend that grammar again in the future, but it's a good
start.

I've updated the manual as well.  For future reference: I've used [[https://crates.io/crates/railroad][railroad]] to
generate the grammar diagrams.

* [2019-07-25 jeu.]
** Implementing filters at the model level                         :emfviews:
So far we cannot hide individual model elements in a view, even though the
weaving model could support it, and I remember the original EMF Views version
did.

It's tricky though, because we cannot actually delete any elements from the
contributing models.  And, even though we can easily not add a filtered element
to the root contents list, we still need to make sure the element does not pop
up in other places, like as a member of a multi-valued reference.

Adding a "isHidden" flag to VirtualEObject is enough to register this info.

For single-valued reference, it's not too much work to check the flag and act
accordingly (the feature is unset, the object is null...).

For many-valued reference however, it comes at a high price.  We return a
VirtualEList, which must now account for potential hidden elements.  For
List.size, we had:

: return concreteList.size();

but now we don't know the virtual list size until we know how many elements are
hidden in it.  I'm working off the assumption that isHidden can be changed at
any point, so we cannot pre-compute that info and look it up.

So now to be correct we must do:

#+BEGIN_SRC java
int size = 0;
for (EObject o : concreteList)
  if (!getVirtual(o).isHidden)
    ++size;
return size;
#+END_SRC

which turns the size method into an O(n) operation.  Same thing for List.get.
And that price is paid for /all lists/, even if the view doesn't effectively
have a single hidden element!

Is there a way to make this correct yet not pay that price?

If the reference knows which elements are hidden by index, size could stay O(1):

#+BEGIN_SRC java
List<int> hiddenObjects;

int size() {
  return concreteList.size() - hiddenObjects.size();
}
#+END_SRC

But it doesn't help much with get, since we would still need to iterate over the
initial contents in order to find the index into the virtual list.

get could be fast if we had list of indices of non-filtered elements instead:

#+BEGIN_SRC java
List<int> visibleObjects;

T get(int idx) {
  return concreteList.get(visibleObjects.get(idx));
}
#+END_SRC

O(1) again, with a small overhead for additional bounds checking and
indirection.

And we could implement straightforwardly:

#+BEGIN_SRC java
int size() {
  return visibleObjects.size();
}
#+END_SRC

But the problem is now how to maintain this visibleObjects information?  We
When toggling the visibility of a virtual object, we can't know all the
references it appears in.

So, I guess a good compromise is to opt-in into this costly behavior.  For
consistency, "isHidden" should do nothing unless the option has been selected on
the view.

So here's how I did it: whenever an object is hidden, it notifies
its view so we now have to use a FilteredVirtualEList that does the right thing.
It's not perfect: if you retrieve a list before filtering, then you got a plain
VirtualEList which still contains the object.  Basically, it's an issue of
iterator invalidation.  But ultimately that's due to how EMF returns whole lists
for features.

* [2019-07-26 ven.]
** Simplifying filtering in viewpoint                              :emfviews:
Going over the leftover task tags.  This asymmetry between whitelist/blacklist
behavior in the code is bugging me.  Besides, blacklisting has it easy: we
simply ignore wildcards.

The problem is this: any element can be filtered.  The meaning of filter changes
if the weaving model is blacklisting or whitelisting.  So far, here is how we
did it:

For blacklists, we go over each filtered element, and hide them on their
container.  But we can only hide elements, not mark them visible.  So, for
whitelists, we go over all the elements in the metamodels, and hide those that
are /not/ filtered.

Thus the whitelist situation is slower.

In addition, instead of targeting one element in particular, the filter can
contain a wildcard.  If we target ~uml.Class.*~, then we should filter all
features of ~uml.Class~, but we don't have to spell them out explicitly.

It means that we should first go to the wildcard prefix, and mark all its
contents as filtered.

I think I've got a simpler solution.  Instead of marking the container, just add
a flag to each BaseVirtualElement: filtered.  The meaning of this flag can be
reversed by whitelisting, so just add a method:

#+BEGIN_SRC java
boolean isVisible() { return filtered == isWhitelist }
#+END_SRC

Now, we can easily check if an element is visible or not.  So the
blacklist/whitelist difference does not matter anymore when applying the filters
when building the viewpoint.  Just go over each filter, and toggle the bit.  The
influence of whitelist is deferred to each isVisible call.

For wildcards, find the prefix element (Package or Class), then toggle the bit
for all its contents.

Trying to implement that... most test passes, except two that I'm having a hard
time tracking why.  They fail in locating metamodel objects, but didn't fail
previously, presumably because the previous whitelisting method was enumerating
all contents of the resource.

Will fix that next time.

* [2019-07-29 lun.]
** Upgraded Eclipse to 4.12                                         :eclipse:
As usual: downloaded the platform binary, and reinstalled the required plugins
(JDT, Xtend, Epsilon, OCL) on top of that.  Only 521 plugins in the PDE
environment, which makes Eclipse responsivity bearable.

Used a clean config, but imported my settings from the last version.  Worked
well.  Didn't notice anything missing so far.

** Simplifying filtering                                       :emfviews:emf:
So the bug was that I forgot that whitelisting should also filter (as in,
include) all the parents of the filtered elements.  Otherwise we don't see
anything.

Now it's fixed!

* [2019-07-30 mar.]
** Revamping view building                                         :emfviews:
So far, we have relied on using the weaving model, which was mostly designed
with the viewpoint in mind, to collect information in order to build views.

The main problem is that the choice of vocabulary in the weaving model does not
convey the semantics as clearly when it applies to views than when it applies to
viewpoints.  Also, it's a roundabount way of doing things.

In EclDelegate, after we collect matches between contributing models with ECL,
we have to wrap these matches in ConcreteElement instances.  The overhead is not
very important, but these are EMF objects, fancier than POJOs.  The bad part is
that we are converting the reference to contributing model objects into strings,
which are then fetched back by View.build with Resource.getEObject.

So we have to extract an URI that identifies the element, and then use that URI
for View to get back a memory reference to that object.  A memory reference that
EclDelegate already had.  It's pretty dumb.

We could have used proxies instead, but they are not as expressive for our
purposes.  Using the ConcreteElement.path string lets us use the wildcard
feature for filters for instance.

While it makes sense to continue accepting these weaving models in View.build,
we could also accept something even simpler.  I prototyped a much simpler
version in the ecl branch, that just outputs POJOs for View.build, and it
worked.

On the other hand, we really only needed this for Epsilon... although it would
make things clearer.  For instance, in ECLDelegate, we really want to return
matches between concrete elements, not concepts.  At the view level, we only
have objects, not Ecore objects.

But, a virtual association can only point to concepts.  Even though we really
only need to get a reference to matching objects.

Well, in the end the nomenclature is not that critical.  I've refactored these
parts a bit and added comments to note the inconsistency.  We'll see with H. if
we want to create a separate View Weaving Model, as it could make sense to
simplify the paper presentation as well.  We could argue that it's the same
concept, but minor variations.  Doesn't need to be exactly the same thing.

* [2019-07-31 mer.]
** Testing ModelJoin                                     :modeljoin:emfviews:
Following the [[https://sdqweb.ipd.kit.edu/wiki/ModelJoin/Getting_Started%0A][tutorial]].  Installing QVT and Palladio dependencies first in a
fresh Eclipse modeling.  Curiously, although there are newer releases for
Palladio, they have dependencies issues on a missing equinox.p2.iu (is that a
typo?).  The 2.0.3 release works.

This gets complicated.  The update site for Modeljoin is actually down, so they
provide an update site archive instead.  But this one doesn't seem to work after
following the previous instructions (on another page).  I'll try to follow these
[[https://sdqweb.ipd.kit.edu/wiki/ModelJoin/Installation][new, very specific, instructions]].  If it fails, I can try the development setup.

Ah wait, the SVN seems private.  Browsing the urls actually requires a password.
So hopefully the update site archive works.

It doesn't.  It complains about a missing de.ikv.medini.qvt plugin.  Luckily, by
browsing around, I found [[https://sdqweb.ipd.kit.edu/eclipse/palladiosimulator/releases/1.1.2/][an update site]] that contains it.  Now the install
finishes.

I create the metamodels, the test Xtext project and ... while the test query
parses correctly, it doesn't generate anything.  Oh wait, the src-gen folder was
created, it was just not updated by Eclipse...

So it works as advertised.  Too bad the tutorial stops here!  In src-gen you
get:

- query-target.ecore
- query-target.ocl
- query-target.qvto
- query-trace.ecore

To recap, here is the query:

#+BEGIN_SRC modeljoin
natural join restaurant.Restaurant with reviewpage.ReviewPage as jointarget.RatedRestaurants {
  keep aggregate avg(reviewpage.Review.rating)
    over reviewpage.ReviewPage.reviews as RatedRestaurants.rating

  keep outgoing restaurant.Restaurant.sells
}
#+END_SRC

The query-target.ecore contains a RatedRestaurant class with a new rating
attribute (note that it's a double, probably implied by the use of ~avg~).

We also have the Food class.  Is this what the ~keep outgoing~ does?  Because we
also have the ~RatedRestaurants.name~ attribute, but that one has no ~keep~
clause.

The real meat and potatoes of the approach seems to lie in the OCL and QVTO
files.  These are 114/150 lines long.

OCL seems to be a bunch of invariants on the trace model: ensuring some
properties about the trace of the QVT transformation.  Is it for debugging?
It wasn't included in the tutorial screenshot.

The QVT is the actual transformation.  We can see how the average is computed:

#+BEGIN_SRC qvto
self.rating := sourceObject->reviews->collect(a|a.rating)->sum()
             / sourceObject->reviews->collect(a|a.rating)->size();
#+END_SRC

That attribute is computed for each RatedRestaurant created.

One interesting observation: the RatedRestaurant are matched by name, and this
doesn't seem to be configurable as the query above doesn't specify this part.

#+BEGIN_SRC qvto
mapping _restaurant::Restaurant::naturalJoin_(right : _reviewpage::ReviewPage) : _jointarget::RatedRestaurants
when { self.name=rightElement.name }
#+END_SRC

Now I would like to test the actual transformation to obtain a view.
Unfortunately, the QVT transformation is not happy because it can't find the
metamodels that have been generated by modeljoin:

#+BEGIN_SRC qvto
// target metamodel
modeltype _jointarget uses "http://mjtest/test";
// trace metamodel
modeltype _mjtrace uses "http://mjtest/test.trace";
#+END_SRC

I should Install Epsilon to register them.  Okay now it finds them.  But it
doesn't find the source metamodels:

: var leftAllRatedRestaurants = __restaurant.objectsOfType(_restaurant::Restaurant);

~__restaurant~ is unknown, and indeed not declared in the QVT transformation.
There doesn't seem to be a way to provide them in the QVT launch configuration
either, so let's edit the file directly.

That got rid of most of the errors:

#+BEGIN_SRC diff
  transformation M1toM2(
+ in __restaurant: _restaurant,
+ in __reviewpage: _reviewpage,
#+END_SRC

#+BEGIN_SRC diff
  // source metamodels
+ modeltype _restaurant uses "http://restaurant.com";
+ modeltype _reviewpage uses "http://reviewpage.com";
#+END_SRC

Now, QVT doesn't like the generated trace metamodel.  Specifically, in mappings
like this:

#+BEGIN_SRC qvto
mapping _jointarget::Food::trace_keepOutgoing_sells(source : _restaurant::Food) : _mjtrace::ref_Food_Food_sells {
  result.source := source;
  result.target := self;
}
#+END_SRC

It complains that ~source~ and ~self~ should be of type null.  If we check the
Metamodel Explorer view, provided by QVT, we see why: it thinks the generated
trace metamodel has null as EType for these references, when in fact they refer
to external metamodels.  I think it's because it fails to find the associated
metamodels, seeing that they are given by platform URIs.  Let's add them to the
workspace?

Yes, that gets rid of the null.  But now I've got a helpful error:

: the type restaurant::Food does not conform to the type restaurant::Food

I'm guessing we are referring to the same metamodel in two different ways, and
QVT is confusing them as two different types.

Well, I can't seem to make QVTo understand these are the same.  We can't pass
something else than the ns URI in the QVTo declaration, and conversely we
cannot pass anything other than the platform URI in the ecore file.

But I think it's okay, since this will just populate the trace, which I don't
really need.

Yes, it works.  After creating the models manually.  Aside: the Ecore editor is
sufficient to create Ecore models, but not EMF models.  You can generate editors
for an EMF model using genmodel, but the editor is not more competent than the
Ecore editor.  You cannot create elements or children!  You can just edit the
attributes of the root element.  What gives?

Anyway, running the QVT transformation produces a target model fine.  Couple of
points though:

1. You have to rerun the transformation when the source models change.  That
   makes it a two-step thing: 1) produce the transformation (which is automatic
   on save due to Xtext generators), and 2) run the QVT transformation.

   Though the paper claims it is executed automatically as part of an Mwe2
   workflow.  Possible, but not documented.

2. The ~keep outgoing Restaurant.sells~ keeps the Food element alright, but they
   have no attribute!  Is there a way to specify that?

Time to dig into the papers to get some answers.

Looks like keep outgoing can accept additional clauses, among which ~keep
attributes~:

#+BEGIN_SRC modeljoin
keep outgoing restaurant.Restaurant.sells {
  keep attributes restaurant.Food.name
}
#+END_SRC

which works as intended.

On the configurability of the matching between join targets: there is the theta
join, which can take an arbitrary OCL expression.  In the Xtext grammar, it's
just a raw OCL string that's pasted in the ~when~ clause of the QVT
transformation.  Rings a bell...

So they have natural join which joins on all similar attributes (same name, same
type), outer join which is just cartesian product, and theta join which accepts
an arbitrary OCL expression.

Can we create new associations like we can in EMF Views?  For the tutorial
Publication/Book example, I think it would work.  It would appear in a new class
though, but it could contain the same chapters.  For the firstChapter view, I
don't think we can express it, as ~keep outgoing~ does not allow you to add a
predicate on which elements to keep in the reference.

On the other hand, ModelJoin has the ability to create calculated attributes and
aggregations (sum, average, min, max, size), which EMF Views does not (yet!).

No word on performance, but the bottleneck is surely the QVT transformation,
which is not their contribution.

* [2019-08-01 jeu.]
** Testing VIATRA                                           :viatra:emfviews:
They have a single update site.  The documentation seems much more complete.

Unfortunately, there's a SHA256 sum mismatch for the 2.2.0 release!  Trying from
the marketplace.  Okay seems to work.  Onto the [[https://www.eclipse.org/viatra/documentation/tutorial.html][tutorial]].

*** Checking out ViewModel from MODELS'18
Wait a minute, I'm not seeing anything view-related in there.  They have a
MODELS'18 paper that talks of a ViewModel approach based on the VIATRA query
language.  The repo is [[https://github.com/ftsrg/viewmodel][here]].

There are benchmarks, but less documentation.  Update site works, but the SDK
fails to install.  The runtime installs though.  The SDK contains editors and
generators for the transformation language, so I should be able to test the
thing still.

There is a simple example [[https://github.com/FTSRG/viewmodel/blob/master/benchmarks/plugins/hu.bme.mit.inf.viewmodel.benchmarks.viewmodel/src/hu/bme/mit/inf/viewmodel/benchmarks/viewmodel/virtualswitchview/RailwayModel2VirtualSwitchModel.viewmodel][here]], but I'm not sure there is much tooling around
it.  There are benchmarks though, so there should be code that creates views and
stress-tests them somehow.

Hmm without the SDK nothing gets generated from the viewmodel file, so I can't
seem to run anything.

There's still the development setup!

Okay so I can clone and import the projects in Eclipse... but I'm seeing red all
around.  Looks like I need to downgrade VIATRA, since the project mentions 1.7.2
and I'm on 2.2.0, and there are clearly files that ViewModel expects that are
not getting generated anymore.

That was it.  After installing 1.7.2 and regenerating the Xtext artifacts for
viewmodel.language, I get no errors.

Looking through the benchmarks code... most of the viewmodel thing is driven by
the Java API.  It does seem to produce an XMI, but I seem to execute the
benchmarks.

Maybe I can try a smaller example?

Hm so I still can't make head or tails of this.  But, as far as I can gather
from the paper and code, the models created by ViewModel exist in memory,
rebuild incrementally to source changes, and can be serialized as XMI.  You
should be able to use standard tooling on the output XMI.  It's unclear what the
metamodel for the target is.

Concerning the user process: the README mentions it starts from a ~viewmodel~
file.  Here is one:

#+BEGIN_SRC viewmodel
import hu.bme.mit.inf.viewmodel.benchmarks.queries.virtualswitchview.*

rule virtualSwitches(_) => createVirtualSwitch(_);

rule connectedTo(Left, Right) => connectVirtualSwitches(VirtualLeft, VirtualRight) {
	lookup virtualSwitches(Left) => (VirtualLeft);
	lookup virtualSwitches(Right) => (VirtualRight);
}

rule countConnectedInFailure(Switch, N) => setConnectedSwitchesInFailureCount(VirtualSwitch, N) {
	lookup virtualSwitches(Switch) => (VirtualSwitch);
}
#+END_SRC

It doesn't seem to say much.  Notice it refers to
~benchmarks.queries.virtualswitchview~.  So that must exist beforehand.  These
are vql queries.  Here is one that is relevant:

#+BEGIN_SRC vql
import "platform:/plugin/hu.bme.mit.inf.viewmodel.benchmarks.models/model/virtualswitchview.ecore"

@Template
pattern createVirtualSwitch(Switch : VirtualSwitch) {
	VirtualSwitch(Switch);
}

@Template
pattern connectVirtualSwitches(Left : VirtualSwitch, Right : VirtualSwitch) {
	VirtualSwitch.connectedTo(Left, Right);
}

@Template
pattern setConnectedSwitchesInFailureCount(Switch : VirtualSwitch, N : java Integer) {
	VirtualSwitch.countConnectedInFailure(Switch, N);
}
#+END_SRC

Again, it doesn't look there's anything interesting going on here.  But there's
a metamodel import, which again implies the view metamodel must exist
beforehand.  Meaning, it isn't derived by the DSL.

In addition, the viewmodel file will generate one Java class that creates a
ViewSpecification, which is really what is used by the ViewModel runtime.  The
VQL query above also generates 47 (!) Java source files.  That's part of VIATRA,
so not really a characteristic of the approach, but still.  47 Java files from 3
patterns that seemingly do nothing.

So there seems to be a great deal of effort to create simple views, and it's
unclear what is the expressivity of the language, or exactly what it does.  The
paper may contain some answers.

*** Reading the ViewModel paper
\sect3.1 is a concise way of describing the Ecore structural constraints.

\sect3.2: The use of 4-valued logic to resolve merges is interesting, but I don't
really understand the use-case of merging arbitrary elements.

Overall, I didn't find the answers I sought.  Figure 7 is an example of view
transformation rules, but I'm still puzzled as to how it can be related to what
views in EMF Views are.

* [2019-08-13 mar.]
** Draft rework of formalization                         :formalism:emfviews:
Okay so here is how it should work:

1. Viewpoint construction from weaving model.
- combine source models packages
- apply filters (blacklist/whitelist)
- add virtual concepts to new package
- add virtual features to new/existing classes

- maintaining structural constraints with filtering

2. View construction from weaving model:
- apply filters (blacklist/whitelist)
- populate virtual features

2b. Using ECL for matching
- apply rules, populate weaving model


# Do we need packages?  Probably not: they don't tell anything interesting

A metamodel is a tuple (P,C,F), where P, C, F are sets of packages, classes, and features respectively.
Each class c is owned exactly by one package, and each feature f is owned by exactly one class c.
owner(f) = c; owner(c) = p

A weaving model is a tuple (Vc,Vf,Fl,WB), where Vc,Vf,Fl are sets of virtual classes, virtual features and filtered objects resp.
WB indicates whether the weaving model is whitelist or blacklist: WB in {w,b}
visible is a predicate on objects that indicates whether an object will be visible in the viewpoint or view.
visible is derived from Fl and WB:
visible(o) = o in Fl      if WB=w
             o not in Fl  otherwise

The key concept of EMF Views is the virtualization.
Each model (and metamodel) object is represented in views and viewpoints by a virtual object, which is a proxy to the source element it represents.
The function virtual() maps source objects to their unique virtual counterpart.
%We use the prefix v to refer to virtual objects. % in blue?

# Can we lump P C F together to only talk about the viewpoint objects?
# For filtering, we don't really care about whether they come from C, F or P, so one definition is enough

A viewpoint is a metamodel (P,C,F) constructed from a set of metamodels (Pi,Ci,Fi)i=1..n and a weaving model (Vc,Vf,visible), where:
# Combine source model packages and apply filters
- the package of the viewpoint contains all visible virtualized source packages and an additional virtual package Vp :
    P = { virtual(p) for p in visible(Pi) } U { Vp }
- Vp owns all virtual classes from the weaving model: forall c in Vc, owner(c) = Vp
- C contains virtual counterparts of visible source classes : C = { virtual(c) for c in visible(Ci) } U Vc
- F contains virtual counterparts of visible source features: F = { virtual(f) for f in visible(Fi) } U Vf

Note that for the viewpoint to obey structural constraints, we want:
- all viewpoint objects have an owner in the viewpoint: forall o in P U C U F, owner(o) in P U C U F

visible may violate this constraint:
metamodel ({p},{a},{f}), with owner(a) = p and owner(f) = a
weaving model ({},{a},{},w) => viewpoint({},{virtual(a)},{}), and now owner(virtual(a)) = virtual(p) but virtual(p) not in P

If we care about restoring structural constraints, we can either:
1. remove a
2. include p

Choice 2 makes more sense in a whitelist, since the user wanted a to be in.

Similarly, if we use a blacklist => viewpoint({virtual(p)},{},{virtual(f)}), and owner(virtual(f)) = virtual(a) but virtual(a) not in C

We can again:
1. remove f
2. include a

Here, choice 2 would be against the user wish, so choice 1 is better.

So, in whitelist:

- P U C U F = { virtual(o) | visible(o) or exists o2: o = owner(o2) and o2 in P U C U F }

(note that this is transitive: visible(f) will add a to PUCUF, and a will add p)

(this suggests building sets P, then C, then F)

in blacklist:

- P U C U F = { virtual(o) | visible(o) and owner(o) in PUCUF }

# If we care about distinguishing attributes from references, then the EType of references also impacts these constructions


# We probably want to treat viewpoint/views as the same, with the same structural constraints
# this gets home the point that EMF Views acts on them in the same way

* [2019-08-19 lun.]
** Finalizing the formalization                          :formalism:emfviews:
One thing that I thought about yesterday: currently the formalization is a bit
disconnected from the previous sections.  Especially, the weaving model looks a
bit different than what we have shown in the previous section.

We could add a paragraph saying how we can map the weaving model operators
(virtual associations, virtual reference, etc.) to synthetic objects and values.

Hmm, in the end, there is already that info in the text: we explicitly say that
virtual concepts are represented by synthetic objects in the formalism.  Good
enough.

* [2019-08-27 mar.]
** Comparing ModelJoin to EMF Views                      :modeljoin:emfviews:
Trying to build the EMF Views tutorial view in ModelJoin.  I only need to write
a modeljoin query:

#+BEGIN_SRC modeljoin
import "platform:/resource/emfviews-tutorial/metamodels/Book.ecore"
import "platform:/resource/emfviews-tutorial/metamodels/Publication.ecore"

target "http://mjtest/test-emfviews"

natural join Publication.Publication with Book.Book
       as jointarget.PubAndBooks {
  keep attributes Publication.Publication.author,
  		  Publication.Publication.publisher,
  		  Publication.Publication.year

  keep outgoing Book.Book.chapters {
    keep attributes Book.Chapter.title
  }
}
#+END_SRC

This generates a target metamodel and a trace metamodel, an OCL file for
validity and a QVTo transformation.  Running the QVTo transformation yields a
trace model and a target model.  The target model is the same as the EMF Views
tutorial one, except that the 'bookChapters' virtual association is actually
just the 'chapters' relation that is kept from the Book class.

There's a hitch in my version: the generated QVTo transformation is invalid due
to a mysterious error:

: the type jointarget::Book does not conform to the type jointarget::Book

A workaround is to comment out these parts of the transformation, which breaks
the generated trace.

In the query we have to state out all the attributes we want to keep, but
otherwise it works well.

So, for comparison: in EMF Views the allChapter view can be described with one
VPDL file plus one eview file:

#+BEGIN_SRC vpdl
create view publicationsAndBooks as

select publication.Publication.*,
       publication.Publication join book.Chapter as bookChapters,
       book.Book.*,
       book.Chapter.title,

from 'http://publication' as publication,
     'http://book' as book,

where s.title = t.eContainer().title for bookChapters
#+END_SRC

#+BEGIN_SRC eview
contributingModels=publication::../models/publication.xmi,\
                   book::../models/book.xmi
viewpoint=../viewpoints/publicationsAndBooks.eviewpoint
matchingModel=allChapters.ecl
#+END_SRC

Although, arguably the eview file contains the same information as the launch
configuration for the QVTo transformation in Modeljoin.  So both are similar in
the amount of setup required.

EMF Views does not generate a trace or an OCL file, and it does not generate a
target metamodel as an Ecore file either.  One does not need to run a
transformation: opening the eview file with a model editor builds the view which
can then be navigated.  A consequence is that one can change the source models
and just open the eview file again to get an updated view, rather than
relaunching the transformations (although this could be a one-click step with an
Mwe2 workflow, as suggested by the Modeljoin authors).

VPDL requires you to state explicitly which attributes or references to include,
although note that wildcards are a convenience to include all of them.

The WHERE clause does the same as the one in a theta join, and supports an OCL
subset.  EMF Views has no concept of natural join where models are joined on
similarly-named fields.

This was for the allChapters view.  For the firstChapter, I can't find a way to
express it.  We would need some kind of selection on the kept outgoing
reference, or a way to compute the reference from scratch (what EMF Views does).
There is a ~keep calculated attributes~ but it doesn't apply to references.

On the other hand, EMF Views cannot express aggregated attributes as in the
Modeljoin tutorial example.

* [2019-09-02 lun.]
** Trying to bench modeljoin                                      :modeljoin:
Using the 4-models views of the MODELS paper...

Have to install a bunch of stuff in that Eclipse workspace so I don't pollute
mine.

The Java model needs MoDisco, which needs... EMF Facet.  Where is that?
[[https://download.eclipse.org/facet/updates/release/1.3.0/][Separate update site]].  Yes, MoDisco installs after that.

Hmm, the view needs ReqIF.  I can add the JAR containing the metamodel to the
target platform, and it works.  But the reqif model is using the PROR metamodel
instead...

Well, it's fine.  I can just use the Java + Trace models for the view.  That's
the stress-test part anyway.

I can run the UML+Java+Log view fine.  Trying to build a modeljoin query doing
the same...

First hurdle: the import statement appears to not work with metamodel URIs.
Urgh.  Can't find a way to pass them.  So have to retreat to Ecore files.  Is
there one for MoDisco/Java?  Ah yes there are.  Hmm the UML ones is for
3.0.0... will start with the Log/Java connection.

Of course I have to use a theta join with a QVT-O query.  And it doesn't have
String.split.  Grmbl.  Finding the [[https://www.omg.org/cgi-bin/doc?ptc/2007-07-07][QVT-O reference]].  There is String:match which
takes a Java regexp.  Good enough.
